{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMuz6soJxEmP"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Implementación de un motor de búsqueda**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 7 / jueves 9 de febrero\n",
    "* Entrega: martes 21 / jueves 23 de febrero (14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-bdu5nO581M"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Xu Chen Xu\n",
    "\n",
    "Ana Martínez Sabiote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlYtzLZp6KwJ"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La iniciación a la implementación de un motor de búsqueda.\n",
    "*\tUna primera comprensión de los elementos básicos necesarios para implementar un motor completo.\n",
    "*\tLa iniciación al uso de la librería [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html) en Python para la creación y utilización de índices, funcionalidades de búsqueda en texto.\n",
    "*\tLa iniciación a la implementación de una función de ránking sencilla.\n",
    "\n",
    "Los documentos que se indexarán en esta práctica, y sobre los que se realizarán consultas de búsqueda serán documentos HTML, que deberán ser tratados para extraer y procesar el texto contenido en ellos. \n",
    "\n",
    "La práctica plantea como punto de partida una pequeña API general sencilla (y cuyo uso se puede ver en un programa de prueba que se encuentra al final del enunciado), que pueda implementarse de diferentes maneras, como así se hará en esta práctica y las siguientes. A modo de toma de contacto y arranque de la asignatura, en esta primera práctica se completará una implementación de la API utilizando Whoosh, con lo que resultará bastante trivial la solución (en cuanto a la cantidad de código a escribir). En la siguiente práctica el estudiante desarrollará sus propias implementaciones, sustituyendo el uso de Whoosh que vamos a hacer en esta primera práctica.\n",
    "\n",
    "En términos de operaciones propias de un motor de búsqueda, en esta práctica el estudiante se encargará fundamentalmente de:\n",
    "\n",
    "a) En el proceso de indexación: recorrer los documentos de texto de una colección dada, eliminar del contenido posibles marcas tales como html, y enviar el texto a indexar por parte de Whoosh. \n",
    "\n",
    "b) En el proceso de responder consultas: implementar una primera versión sencilla de una o dos funciones de ránking en el modelo vectorial, junto con alguna pequeña estructura auxiliar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AjDofIw6Ns6"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python (mayormente incompletas) a lo largo de este *notebook*, desde las que el estudiante partirá para completar código e integrará con ellas las suyas propias. \n",
    "La celda de prueba *al final de este notebook* implementa un programa que deberá funcionar con el código a implementar por el estudiante. Además, se proporciona a continuación una celda con código ejemplo que ilustra las funciones más útiles de la API de Whoosh.\n",
    "*\tUna pequeña colección <ins>docs1k.zip</ins> con aproximadamente 1.000 documentos HTML, y un pequeño fichero <ins>urls.txt</ins>. Ambas representan colecciones de prueba para depurar las implementaciones y comprobar su corrección.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería cumplirse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9udSskn_eY7"
   },
   "source": [
    "## Ejemplo API Whoosh\n",
    "\n",
    "En la siguiente celda de código se incluyen varios ejemplos para comprobar cómo usar la API de la librería *Whoosh*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for ' probability '\n",
      "1.465530400906711 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "1.4196220208405084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.6572086214683646 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Total nr. of documents in the collection: 3\n",
      "Total frequency of ' probability ': 26.0\n",
      "Nr. documents containing ' probability ': 3\n",
      "\tFrequency of ' probability ' in document 0 : 9\n",
      "\tFrequency of ' probability ' in document 1 : 1\n",
      "\tFrequency of ' probability ' in document 2 : 16\n",
      "Frequency of ' probability ' in document 0 https://en.wikipedia.org/wiki/Simpson's_paradox : 9\n",
      "Top 5 most frequent terms in document 0 https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "\t ('paradox', 53)\n",
      "\t ('simpson', 51)\n",
      "\t ('data', 25)\n",
      "\t ('two', 19)\n",
      "\t ('displaystyle', 17)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Whoosh API\n",
    "import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os, os.path\n",
    "import shutil\n",
    "\n",
    "Document = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(vector=Format))\n",
    "\n",
    "def whooshexample_buildindex(dir, urls):\n",
    "    if os.path.exists(dir): shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "    writer = whoosh.index.create_in(dir, Document).writer()\n",
    "    for url in urls:\n",
    "        writer.add_document(path=url, content=BeautifulSoup(urlopen(url).read(), \"lxml\").text)\n",
    "    writer.commit()\n",
    "\n",
    "def whooshexample_search(dir, query):\n",
    "    index = whoosh.index.open_dir(dir)\n",
    "    searcher = index.searcher()\n",
    "    qparser = QueryParser(\"content\", schema=index.schema)\n",
    "    print(\"Search results for '\", query, \"'\")\n",
    "    for docid, score in searcher.search(qparser.parse(query)).items():\n",
    "        print(score, \"\\t\", index.reader().stored_fields(docid)['path'])\n",
    "    print()\n",
    "\n",
    "def whooshexample_examine(dir, term, docid, n):\n",
    "    reader = whoosh.index.open_dir(dir).reader()\n",
    "    print(\"Total nr. of documents in the collection:\", reader.doc_count())\n",
    "    print(\"Total frequency of '\", term, \"':\", reader.frequency(\"content\", term))\n",
    "    print(\"Nr. documents containing '\", term, \"':\", reader.doc_frequency(\"content\", term))\n",
    "    for p in reader.postings(\"content\", term).items_as(\"frequency\") if reader.doc_frequency(\"content\", term) > 0 else []:\n",
    "        print(\"\\tFrequency of '\", term, \"' in document\", p[0], \":\", p[1])\n",
    "    raw_vec = reader.vector(docid, \"content\")\n",
    "    raw_vec.skip_to(term)\n",
    "    if raw_vec.id() == term:\n",
    "        print(\"Frequency of '\", raw_vec.id(), \"' in document\", docid, reader.stored_fields(docid)['path'], \":\", raw_vec.value_as(\"frequency\"))\n",
    "    else:\n",
    "        print(\"Term '\", term, \"' not found in document\", docid)\n",
    "    print(\"Top\", n, \"most frequent terms in document\", docid, reader.stored_fields(docid)['path']) \n",
    "    vec = reader.vector(docid, \"content\").items_as(\"frequency\")\n",
    "    for p in sorted(vec, key=lambda x: x[1], reverse=True)[0:n]:\n",
    "        print(\"\\t\", p)\n",
    "    print()\n",
    "\n",
    "urls = [\"https://en.wikipedia.org/wiki/Simpson's_paradox\", \n",
    "        \"https://en.wikipedia.org/wiki/Bias\",\n",
    "        \"https://en.wikipedia.org/wiki/Entropy\"]\n",
    "\n",
    "dir = \"index/whoosh/example/urls\"\n",
    "\n",
    "whooshexample_buildindex(dir, urls)\n",
    "whooshexample_search(dir, \"probability\")\n",
    "whooshexample_examine(dir, \"probability\", 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ciKzD4D6Xn6"
   },
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado.  \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **20%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. \n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) **sin ninguna modificación**. El profesor comprobará este aspecto ejecutando la celda de prueba así como otras pruebas adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnln3zQV6anE"
   },
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p1-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...).\n",
    "\n",
    "En concreto, se debe documentar:\n",
    "\n",
    "- Qué version(es) del modelo vectorial se ha(n) implementado en el ejercicio 2.\n",
    "- Cómo se ha conseguido colocar un documento en la primera posición de ránking, para cada buscador implementado en el ejercicio 2.\n",
    "- El trabajo realizado en el ejercicio 3. \n",
    "- Y cualquier otro aspecto que el estudiante considere oportuno destacar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GtUWMA76bP0"
   },
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se podrán definir clases adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: \n",
    "\n",
    "*\tNo deberá editarse el código proporcionado más allá de donde se indica explícitamente.\n",
    "*\t**La celda de prueba deberá ejecutar** correctamente sin ninguna modificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gICjfJ-B6g0Y"
   },
   "source": [
    "# Ejercicio 1: Implementación basada en Whoosh\n",
    "\n",
    "Implementar las clases y módulos necesarios para que la celda de prueba funcione. Se deja al estudiante deducir alguna de las relaciones jerárquicas entre las clases Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m18qmDwAzANn"
   },
   "source": [
    "## Ejercicio 1.1: Indexación (3.5pt)\n",
    "\n",
    "Definir las siguientes clases:\n",
    "\n",
    "* Index: clase general (no depende de Whoosh) y que encapsule los métodos necesarios para que funcione la celda de prueba que se encuentra al final del enunciado.\n",
    "* Builder: clase general (no depende de Whoosh) que permite construir un índice (a través del método Builder.build()), tal y como se llama desde la celda de prueba entregada.\n",
    "* WhooshIndex: clase que cumpla con la interfaz definida en *Index* usando la librería de whoosh.\n",
    "* WhooshBuilder: clase que cumpla con la interfaz definida en *Builder* pero que use internamente la librería de whoosh.\n",
    "\n",
    "La entrada para construir el índice (método Builder.build()) podrá ser, tal y como se puede ver en el programa de prueba al final de este notebook, a) un fichero de texto con direcciones Web (una por línea); b) una carpeta del disco (se indexarán todos los ficheros de la carpeta, sin entrar en subcarpetas); o c) un archivo zip que contiene archivos comprimidos a indexar. Para simplificar, supondremos que el contenido a indexar es siempre HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YiNJ9ru19cN0"
   },
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "\n",
    "    def doc_freq(self, term):\n",
    "        pass\n",
    "\n",
    "    def all_terms_with_freq(self):\n",
    "        pass\n",
    "\n",
    "    def ndocs(self):\n",
    "        pass\n",
    "\n",
    "    def all_terms(self):\n",
    "        pass\n",
    "\n",
    "    def total_freq(self, term):\n",
    "        pass\n",
    "\n",
    "    def term_freq(self, term, doc_id):\n",
    "        pass\n",
    "\n",
    "    def doc_path(self, doc_id):\n",
    "        pass\n",
    "\n",
    "class Builder:\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "        self.writer = None\n",
    "        pass\n",
    "\n",
    "    # Collection es un string\n",
    "    def build(self, collection):\n",
    "        pass\n",
    "\n",
    "    def commit(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Eu23hSs6_wvX"
   },
   "outputs": [],
   "source": [
    "import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "import zipfile\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "Document = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(vector=Format))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def build(self, collection):\n",
    "        \n",
    "        if not os.path.exists(self.index_path):\n",
    "            os.makedirs(self.index_path)\n",
    "\n",
    "        if self.writer is None:\n",
    "            self.writer = whoosh.index.create_in(self.index_path, Document).writer()\n",
    "\n",
    "        if os.path.isdir(collection):\n",
    "            for file in os.listdir(collection):\n",
    "                filepath = os.path.join(collection, file)\n",
    "\n",
    "                with open(filepath, \"r\") as f:\n",
    "                    self.writer.add_document(path=filepath, content=BeautifulSoup(f.read(), \"lxml\").text)\n",
    "\n",
    "        elif zipfile.is_zipfile(collection):\n",
    "            with zipfile.ZipFile(collection, 'r') as zp:\n",
    "                for name in zp.namelist():\n",
    "                    self.writer.add_document(path=name, content=BeautifulSoup(zp.read(name), \"lxml\").text)     \n",
    "                    \n",
    "        elif collection.endswith(\".txt\"):\n",
    "            with open(collection, \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    self.writer.add_document(path=line, content=BeautifulSoup(urlopen(line).read(), \"lxml\").text)\n",
    "                    \n",
    "\n",
    "    def commit(self):\n",
    "        self.writer.commit()\n",
    "        with open(self.index_path + '/mod_file.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(self.all_modulus())\n",
    "        #mod_file = open(\"mod_file.txt\", \"w\")\n",
    "        #mod_file.writelines(self.mods)\n",
    "        #mod_file.close()\n",
    "    \n",
    "    def all_modulus(self):\n",
    "        index = WhooshIndex(self.index_path)\n",
    "        reader = index.reader\n",
    "        mods = []\n",
    "        mods.append([\"doc_id\", \"modulo\"])\n",
    "        for doc_id in reader.all_doc_ids():\n",
    "            line = [doc_id, self.modulus(doc_id, index)]\n",
    "            mods.append(line)\n",
    "        return mods\n",
    "    \n",
    "    def modulus(self, doc_id, index):\n",
    "        sum = 0\n",
    "        document = index.reader.vector(doc_id, \"content\").items_as(\"frequency\")\n",
    "        #document = from_query_to_terms(doc)\n",
    "        \n",
    "        for term,frecuency in document:\n",
    "            idf = math.log(( (index.ndocs()+1) / (index.doc_freq(term)+0.5)), 2)\n",
    "\n",
    "            #frecuency = index.term_freq(term, doc_id)\n",
    "\n",
    "            if frecuency > 0:\n",
    "                tf = 1 + math.log(frecuency, 2)\n",
    "            else:\n",
    "                tf = 0\n",
    "\n",
    "            sum += pow(tf * idf,2)\n",
    "        mod = math.sqrt(sum)\n",
    "        return mod\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "        self.reader = whoosh.index.open_dir(self.index_path).reader()\n",
    "\n",
    "    # Numero de documentos en los que aparece term\n",
    "    def doc_freq(self, term):\n",
    "        return self.reader.doc_frequency(\"content\", term)\n",
    "\n",
    "    def all_terms_with_freq(self):\n",
    "        result = []\n",
    "        for term in self.all_terms():\n",
    "            result.append((term, self.total_freq(term)))\n",
    "        return result\n",
    "\n",
    "    def ndocs(self):\n",
    "        return self.reader.doc_count()\n",
    "\n",
    "    def all_terms(self):\n",
    "        return list(self.reader.field_terms(\"content\"))\n",
    "\n",
    "    def total_freq(self, term):\n",
    "        return self.reader.frequency(\"content\", term)\n",
    "\n",
    "    def term_freq(self, term, doc_id):\n",
    "        raw_vec = self.reader.vector(doc_id, \"content\")\n",
    "        raw_vec.skip_to(term)\n",
    "        if raw_vec.id() == term:\n",
    "            return raw_vec.value_as(\"frequency\")\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def doc_path(self, doc_id):\n",
    "        return self.reader.stored_fields(doc_id)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGjT-DbW-yHR"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "\n",
    "**WhooshBuilder**: Clase que hereda de Builder y depende de Whoosh. Permite construir un índice utilizando métodos de la librería Whoosh.\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(index_path)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *index_path*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "* **build(collection)**: Método que construye el índice. Recibe un único parámetro *collection*. Este puede ser la ruta de un fichero de texto con direcciones Web (una por línea), una carpeta del disco (se indexarán todos los ficheros de la carpeta, sin entrar en subcarpetas) o un archivo zip que contiene archivos comprimidos a indexar. El contenido a indexar tiene que ser HTML. Para construir el índice, se utiliza la librería Whoosh.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *collection*: Ruta de los documentos que queremos indexar.\n",
    "<br/><br/>\n",
    "* **commit()**: Método que guarda de forma definitiva el índice en el disco. Tras ello se calcula el módulo de todos los documentos que forman el índice (usando la función **all_modulus**) y se guardan en un archivo .csv en la carpeta de índice junto a los ficheros que genera el índice. Este archivo contiene en cada fila el doc_id, seguido de su correspondiente módulo. \n",
    "<br/><br/>\n",
    "* **all_modulus()**: Método auxiliar que se ha implentado para calcular el módulo de todos los documentos del índice. Se itera por doc_id y se va guardando en un vector el doc_id y su correspondiente módulo. \n",
    "<br/><br/>\n",
    "* **modulus(doc_id, index)**: Método auxiliar que calcula el módulo de un documento. Para ello calcula tf-idf de cada término del documento y devuelve la suma de los cuadrados de tf-idf de cada término del documento.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *doc_id*: id del documento del cual queremos calcular su módulo.\n",
    "    * *index*: objeto WhooshIndex del índice en el que está indexado el documento identificado por doc_id \n",
    "<br/><br/>\n",
    "\n",
    "**WhooshIndex**: Clase que hereda de Index y depende de Whoosh. Permite obtener información del índice construido con WhooshBuilder. Se puede utilizar para obtener datos como el número de términos totales en el índice, la frecuencia de un término en un documento, el número de documentos que contienen un término...\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(index_path)**: Constructor de la clase. Recibe como parámetros la ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *index_path*: Ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "* **doc_freq(term):**: Método que dado un término, devuelve el número de documentos que lo contienen.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*: Término del cual queremos encontrar el número de documentos que lo contienen.\n",
    "<br/><br/>\n",
    "* **all_terms_with_freq()**: Método que devuelve una lista en la cada elemento es una tupla con el término y su frecuencia en el índice.\n",
    "<br/><br/>\n",
    "* **n_docs()**: Devuelve un entero con el número de documentos en el índice.\n",
    "<br/><br/>\n",
    "* **all_terms_with_freq()**: Devuelve una lista con todos los términos del índice.\n",
    "<br/><br/>\n",
    "* **total_freq(term):**: Método que dado un término, devuelve la frecuencia total de ese término en el índice, es decir, la suma de las frecuencias de ese término entre todos los documentos.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*: Término del cual queremos encontrar la frecuencia total.\n",
    "<br/><br/>\n",
    "* **term_freq(term, doc_id):**: Método que dado un término y el id de un documento en el índice, devuelve la frecuencia de dicho término en ese documento.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*: Término del cual queremos encontrar la frecuencia en un documento.\n",
    "    * *doc_id*: Id del documento en el que queremos encontrar la frecuencia del término.\n",
    "<br/><br/>\n",
    "* **doc_path(doc_id):**: Método que dado el id de un documento devuelve la ruta del documento.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *doc_id*: Id del documento del que queremos encontrar la ruta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEOpKvZi9cos"
   },
   "source": [
    "## Ejercicio 1.2: Búsqueda (2pt)\n",
    "\n",
    "Implementar la clase WhooshSearcher como subclase de Searcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "\n",
    "def from_query_to_terms(text):\n",
    "    return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents built as a pair of path and score.\n",
    "            As a simplification, the query can be divided in terms by considering blank spaces. \n",
    "            Moreover, the terms can be normalized to lower case (i.e., you may use function 'from_query_to_terms').\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IGtH0CpF9owH"
   },
   "outputs": [],
   "source": [
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "        self.WhooshIndex = WhooshIndex(index_path)\n",
    "        self.index = whoosh.index.open_dir(self.index_path)\n",
    "        \n",
    "    def search(self, query, cutoff):\n",
    "        searcher = self.index.searcher()\n",
    "        qparser = QueryParser(\"content\", schema=self.index.schema)\n",
    "\n",
    "        results = [(self.WhooshIndex.doc_path(docid), score) for docid, score in searcher.search(qparser.parse(query)).items()]\n",
    "        results.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        return results[0:cutoff]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNDHrfjN-zTV"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JH9u7bWi9pGc"
   },
   "source": [
    "# Ejercicio 2: Modelo vectorial\n",
    "\n",
    "Implementar dos modelos de ránking propios, basados en el modelo vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SphKCsSN9udY"
   },
   "source": [
    "## Ejercicio 2.1: Producto escalar (2.5pt)\n",
    "\n",
    "Implementar un modelo vectorial propio que utilice el producto escalar (sin dividir por las normas de los vectores) como función de ránking, por medio de la clase VSMDotProductSearcher, como subclase de Searcher.\n",
    "\n",
    "Este modelo hará uso de la clase Index y se podrá probar con la implementación WhooshIndex (puedes ver un ejemplo de esto en la celda de prueba).\n",
    "\n",
    "Además, la clase VSMDotProductSearcher será intercambiable con WhooshSearcher, como se puede ver en la celda de prueba, donde la función test_search utiliza una implementación u otra sin distinción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6JQ2xbth-A-G"
   },
   "outputs": [],
   "source": [
    "class VSMDotProductSearcher(Searcher):\n",
    "\n",
    "    def __init__(self, index):\n",
    "        super().__init__(index)\n",
    "        self.reader = self.index.reader\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        query_terms = from_query_to_terms(query)\n",
    "        results = []\n",
    "\n",
    "        for doc_id in self.reader.all_doc_ids():\n",
    "            dot_product = self.dot_product(query_terms, doc_id)\n",
    "            if dot_product > 0:\n",
    "                results.append((self.index.doc_path(doc_id), self.dot_product(query_terms, doc_id)))\n",
    "\n",
    "        results.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        return results[0:cutoff]\n",
    "\n",
    "    def dot_product(self, query_terms, doc_id):\n",
    "        sum = 0\n",
    "        for term in query_terms:\n",
    "            idf = math.log(( (self.index.ndocs()+1) / (self.index.doc_freq(term)+0.5)), 2)\n",
    "\n",
    "            frecuency = self.index.term_freq(term, doc_id)\n",
    "\n",
    "            if frecuency > 0:\n",
    "                tf = 1 + math.log(frecuency, 2)\n",
    "            else:\n",
    "                tf = 0\n",
    "\n",
    "            sum += tf * idf\n",
    "\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgumTPoT-1WD"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o5TvvE5-BVK"
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPPNbWNe-HcR"
   },
   "source": [
    "## Ejercicio 2.2: Coseno (2pt)\n",
    "\n",
    "Refinar la implementación del modelo para que calcule el coseno, definiendo para ello una clase VSMCosineSearcher. Para ello se necesitará extender Builder (o WhooshBuilder) con el cálculo de los módulos de los vectores, que deberán almacenarse en un fichero, en la carpeta de índice junto a los ficheros que genera cada índice. \n",
    "\n",
    "Pensad en qué parte del diseño interesa hacer esto, en concreto, qué clase y en qué momento tendría que calcular, devolver y/o almacenar estos módulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9n30WQX_-RMR"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "class VSMCosineSearcher(VSMDotProductSearcher):\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        query_terms = from_query_to_terms(query)\n",
    "        results = []\n",
    "\n",
    "        for doc_id in self.reader.all_doc_ids():\n",
    "            dot_product = self.dot_product(query_terms, doc_id)\n",
    "            if dot_product > 0:\n",
    "                # Buscar modulo del doc en el fichero de modulos del índice\n",
    "                mod_d = self.mod_docid(doc_id)\n",
    "                mod_q = self.mod_query(query_terms)\n",
    "                modulus = mod_d * mod_q\n",
    "                cos = dot_product / modulus\n",
    "                results.append((self.index.doc_path(doc_id), cos))\n",
    "\n",
    "        results.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        return results[0:cutoff]\n",
    "    \n",
    "    def mod_docid(self, doc_id):\n",
    "        #file = open(self.indes.index_path + \"/mod_file.txt\", \"r\")\n",
    "        #mods = file.readlines()\n",
    "        #docpath = self.index.doc_path(doc_id)\n",
    "        df = pd.read_csv(self.index.index_path + '/mod_file.csv')\n",
    "        rslt_df = df[(df['doc_id'] == doc_id)] \n",
    "        mod = rslt_df['modulo'].values[0]\n",
    "        return mod\n",
    "        \n",
    "    def mod_query(self,vec):\n",
    "        vec_frec = collections.Counter(vec)\n",
    "        sum = 0\n",
    "        for term in vec:\n",
    "            sum += pow(vec_frec[term],2)\n",
    "        return math.sqrt(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJqVKt6o-2oB"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**VSMCosineSearcher**: Clase que hereda de VSMDotProductSearcher. Implementa el coseno como función de ránking\n",
    "**Métodos:**\n",
    "* **search(query, cutoff)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *query*: string que contiene la consulta a buscar.\n",
    "    * *cutoff*: número de resultados que queremos que devuelva el buscador\n",
    "<br/><br/>\n",
    "* **mod_docid(doc_id)**: función auxiliar que busca el módulo del documento identificado por doc_id en el fichero mod_file.csv del índice. Para ello se utiliza la librería pandas de python.\n",
    "<br/><br/>\n",
    "Parámetros: \n",
    "    * *doc_id*: id del documento del cual queremos obtener su módulo.\n",
    "<br/><br/>\n",
    "* **mod_query(vec)**: función auxiliar que calcula el módulo de la consulta. El módulo de la consulta es la raíz de la suma de la frecuencia al cuadrado de cada término de la consulta.\n",
    "<br/><br/>\n",
    "Parámetros: \n",
    "    * *vec*: vector de términos de la consulta.\n",
    "<br/><br/>\n",
    "\n",
    "En el apartado anterior de Builder se han descrito los métodos que se han utilizado para calcular el módulo de cada documento y guardarlo en un archivo. Tal y como se indica en el enunciado de esta parte se ha extendido la clase Builder con el cálculo del módulo de los documentos. Esta operación no se puede realizar hasta que no se cree y guarde el índice, ya que no se puede añadir información y operar con el índice hasta que éste no esté almacenado y la información sea persistente. Por esto, no se pueden hallar los módulos de los documentos hasta que no se realice *writer.commit()*. Hemos decidido calcular los módulos de los documentos a continuación de la operación anterior, en el método **commit()** de la clase Builder. Siguendo los ejemplos de clase, el módulo de un documento es la raíz cuadrada de la suma de los cuadrados de tf-idf de cada término del documento. La información calculada se guarda en un archivo mod_file.csv (cada línea doc_id, módulo) en la carpeta del índice junto a los otros ficheros que genera. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh83cdIu-Re1"
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Wf1RFu8-V8P"
   },
   "source": [
    "# Ejercicio 3: Estadísticas de frecuencias (1pt)\n",
    "\n",
    "Utilizando las funcionalidades de la clase Index, implementar una función term_stats que calcule a) las frecuencias totales en la colección de los términos, ordenadas de mayor a menor, y b) el número de documentos que contiene cada término, igualmente de mayor a menor. Visualizar las estadísticas obtenidas en dos gráficas en escala log-log (dos gráficas por cada colección, seis gráficas en total), que se mostrarán en el cuaderno entregado.\n",
    "\n",
    "De esta forma, podrás comprobar si las estadísticas de la colección siguen algún tipo de comportamiento esperado (como la conocida [Ley de Zipf](https://es.wikipedia.org/wiki/Ley_de_Zipf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "u2EgV-4w-erd"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (558125456.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [8], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    # Your code here #\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def term_stats(index):\n",
    "    ## TODO ##\n",
    "    # Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlXLFNH3-4MH"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfgNDMM6-e7k"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta *collections* en el mismo directorio que este *notebook*. El fichero *toy.zip* hay que descomprimirlo para indexar la carpeta que contiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VXS8648MzPO3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy/\n",
      "Done ( 0.02725529670715332 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Top 5 most frequent terms:\n",
      "\taa\t9.0=9.0\n",
      "\tbb\t5.0=5.0\n",
      "\tsleep\t5.0=5.0\n",
      "\tcc\t3.0=3.0\n",
      "\tdie\t2.0=2.0\n",
      "\n",
      "\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word'cc': 2\n",
      "Done ( 0.0017671585083007812 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'aa dd'\n",
      "\n",
      "Done ( 0.0016503334045410156 seconds )\n",
      "\n",
      "  VSMDotProductSearcher for query 'aa dd'\n",
      "4.0 \t ./collections/toy/d1.txt\n",
      "1.7369655941662063 \t ./collections/toy/d2.txt\n",
      "1.0 \t ./collections/toy/d3.txt\n",
      "\n",
      "Done ( 0.0012679100036621094 seconds )\n",
      "\n",
      "  VSMCosineSearcher for query 'aa dd'\n",
      "0.7071067811865475 \t ./collections/toy/d2.txt\n",
      "0.5252257314388902 \t ./collections/toy/d1.txt\n",
      "0.40824829046386296 \t ./collections/toy/d3.txt\n",
      "\n",
      "Done ( 0.009935855865478516 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 4.0106635093688965 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6058\n",
      "  Top 5 most frequent terms:\n",
      "\tentropy\t375.0=375.0\n",
      "\tbias\t201.0=201.0\n",
      "\tsystem\t154.0=154.0\n",
      "\tdisplaystyle\t138.0=138.0\n",
      "\theat\t111.0=111.0\n",
      "\n",
      "\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      ": 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word'wikipedia': 3\n",
      "Done ( 0.27845191955566406 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'information probability'\n",
      "2.9256657209314 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.762888436480296 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "\n",
      "2.0995062376814877 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "\n",
      "Done ( 0.0036246776580810547 seconds )\n",
      "\n",
      "  VSMDotProductSearcher for query 'information probability'\n",
      "2.1879764911644646 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.301295829346397 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "\n",
      "1.155870467654375 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "\n",
      "Done ( 0.008412599563598633 seconds )\n",
      "\n",
      "  VSMCosineSearcher for query 'information probability'\n",
      "0.017218784474681177 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "\n",
      "0.01232516173043447 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.006786323950422498 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "\n",
      "Done ( 0.016021728515625 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 92.2968590259552 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 1000\n",
      "Vocabulary size: 117997\n",
      "  Top 5 most frequent terms:\n",
      "\tfamily\t177556.0=177556.0\n",
      "\ttree\t48449.0=48449.0\n",
      "\thistory\t45749.0=45749.0\n",
      "\tgenealogy\t45407.0=45407.0\n",
      "\tsurname\t44965.0=44965.0\n",
      "\n",
      "\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word'seat': 119\n",
      "Done ( 2.807520627975464 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'obama family tree'\n",
      "16.82655363679052 \t top11DotProduct.html\n",
      "16.447106570261067 \t clueweb09-en0010-79-2218.html\n",
      "16.4313799994618 \t top1DotProductSearcher.html\n",
      "15.851672821537992 \t clueweb09-en0010-57-32937.html\n",
      "15.773150284386036 \t clueweb09-en0001-02-21241.html\n",
      "\n",
      "Done ( 0.04070448875427246 seconds )\n",
      "\n",
      "  VSMDotProductSearcher for query 'obama family tree'\n",
      "79.00509973260714 \t top11DotProduct.html\n",
      "56.275197839859764 \t top1DotProductSearcher.html\n",
      "49.38496944750155 \t clueweb09-enwp01-59-16163.html\n",
      "49.38496944750155 \t clueweb09-enwp02-06-15081.html\n",
      "49.372060754092885 \t clueweb09-enwp03-00-6901.html\n",
      "\n",
      "Done ( 2.1462557315826416 seconds )\n",
      "\n",
      "  VSMCosineSearcher for query 'obama family tree'\n",
      "0.29739903451902117 \t top11DotProduct.html\n",
      "0.16343078668711064 \t clueweb09-en0010-79-2218.html\n",
      "0.13013168730748526 \t clueweb09-en0009-30-2768.html\n",
      "0.1294699904322817 \t clueweb09-en0001-02-21241.html\n",
      "0.12872469251591115 \t clueweb09-en0009-30-2441.html\n",
      "\n",
      "Done ( 2.443016767501831 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 3 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 98.67367482185364 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 1007\n",
      "Vocabulary size: 120078\n",
      "  Top 5 most frequent terms:\n",
      "\tfamily\t177556.0=177556.0\n",
      "\ttree\t48449.0=48449.0\n",
      "\thistory\t45772.0=45772.0\n",
      "\tgenealogy\t45407.0=45407.0\n",
      "\tsurname\t44965.0=44965.0\n",
      "\n",
      "\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy/d1.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word'seat': 119\n",
      "Done ( 2.756425142288208 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'obama family tree'\n",
      "16.872641422922005 \t top11DotProduct.html\n",
      "16.493073878727163 \t clueweb09-en0010-79-2218.html\n",
      "16.47789207835745 \t top1DotProductSearcher.html\n",
      "15.897276206788417 \t clueweb09-en0010-57-32937.html\n",
      "15.818567372984113 \t clueweb09-en0001-02-21241.html\n",
      "\n",
      "Done ( 0.03930068016052246 seconds )\n",
      "\n",
      "  VSMDotProductSearcher for query 'obama family tree'\n",
      "79.35843887833057 \t top11DotProduct.html\n",
      "56.51694155526108 \t top1DotProductSearcher.html\n",
      "49.59033583802463 \t clueweb09-enwp01-59-16163.html\n",
      "49.59033583802463 \t clueweb09-enwp02-06-15081.html\n",
      "49.57721522389516 \t clueweb09-enwp03-00-6901.html\n",
      "\n",
      "Done ( 2.7711753845214844 seconds )\n",
      "\n",
      "  VSMCosineSearcher for query 'obama family tree'\n",
      "0.2988479630574232 \t top11DotProduct.html\n",
      "0.16438525835688778 \t clueweb09-en0010-79-2218.html\n",
      "0.13077485873072825 \t clueweb09-en0009-30-2768.html\n",
      "0.13005473869906273 \t clueweb09-en0001-02-21241.html\n",
      "0.12934942943418454 \t clueweb09-en0009-30-2441.html\n",
      "\n",
      "Done ( 3.254262685775757 seconds )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def clear (index_path: str):\n",
    "    if os.path.exists(index_path): shutil.rmtree(index_path)\n",
    "    else: print(\"Creating \" + index_path)\n",
    "    os.makedirs(index_path)\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, query: str):\n",
    "    start_time = time.time()\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # Let's create the folder if it did not exist\n",
    "    # and delete the index if it did\n",
    "    clear(index_path)\n",
    "\n",
    "    # We now test building an index\n",
    "    test_build(WhooshBuilder(index_path), collection_paths)\n",
    "\n",
    "    # We now inspect the index\n",
    "    index = WhooshIndex(index_path)\n",
    "    test_read(index, word)\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "    print(\"Checking search results\")\n",
    "    test_search(WhooshSearcher(index_path), query, 5)\n",
    "    test_search(VSMDotProductSearcher(WhooshIndex(index_path)), query, 5)\n",
    "    test_search(VSMCosineSearcher(WhooshIndex(index_path)), query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # This function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # When we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    terms = index.all_terms_with_freq()\n",
    "    terms.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    print(\"  Top 5 most frequent terms:\")\n",
    "    for term in terms[0:5]:\n",
    "        print(\"\\t\" + term[0] + \"\\t\" + str(term[1]) + \"=\" + str(index.total_freq(term)))\n",
    "    print()\n",
    "    # More tests\n",
    "    doc_id = 0\n",
    "    print()\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word'\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "test_collection ([collections_root_dir + \"toy/\"], index_root_dir + \"toy\", \"cc\", \"aa dd\")\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls\", \"wikipedia\", \"information probability\")\n",
    "test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs\", \"seat\", \"obama family tree\")\n",
    "test_collection ([collections_root_dir + \"toy/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"all_together\", \"seat\", \"obama family tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTtUTF7j_QdF"
   },
   "source": [
    "### Salida obtenida por el estudiante\n",
    "\n",
    "*(por hacer)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
