{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMuz6soJxEmP"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Implementación de un motor de búsqueda**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 7 / jueves 9 de febrero\n",
    "* Entrega: martes 21 / jueves 23 de febrero (14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-bdu5nO581M"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Xu Chen\n",
    "\n",
    "Ana Martínez Sabiote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlYtzLZp6KwJ"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La iniciación a la implementación de un motor de búsqueda.\n",
    "*\tUna primera comprensión de los elementos básicos necesarios para implementar un motor completo.\n",
    "*\tLa iniciación al uso de la librería [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html) en Python para la creación y utilización de índices, funcionalidades de búsqueda en texto.\n",
    "*\tLa iniciación a la implementación de una función de ránking sencilla.\n",
    "\n",
    "Los documentos que se indexarán en esta práctica, y sobre los que se realizarán consultas de búsqueda serán documentos HTML, que deberán ser tratados para extraer y procesar el texto contenido en ellos. \n",
    "\n",
    "La práctica plantea como punto de partida una pequeña API general sencilla (y cuyo uso se puede ver en un programa de prueba que se encuentra al final del enunciado), que pueda implementarse de diferentes maneras, como así se hará en esta práctica y las siguientes. A modo de toma de contacto y arranque de la asignatura, en esta primera práctica se completará una implementación de la API utilizando Whoosh, con lo que resultará bastante trivial la solución (en cuanto a la cantidad de código a escribir). En la siguiente práctica el estudiante desarrollará sus propias implementaciones, sustituyendo el uso de Whoosh que vamos a hacer en esta primera práctica.\n",
    "\n",
    "En términos de operaciones propias de un motor de búsqueda, en esta práctica el estudiante se encargará fundamentalmente de:\n",
    "\n",
    "a) En el proceso de indexación: recorrer los documentos de texto de una colección dada, eliminar del contenido posibles marcas tales como html, y enviar el texto a indexar por parte de Whoosh. \n",
    "\n",
    "b) En el proceso de responder consultas: implementar una primera versión sencilla de una o dos funciones de ránking en el modelo vectorial, junto con alguna pequeña estructura auxiliar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AjDofIw6Ns6"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python (mayormente incompletas) a lo largo de este *notebook*, desde las que el estudiante partirá para completar código e integrará con ellas las suyas propias. \n",
    "La celda de prueba *al final de este notebook* implementa un programa que deberá funcionar con el código a implementar por el estudiante. Además, se proporciona a continuación una celda con código ejemplo que ilustra las funciones más útiles de la API de Whoosh.\n",
    "*\tUna pequeña colección <ins>docs1k.zip</ins> con aproximadamente 1.000 documentos HTML, y un pequeño fichero <ins>urls.txt</ins>. Ambas representan colecciones de prueba para depurar las implementaciones y comprobar su corrección.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería cumplirse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9udSskn_eY7"
   },
   "source": [
    "## Ejemplo API Whoosh\n",
    "\n",
    "En la siguiente celda de código se incluyen varios ejemplos para comprobar cómo usar la API de la librería *Whoosh*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for ' probability '\n",
      "1.4655068943024385 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "1.4195698196433444 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.6570661342415882 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Total nr. of documents in the collection: 3\n",
      "Total frequency of ' probability ': 26.0\n",
      "Nr. documents containing ' probability ': 3\n",
      "\tFrequency of ' probability ' in document 0 : 9\n",
      "\tFrequency of ' probability ' in document 1 : 1\n",
      "\tFrequency of ' probability ' in document 2 : 16\n",
      "Frequency of ' probability ' in document 0 https://en.wikipedia.org/wiki/Simpson's_paradox : 9\n",
      "Top 5 most frequent terms in document 0 https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "\t ('paradox', 53)\n",
      "\t ('simpson', 51)\n",
      "\t ('data', 25)\n",
      "\t ('two', 19)\n",
      "\t ('displaystyle', 17)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Whoosh API\n",
    "import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os, os.path\n",
    "import shutil\n",
    "\n",
    "Document = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(vector=Format))\n",
    "\n",
    "def whooshexample_buildindex(dir, urls):\n",
    "    if os.path.exists(dir): shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "    writer = whoosh.index.create_in(dir, Document).writer()\n",
    "    for url in urls:\n",
    "        writer.add_document(path=url, content=BeautifulSoup(urlopen(url).read(), \"lxml\").text)\n",
    "    writer.commit()\n",
    "\n",
    "def whooshexample_search(dir, query):\n",
    "    index = whoosh.index.open_dir(dir)\n",
    "    searcher = index.searcher()\n",
    "    qparser = QueryParser(\"content\", schema=index.schema)\n",
    "    print(\"Search results for '\", query, \"'\")\n",
    "    for docid, score in searcher.search(qparser.parse(query)).items():\n",
    "        print(score, \"\\t\", index.reader().stored_fields(docid)['path'])\n",
    "    print()\n",
    "\n",
    "def whooshexample_examine(dir, term, docid, n):\n",
    "    reader = whoosh.index.open_dir(dir).reader()\n",
    "    print(\"Total nr. of documents in the collection:\", reader.doc_count())\n",
    "    print(\"Total frequency of '\", term, \"':\", reader.frequency(\"content\", term))\n",
    "    print(\"Nr. documents containing '\", term, \"':\", reader.doc_frequency(\"content\", term))\n",
    "    for p in reader.postings(\"content\", term).items_as(\"frequency\") if reader.doc_frequency(\"content\", term) > 0 else []:\n",
    "        print(\"\\tFrequency of '\", term, \"' in document\", p[0], \":\", p[1])\n",
    "    raw_vec = reader.vector(docid, \"content\")\n",
    "    raw_vec.skip_to(term)\n",
    "    if raw_vec.id() == term:\n",
    "        print(\"Frequency of '\", raw_vec.id(), \"' in document\", docid, reader.stored_fields(docid)['path'], \":\", raw_vec.value_as(\"frequency\"))\n",
    "    else:\n",
    "        print(\"Term '\", term, \"' not found in document\", docid)\n",
    "    print(\"Top\", n, \"most frequent terms in document\", docid, reader.stored_fields(docid)['path']) \n",
    "    vec = reader.vector(docid, \"content\").items_as(\"frequency\")\n",
    "    for p in sorted(vec, key=lambda x: x[1], reverse=True)[0:n]:\n",
    "        print(\"\\t\", p)\n",
    "    print()\n",
    "\n",
    "urls = [\"https://en.wikipedia.org/wiki/Simpson's_paradox\", \n",
    "        \"https://en.wikipedia.org/wiki/Bias\",\n",
    "        \"https://en.wikipedia.org/wiki/Entropy\"]\n",
    "\n",
    "dir = \"index/whoosh/example/urls\"\n",
    "\n",
    "whooshexample_buildindex(dir, urls)\n",
    "whooshexample_search(dir, \"probability\")\n",
    "whooshexample_examine(dir, \"probability\", 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ciKzD4D6Xn6"
   },
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado.  \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **20%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. \n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) **sin ninguna modificación**. El profesor comprobará este aspecto ejecutando la celda de prueba así como otras pruebas adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnln3zQV6anE"
   },
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p1-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...).\n",
    "\n",
    "En concreto, se debe documentar:\n",
    "\n",
    "- Qué version(es) del modelo vectorial se ha(n) implementado en el ejercicio 2.\n",
    "- Cómo se ha conseguido colocar un documento en la primera posición de ránking, para cada buscador implementado en el ejercicio 2.\n",
    "- El trabajo realizado en el ejercicio 3. \n",
    "- Y cualquier otro aspecto que el estudiante considere oportuno destacar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GtUWMA76bP0"
   },
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se podrán definir clases adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: \n",
    "\n",
    "*\tNo deberá editarse el código proporcionado más allá de donde se indica explícitamente.\n",
    "*\t**La celda de prueba deberá ejecutar** correctamente sin ninguna modificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gICjfJ-B6g0Y"
   },
   "source": [
    "# Ejercicio 1: Implementación basada en Whoosh\n",
    "\n",
    "Implementar las clases y módulos necesarios para que la celda de prueba funcione. Se deja al estudiante deducir alguna de las relaciones jerárquicas entre las clases Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m18qmDwAzANn"
   },
   "source": [
    "## Ejercicio 1.1: Indexación (3.5pt)\n",
    "\n",
    "Definir las siguientes clases:\n",
    "\n",
    "* Index: clase general (no depende de Whoosh) y que encapsule los métodos necesarios para que funcione la celda de prueba que se encuentra al final del enunciado.\n",
    "* Builder: clase general (no depende de Whoosh) que permite construir un índice (a través del método Builder.build()), tal y como se llama desde la celda de prueba entregada.\n",
    "* WhooshIndex: clase que cumpla con la interfaz definida en *Index* usando la librería de whoosh.\n",
    "* WhooshBuilder: clase que cumpla con la interfaz definida en *Builder* pero que use internamente la librería de whoosh.\n",
    "\n",
    "La entrada para construir el índice (método Builder.build()) podrá ser, tal y como se puede ver en el programa de prueba al final de este notebook, a) un fichero de texto con direcciones Web (una por línea); b) una carpeta del disco (se indexarán todos los ficheros de la carpeta, sin entrar en subcarpetas); o c) un archivo zip que contiene archivos comprimidos a indexar. Para simplificar, supondremos que el contenido a indexar es siempre HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "YiNJ9ru19cN0"
   },
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "\n",
    "    def doc_freq(self, term):\n",
    "        pass\n",
    "\n",
    "    def all_terms_with_freq(self):\n",
    "        pass\n",
    "\n",
    "    def ndocs(self):\n",
    "        pass\n",
    "\n",
    "    def all_terms(self):\n",
    "        pass\n",
    "\n",
    "    def total_freq(self, term):\n",
    "        pass\n",
    "\n",
    "    def doc_freq(self, doc_id):\n",
    "        pass\n",
    "\n",
    "    def term_freq(self, term, doc_id):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Builder:\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "        pass\n",
    "\n",
    "    # Collection es un string\n",
    "    def build(self, collection):\n",
    "        pass\n",
    "\n",
    "    def commit(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Eu23hSs6_wvX"
   },
   "outputs": [],
   "source": [
    "import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "import zipfile\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "Document = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(vector=Format))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def build(self, collection):\n",
    "        if os.path.exists(self.index_path): shutil.rmtree(self.index_path)\n",
    "        os.makedirs(self.index_path)\n",
    "        self.writer = whoosh.index.create_in(self.index_path, Document).writer()\n",
    "\n",
    "        if os.path.isdir(collection):\n",
    "            for file in os.listdir(collection):\n",
    "                filepath = os.path.join(collection, file)\n",
    "\n",
    "                with open(filepath, \"r\") as f:\n",
    "                    self.writer.add_document(path=filepath, content=BeautifulSoup(f.read(), \"lxml\").text)\n",
    "\n",
    "        elif zipfile.is_zipfile(collection):\n",
    "            with zipfile.ZipFile(collection, 'r') as zp:\n",
    "                for name in zp.namelist():\n",
    "                    self.writer.add_document(path=name, content=BeautifulSoup(zp.read(name), \"lxml\").text)\n",
    "        elif collection.endswith(\".txt\"):\n",
    "            with open(collection, \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    self.writer.add_document(path=line, content=BeautifulSoup(urlopen(line).read(), \"lxml\").text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.writer.commit()\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "        self.reader = whoosh.index.open_dir(self.index_path).reader()\n",
    "\n",
    "    # Numero de documentos en los que aparece term\n",
    "    def doc_freq(self, term):\n",
    "        return self.reader.doc_frequency(\"content\", term)\n",
    "\n",
    "    def all_terms_with_freq(self):\n",
    "        result = []\n",
    "        for term in self.all_terms():\n",
    "            result.append((term, self.total_freq(term)))\n",
    "        return result\n",
    "\n",
    "    def ndocs(self):\n",
    "        return self.reader.doc_count()\n",
    "\n",
    "    def all_terms(self):\n",
    "        return list(self.reader.field_terms(\"content\"))\n",
    "\n",
    "    def total_freq(self, term):\n",
    "        return self.reader.frequency(\"content\", term)\n",
    "\n",
    "    def term_freq(self, term, doc_id):\n",
    "        raw_vec = self.reader.vector(doc_id, \"content\")\n",
    "        raw_vec.skip_to(term)\n",
    "        if raw_vec.id() == term:\n",
    "            return raw_vec.value_as(\"frequency\")\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def doc_path(self, doc_id):\n",
    "        return self.reader.stored_fields(doc_id)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGjT-DbW-yHR"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEOpKvZi9cos"
   },
   "source": [
    "## Ejercicio 1.2: Búsqueda (2pt)\n",
    "\n",
    "Implementar la clase WhooshSearcher como subclase de Searcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "\n",
    "def from_query_to_terms(text):\n",
    "    return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents built as a pair of path and score.\n",
    "            As a simplification, the query can be divided in terms by considering blank spaces. \n",
    "            Moreover, the terms can be normalized to lower case (i.e., you may use function 'from_query_to_terms').\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "IGtH0CpF9owH"
   },
   "outputs": [],
   "source": [
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, index_path):\n",
    "        self.index_path = index_path\n",
    "        self.WhooshIndex = WhooshIndex(index_path)\n",
    "        self.index = whoosh.index.open_dir(self.index_path)\n",
    "        \n",
    "    def search(self, query, cutoff):\n",
    "        searcher = self.index.searcher()\n",
    "        qparser = QueryParser(\"content\", schema=self.index.schema)\n",
    "\n",
    "        results = [(self.WhooshIndex.doc_path(docid), score) for docid, score in searcher.search(qparser.parse(query)).items()]\n",
    "        results.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        return results[0:cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNDHrfjN-zTV"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JH9u7bWi9pGc"
   },
   "source": [
    "# Ejercicio 2: Modelo vectorial\n",
    "\n",
    "Implementar dos modelos de ránking propios, basados en el modelo vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SphKCsSN9udY"
   },
   "source": [
    "## Ejercicio 2.1: Producto escalar (2.5pt)\n",
    "\n",
    "Implementar un modelo vectorial propio que utilice el producto escalar (sin dividir por las normas de los vectores) como función de ránking, por medio de la clase VSMDotProductSearcher, como subclase de Searcher.\n",
    "\n",
    "Este modelo hará uso de la clase Index y se podrá probar con la implementación WhooshIndex (puedes ver un ejemplo de esto en la celda de prueba).\n",
    "\n",
    "Además, la clase VSMDotProductSearcher será intercambiable con WhooshSearcher, como se puede ver en la celda de prueba, donde la función test_search utiliza una implementación u otra sin distinción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "6JQ2xbth-A-G"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (71648419.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [45], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    # Your code here #\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class VSMDotProductSearcher(Searcher):\n",
    "    ## TODO ##\n",
    "    # Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgumTPoT-1WD"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o5TvvE5-BVK"
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPPNbWNe-HcR"
   },
   "source": [
    "## Ejercicio 2.2: Coseno (2pt)\n",
    "\n",
    "Refinar la implementación del modelo para que calcule el coseno, definiendo para ello una clase VSMCosineSearcher. Para ello se necesitará extender Builder (o WhooshBuilder) con el cálculo de los módulos de los vectores, que deberán almacenarse en un fichero, en la carpeta de índice junto a los ficheros que genera cada índice. \n",
    "\n",
    "Pensad en qué parte del diseño interesa hacer esto, en concreto, qué clase y en qué momento tendría que calcular, devolver y/o almacenar estos módulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n30WQX_-RMR"
   },
   "outputs": [],
   "source": [
    "class VSMCosineSearcher(VSMDotProductSearcher):\n",
    "    ## TODO ##\n",
    "    # Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJqVKt6o-2oB"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh83cdIu-Re1"
   },
   "source": [
    "### Ejercicio\n",
    "\n",
    "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Wf1RFu8-V8P"
   },
   "source": [
    "# Ejercicio 3: Estadísticas de frecuencias (1pt)\n",
    "\n",
    "Utilizando las funcionalidades de la clase Index, implementar una función term_stats que calcule a) las frecuencias totales en la colección de los términos, ordenadas de mayor a menor, y b) el número de documentos que contiene cada término, igualmente de mayor a menor. Visualizar las estadísticas obtenidas en dos gráficas en escala log-log (dos gráficas por cada colección, seis gráficas en total), que se mostrarán en el cuaderno entregado.\n",
    "\n",
    "De esta forma, podrás comprobar si las estadísticas de la colección siguen algún tipo de comportamiento esperado (como la conocida [Ley de Zipf](https://es.wikipedia.org/wiki/Ley_de_Zipf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2EgV-4w-erd"
   },
   "outputs": [],
   "source": [
    "def term_stats(index):\n",
    "    ## TODO ##\n",
    "    # Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlXLFNH3-4MH"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "*(por hacer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfgNDMM6-e7k"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta *collections* en el mismo directorio que este *notebook*. El fichero *toy.zip* hay que descomprimirlo para indexar la carpeta que contiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "VXS8648MzPO3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Creating ./index/toy\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy/\n",
      "Done ( 0.017806291580200195 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Top 5 most frequent terms:\n",
      "\taa\t9.0=9.0\n",
      "\tbb\t5.0=5.0\n",
      "\tsleep\t5.0=5.0\n",
      "\tcc\t3.0=3.0\n",
      "\tdie\t2.0=2.0\n",
      "\n",
      "\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word'cc': 2\n",
      "Done ( 0.0017817020416259766 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'aa dd'\n",
      "\n",
      "Done ( 0.0024726390838623047 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Creating ./index/urls\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.710953712463379 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6056\n",
      "  Top 5 most frequent terms:\n",
      "\tentropy\t375.0=375.0\n",
      "\tbias\t201.0=201.0\n",
      "\tsystem\t154.0=154.0\n",
      "\tdisplaystyle\t138.0=138.0\n",
      "\theat\t111.0=111.0\n",
      "\n",
      "\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      ": 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word'wikipedia': 3\n",
      "Done ( 0.2142472267150879 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'information probability'\n",
      "2.925590592290311 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.762809667488281 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "\n",
      "2.0993208521164504 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "\n",
      "Done ( 0.0048370361328125 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Creating ./index/docs\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 81.71116900444031 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 117996\n",
      "  Top 5 most frequent terms:\n",
      "\tfamily\t175766.0=175766.0\n",
      "\ttree\t46705.0=46705.0\n",
      "\thistory\t45744.0=45744.0\n",
      "\tgenealogy\t45405.0=45405.0\n",
      "\tsurname\t44965.0=44965.0\n",
      "\n",
      "\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word'seat': 119\n",
      "Done ( 2.4411919116973877 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'obama family tree'\n",
      "16.510318971179043 \t clueweb09-en0010-79-2218.html\n",
      "15.912173443488205 \t clueweb09-en0010-57-32937.html\n",
      "15.833469715479396 \t clueweb09-en0001-02-21241.html\n",
      "15.635280603464125 \t clueweb09-en0008-45-29117.html\n",
      "15.57838614824152 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.03804349899291992 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 3 collections\n",
      "Creating ./index/all_together\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 90.09126543998718 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 117996\n",
      "  Top 5 most frequent terms:\n",
      "\tfamily\t175766.0=175766.0\n",
      "\ttree\t46705.0=46705.0\n",
      "\thistory\t45744.0=45744.0\n",
      "\tgenealogy\t45405.0=45405.0\n",
      "\tsurname\t44965.0=44965.0\n",
      "\n",
      "\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word'seat': 119\n",
      "Done ( 2.185279130935669 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results\n",
      "  WhooshSearcher for query 'obama family tree'\n",
      "16.510318971179043 \t clueweb09-en0010-79-2218.html\n",
      "15.912173443488205 \t clueweb09-en0010-57-32937.html\n",
      "15.833469715479396 \t clueweb09-en0001-02-21241.html\n",
      "15.635280603464125 \t clueweb09-en0008-45-29117.html\n",
      "15.57838614824152 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.02906346321105957 seconds )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def clear (index_path: str):\n",
    "    if os.path.exists(index_path): shutil.rmtree(index_path)\n",
    "    else: print(\"Creating \" + index_path)\n",
    "    os.makedirs(index_path)\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, query: str):\n",
    "    start_time = time.time()\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # Let's create the folder if it did not exist\n",
    "    # and delete the index if it did\n",
    "    clear(index_path)\n",
    "\n",
    "    # We now test building an index\n",
    "    test_build(WhooshBuilder(index_path), collection_paths)\n",
    "\n",
    "    # We now inspect the index\n",
    "    index = WhooshIndex(index_path)\n",
    "    test_read(index, word)\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "    print(\"Checking search results\")\n",
    "    test_search(WhooshSearcher(index_path), query, 5)\n",
    "    #test_search(VSMDotProductSearcher(WhooshIndex(index_path)), query, 5)\n",
    "    #test_search(VSMCosineSearcher(WhooshIndex(index_path)), query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # This function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # When we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    terms = index.all_terms_with_freq()\n",
    "    terms.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    print(\"  Top 5 most frequent terms:\")\n",
    "    for term in terms[0:5]:\n",
    "        print(\"\\t\" + term[0] + \"\\t\" + str(term[1]) + \"=\" + str(index.total_freq(term)))\n",
    "    print()\n",
    "    # More tests\n",
    "    doc_id = 0\n",
    "    print()\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word'\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "test_collection ([collections_root_dir + \"toy/\"], index_root_dir + \"toy\", \"cc\", \"aa dd\")\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls\", \"wikipedia\", \"information probability\")\n",
    "test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs\", \"seat\", \"obama family tree\")\n",
    "test_collection ([collections_root_dir + \"toy/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"all_together\", \"seat\", \"obama family tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTtUTF7j_QdF"
   },
   "source": [
    "### Salida obtenida por el estudiante\n",
    "\n",
    "*(por hacer)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
