{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eq_QfGIGXC_"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Motores de búsqueda e indexación**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 21 / jueves 23 de febrero\n",
    "* Entrega: martes 28 / jueves 30 de marzo (14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Xu Chen Xu <br>\n",
    "Ana Martínez Sabiote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYT0Qlrnoy7l"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFY_K6_pA_J"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
    "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
    "*\tLa implementación de un método de búsqueda proximal.\n",
    "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
    "*\tLa implementación del algoritmo PageRank.\n",
    "\n",
    "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
    "\n",
    "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
    "\n",
    "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El **único buscador que no será intercambiable es el de Whoosh**, que sólo funcionará con sus propios índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
    "\n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la *celda de prueba* así como otros tests adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p2-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
    "\n",
    "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la **celda de prueba** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (ten en cuenta que, aquellos ejercicios que no se hayan realizado, lanzan una excepción que se captura en dicha celda, por lo que no debería ser necesario modificarla).\n",
    "\n",
    "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjq_DVVpDEL"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
    "Las clases parten del código de la práctica anterior.\n",
    "Igual que en la práctica 1, la **celda de prueba** (al final del enunciado) implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
    "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
    "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
    "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería mantenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases genéricas ya implementadas\n",
    "\n",
    "En la siguiente celda de código, se encuentran ya implementadas las clases *Index* y *Builder* de manera que facilite la creación de otros índices a partir de las mismas. \n",
    "\n",
    "Estudia esta implementación y compara las **decisiones de diseño** tomadas con las vuestras en la práctica anterior.\n",
    "Ten en cuenta que las funciones de TF e IDF están **sin implementar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "id": "xAKBQZLLqVXR"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Config(object):\n",
    "  # variables de clase\n",
    "  NORMS_FILE = \"docnorms.dat\"\n",
    "  PATHS_FILE = \"docpaths.dat\"\n",
    "  INDEX_FILE = \"serialindex.dat\"\n",
    "  DICTIONARY_FILE = \"dictionary.dat\"\n",
    "  POSTINGS_FILE = \"postings.dat\"\n",
    "\n",
    "class BasicParser:\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "# Parámetro freq: frecuencia de un término\n",
    "def tf(freq):\n",
    "    if freq > 0:\n",
    "        tf = 1 + math.log(freq, 2)\n",
    "    else:\n",
    "        tf = 0\n",
    "\n",
    "    return tf \n",
    "\n",
    "# Parámetros\n",
    "#    df: doc_freq(term) frecuencia de un término\n",
    "#    n: ndocs() número total de documentos\n",
    "def idf(df, n):\n",
    "    idf = math.log(( (n+1) / (df+0.5)), 2)\n",
    "    \n",
    "    return idf \n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        self.index = index\n",
    "        self.parser = parser\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, dir=None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        if dir: self.open(dir)\n",
    "    def add_doc(self, path):\n",
    "        self.docmap.append(path)  # Assumed to come in order\n",
    "    def doc_path(self, docid):\n",
    "        return self.docmap[docid]\n",
    "    def doc_module(self, docid):\n",
    "        if docid in self.modulemap:\n",
    "            return self.modulemap[docid]\n",
    "        return None\n",
    "    def ndocs(self):\n",
    "        return len(self.docmap)\n",
    "    def doc_freq(self, term):\n",
    "        return len(self.postings(term))\n",
    "    def term_freq(self, term, docID):\n",
    "        post = self.postings(term)\n",
    "        if post is None: return 0\n",
    "        for posting in post:\n",
    "            if posting[0] == docID:\n",
    "                return posting[1]\n",
    "        return 0\n",
    "    def total_freq(self, term):\n",
    "        freq = 0\n",
    "        for posting in self.postings(term):\n",
    "            freq += posting[1]\n",
    "        return freq\n",
    "    def postings(self, term):\n",
    "        # used in more efficient implementations\n",
    "        return list()\n",
    "    def positional_postings(self, term):\n",
    "        # used in positional implementations\n",
    "        return list()\n",
    "    def all_terms(self):\n",
    "        return list()\n",
    "    def save(self, dir):\n",
    "        if not self.modulemap: self.compute_modules()\n",
    "        p = os.path.join(dir, Config.NORMS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.modulemap, f)        \n",
    "    def open(self, dir):\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.NORMS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.modulemap = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "    def compute_modules(self):\n",
    "        for term in self.all_terms():\n",
    "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
    "            post = self.postings(term)\n",
    "            if post is None: continue\n",
    "            for docid, freq in post:\n",
    "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
    "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
    "        for docid in range(self.ndocs()):\n",
    "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
    "\n",
    "import shutil\n",
    "class Builder:\n",
    "    def __init__(self, dir, parser=BasicParser()):\n",
    "        if os.path.exists(dir): shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "        self.parser = parser\n",
    "    def build(self, path):\n",
    "        if zipfile.is_zipfile(path):\n",
    "            self.index_zip(path)\n",
    "        elif os.path.isdir(path):\n",
    "            self.index_dir(path)\n",
    "        else:\n",
    "            self.index_url_file(path)\n",
    "    def index_zip(self, filename):\n",
    "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
    "        for name in sorted(file.namelist()):\n",
    "            with file.open(name, \"r\", force_zip64=True) as f:\n",
    "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "        file.close()\n",
    "    def index_dir(self, dir):\n",
    "        for subdir, dirs, files in os.walk(dir):\n",
    "            for file in sorted(files):\n",
    "                path = os.path.join(dir, file)\n",
    "                with open(path, \"r\") as f:\n",
    "                    self.index_document(path, f.read())\n",
    "    def index_url_file(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            self.index_urls(line.rstrip('\\n') for line in f)\n",
    "    def index_urls(self, urls):\n",
    "        for url in urls:\n",
    "            self.index_document(url, BeautifulSoup(urlopen(url).read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "    def index_document(self, path, text):\n",
    "        raise NotImplementedError # to be implemented by child class\n",
    "    def commit(self):\n",
    "        raise NotImplementedError # to be implemented by child class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de buscador\n",
    "\n",
    "En la siguiente celda se encuentra una implementación de un buscador basado en coseno que es relativamente lento. En los siguientes ejercicios veremos formas de acelerar el proceso (sin cambiar los resultados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "id": "yMoae4N7y38C"
   },
   "outputs": [],
   "source": [
    "# from previous lab\n",
    "class SlowVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for docid in range(self.index.ndocs()):\n",
    "            score = self.score(docid, qterms)\n",
    "            if score:\n",
    "                ranking.push(self.index.doc_path(docid), score)\n",
    "        return ranking\n",
    "\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases Whoosh\n",
    "\n",
    "En la siguiente celda podrás encontrar la adaptación a nuestras interfaces de los índices de Whoosh, en concreto, de tres variantes que permite usar la librería (observa los distintos Schema's usados y qué metodos se han reimplementado en cada caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "id": "I-7gj9Rxx6LD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import whoosh\n",
    "except ModuleNotFoundError:\n",
    "  !pip install whoosh\n",
    "  import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "SimpleDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False))\n",
    "ForwardDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False,vector=Format))\n",
    "PositionalDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=True))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def __init__(self, dir, schema=SimpleDocument):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
    "        self.dir = dir\n",
    "\n",
    "    def index_document(self, p, text):\n",
    "        self.whoosh_writer.add_document(path=p, content=text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshForwardBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, ForwardDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshForwardIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshPositionalBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, PositionalDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshPositionalIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
    "    def total_freq(self, term):\n",
    "        return self.whoosh_reader.frequency(\"content\", term)\n",
    "    def doc_freq(self, term):\n",
    "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_reader.stored_fields(docid)['path']\n",
    "    def ndocs(self):\n",
    "        return self.whoosh_reader.doc_count()\n",
    "    def all_terms(self):\n",
    "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
    "    def postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshForwardIndex(WhooshIndex):\n",
    "    def term_freq(self, term, docID) -> int:\n",
    "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
    "            v = self.whoosh_reader.vector(docID, \"content\")\n",
    "            v.skip_to(term)\n",
    "            if v.id() == term:\n",
    "                return v.value_as(\"frequency\")\n",
    "        return 0\n",
    "\n",
    "class WhooshPositionalIndex(WhooshIndex):\n",
    "    def positional_postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, dir):\n",
    "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
    "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
    "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
    "    def search(self, query, cutoff):\n",
    "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
    "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_index.reader().stored_fields(docid)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3jRLNZmpEk_"
   },
   "source": [
    "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
    "\n",
    "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
    "\n",
    "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be3vDQNxdWbo"
   },
   "source": [
    "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
    "\n",
    "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "id": "ppr9PtZmduql"
   },
   "outputs": [],
   "source": [
    "class TermBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.1) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "        \n",
    "    def search(self, query, cutoff):\n",
    "        scores={}\n",
    "        query_terms=self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            for doc_id, freq in self.index.postings(term):\n",
    "                if doc_id not in scores:\n",
    "                    scores[doc_id]=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                else:\n",
    "                    scores[doc_id]+=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                    \n",
    "        for doc_id, freq in scores.items():\n",
    "            mod = self.index.doc_module(doc_id)\n",
    "\n",
    "            if mod:\n",
    "                scores[doc_id]=freq/mod\n",
    "            if scores[doc_id]:\n",
    "                ranking.push(self.index.doc_path(doc_id), scores[doc_id])\n",
    "                \n",
    "        return ranking\n",
    "\n",
    "        #dic.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YGEGm7haop"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**TermBasedVSMSearcher**: Clase que hereda de Searcher. Implementa el coseno como función de ránking según la búsqueda orientada a términos.\n",
    "**Métodos:**\n",
    "* **search(query, cutoff)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *query*: string que contiene la consulta a buscar.\n",
    "    * *cutoff*: número de resultados que queremos que devuelva el buscador\n",
    "<br/><br/>\n",
    "Hemos implementado el TermBasedVSMSearcher utilizando un diccionario. Hemos iterado sobre los términos de la consulta y para cada término hemos iterado en su lista de postings (que almacena la frecuencia del término en cada documento, identificado por su doc_id) para calcular el score de dicho término asociado a cada documento. La clave del diccionario es el doc_id y el valor es el score. Cuando se calcula el score del documento, si el doc_id no está en el diccionario se añade a él, sino, se incrementa el valor de la clave doc:id ya existente. De esta manera, en el diccionario tenemos el acumulador de score de cada documento tal y como es necesario para la búsqueda orientada a términos. \n",
    "Finalmente, iteramos en el diccionario y obtenemos el score final para cada documento dividiendo por el módulo de cada documento y finalmente añadiéndolos al SearchRanking que se devuelve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ti8qGedgNB"
   },
   "source": [
    "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
    "\n",
    "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "id": "wzZ-6OG0dvwX"
   },
   "outputs": [],
   "source": [
    "class DocBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.2*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        raise NotImplementedError\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xYd4hzhukr"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXHr18Cdl2Q"
   },
   "source": [
    "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
    "\n",
    "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)), es decir, que permita almacenar un **número limitado de documentos** en memoria y su puntuación asociada. \n",
    "\n",
    "Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "id": "MOfT2yZGpMNi"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class SearchRanking:\n",
    "    def __init__(self, cutoff):\n",
    "        self.cutoff = cutoff\n",
    "        self.ranking = list()\n",
    "\n",
    "    def push(self, docid, score):\n",
    "        if len(self.ranking) < self.cutoff:\n",
    "            heapq.heappush(self.ranking, (score, docid))\n",
    "        else:\n",
    "            heapq.heappushpop(self.ranking, (score, docid))\n",
    "\n",
    "    def __iter__(self):\n",
    "        ## sort ranking\n",
    "        orderedRanking = sorted(self.ranking, reverse=True)\n",
    "\n",
    "        # Invertimos la tupla para que el docid sea el primer elemento y el score el segundo\n",
    "        orderedRanking = [(x[1], x[0]) for x in orderedRanking]\n",
    "        return iter(orderedRanking)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDzjUp-hwNZ"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**SearchRanking**: Clase que implementa el heap de ranking\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(cutoff)**: Constructor de la clase.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *cutoff*: número de resultados del ranking.\n",
    "<br/><br/>\n",
    "Para implementar el heap de ránking nos ayudamos del módulo heapq de Python. SearchRanking almacena el cutoff y el ranking en sí: una lista de tuplas score y su respectivo doc_id. El heap tiene el tamaño del cutoff.\n",
    "\n",
    "\n",
    "* **push(docid, score)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *docid*: id del documento que queremos añadir al ranking\n",
    "    * *score*: score del docid pasado como parámetros.\n",
    "<br/><br/>\n",
    " La función push añade una tupla (score, doc_id) al heap. Si el heap no está lleno, se añade la tupla (heappush) conservando la propiedad del heap. Si el heap está lleno, a la hora de hacer push de una tupla, usamos la función heappushpop que es equivalente a hacer un heappush seguido de un heappop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNkPcUjMpNRn"
   },
   "source": [
    "# Ejercicio 2: Índice en RAM (3pt)\n",
    "\n",
    "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
    "\n",
    "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
    "\n",
    "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVzsIg0Zev7a"
   },
   "source": [
    "## Ejercicio 2.1: Estructura de índice\n",
    "\n",
    "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
    "\n",
    "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "id": "VqSKneeSe2bN"
   },
   "outputs": [],
   "source": [
    "class RAMIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        # Diccionario que contendrá los postings de cada término.\n",
    "        # La clave será el término y el valor será una lista de postings,\n",
    "        # donde cada elemento de la lista es una tupla (doc_id, freq).\n",
    "        self.dict_postings = {}\n",
    "\n",
    "        # El constructor del super llamará a open si dir no es None.\n",
    "        super().__init__(dir)\n",
    "\n",
    "    def postings(self, term):\n",
    "        return self.dict_postings[term] if term in self.dict_postings else []\n",
    "\n",
    "    def all_terms(self):\n",
    "        return list(self.dict_postings)\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq))\n",
    "\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        # Guardamos la lista con los paths de los documentos.\n",
    "        p = os.path.join(dir, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "\n",
    "        # Guardamos el diccionario con los postings.\n",
    "        p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings, f)\n",
    "\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "        # Cargamos de disco la lista con los paths de los documentos y\n",
    "        # el diccionario con los postings.\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.PATHS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "            p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.dict_postings = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ucORmwfCh4Um"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**RAMIndex**: Clase que hereda de Index. Construye el índice en RAM, para ello construimos las listas de postings.\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(index_path)**: Constructor de la clase. Recibe como parámetros la ruta donde se encuentra el índice. El índice almacena el diccionario que contendrá los postings de cada término. La clave será el término y el valor será una lista de postings, donde cada elemento de la lista es una tupla (doc_id, freq)\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *index_path*: Ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **postings(term)**: Devuelve la lista de postings del término pasado como parámetro. Si el término no está en el diccionario, se devuelve una lista vacía.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **all_terms()**: Método que devuelve el la lista de tuplas formadas por un término y su lista de postings.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **add_postings(term,doc_id,freq)**: Método que añade un elemento a la lista de postings del término pasado como parámetro. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "    * *doc_id*: id del documento del cual queremos obtener su módulo.\n",
    "    * *freq*: frencuencia del término en el documento identificado por doc_id.\n",
    "<br/><br/>\n",
    "Si el término no está en el diccionario, primero se añade al diccionario y se inicializa su valor a una lista vacía. Después, se añade a la lista de postings del término, la tupla formada por docid y frequencia pasadas como parámetros.\n",
    "\n",
    "\n",
    "* **save(dir)**: Método que guarda en disco los archivos generados por el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se guardarán los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el save de la clase padre Index. Además, guardamos la lista con los paths de los documentos y guardamos el diccionario con las listas de postings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbc9ng8e28p"
   },
   "source": [
    "## Ejercicio 2.2: Construcción del índice\n",
    "\n",
    "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "id": "tHQ4UCf5pTw8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class RAMIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 2.2) #\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir=dir\n",
    "        self.index=RAMIndex(None)\n",
    "\n",
    "    def index_document(self, path, text):\n",
    "        text_terms=self.parser.parse(text)\n",
    "\n",
    "        self.index.add_doc(path)\n",
    "        doc_id=self.index.ndocs()-1\n",
    "\n",
    "        term_freq=Counter(text_terms)\n",
    "        for term, freq in term_freq.items():\n",
    "            self.index.add_posting(term, doc_id, freq)\n",
    "\n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "C_mxRswZh74N"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**RAMIndexBuilder**: Clase que hereda de Builder. Permite construir un índice RAMIndex.\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(dir)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "* **index_document(path, text)**: Método que añade un documento al índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *path*: Ruta del documento que queremos indexar.\n",
    "    * *text*: contenido del documento que queremos indexar.\n",
    "1. Primero aplicamos el parser al texto del documento para obtener la lista de términos del documento.\n",
    "2. Después, añadimos el path del documento con la función add_doc del index. \n",
    "3. Hallamos la frecuencia de cada término en el documento, utilizando la clase Counter del módulo collections de Python.\n",
    "4. Añadimos a la lista de postings cada término con su frecuencia en el documento, con la función add_posting del RAMIndex.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **commit()**: Método que guarda de forma definitiva el índice en el disco. Para ello llama a la función save del RAMIndex\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOWgbqZpV01"
   },
   "source": [
    "# Ejercicio 3: Índice en disco* (1pt)\n",
    "\n",
    "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
    "\n",
    "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
    "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
    "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
    "\n",
    "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
    "\n",
    "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación igual que indice RAM, tenemos que jugar con guardarlo en disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "id": "br7yqFrnpZYl"
   },
   "outputs": [],
   "source": [
    "class DiskIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        # Diccionario que contendrá los postings de cada término.\n",
    "        # La clave será el término y el valor será una lista de postings,\n",
    "        self.dict_postings = {}\n",
    "\n",
    "        # Diccionario que contendrá las posiciones de los postings de cada término.\n",
    "        # La clave será el término y el valor será la posicion del posting en el archivo.\n",
    "        self.dict_postings_pos = {}\n",
    "\n",
    "        self.dir = dir\n",
    "        super().__init__(dir)\n",
    "\n",
    "    def postings(self, term):\n",
    "        # TODO: Solo hay que hacer la situacion en la que los postings se han guardado\n",
    "        # en disco y hay que cargarlos? Es decir, no se puede dar la situacion de que\n",
    "        # creemos el indice en RAM, y sin guardarlo en memoria,\n",
    "        # llamemos a esta funcion.\n",
    "        if self.dict_postings_pos:\n",
    "            postings_list = []\n",
    "\n",
    "            if term in self.dict_postings_pos:\n",
    "                p = os.path.join(self.dir, Config.POSTINGS_FILE)\n",
    "\n",
    "                with open(p, 'r') as f:\n",
    "                    # Nos colocamos en la posición del posting.\n",
    "                    f.seek(self.dict_postings_pos[term])\n",
    "\n",
    "                    # Leemos el posting y lo parseamos obteniendo\n",
    "                    # una lista de strings con el formato \"doc_id,freq\".\n",
    "                    postings = f.readline()\n",
    "\n",
    "                    postings = postings.rstrip('\\n')\n",
    "                    postings = postings.split('|')\n",
    "\n",
    "                    for posting in postings:\n",
    "                        posting = posting.split(',')\n",
    "                        doc_id = int(posting[0])\n",
    "                        freq = int(posting[1])\n",
    "                        postings_list.append((doc_id, freq))\n",
    "            return postings_list\n",
    "        else:\n",
    "            return self.dict_postings[term] if term in self.dict_postings else []\n",
    "\n",
    "    def all_terms(self):\n",
    "        if self.dict_postings_pos:\n",
    "            return list(self.dict_postings_pos)\n",
    "        else:\n",
    "            return list(self.dict_postings)\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq))\n",
    "\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        # Guardamos la lista con los paths de los documentos.\n",
    "        p = os.path.join(dir, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "\n",
    "        # Creamos el fichero con los postings y nos guardamos la posicion del posting.\n",
    "        p = os.path.join(dir, Config.POSTINGS_FILE)\n",
    "        with open(p, 'w') as f:\n",
    "            for term, postings in self.dict_postings.items():\n",
    "                # Obtenemos la posicion del posting en el fichero y la guardamos en el diccionario.\n",
    "                self.dict_postings_pos[term] = f.tell()\n",
    "\n",
    "                # Creamos una lista con los postings, que son strings con el formato \n",
    "                # 'doc_id,freq'\n",
    "                postings_list = []\n",
    "                for posting in postings:\n",
    "                    postings_list.append(str(posting[0]) + ',' + str(posting[1]))\n",
    "                f.write('|'.join(postings_list))\n",
    "                f.write('\\n')\n",
    "\n",
    "        # Guardamos el diccionario con las posiciones de los postings.\n",
    "        p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings_pos, f)\n",
    "\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "\n",
    "        # Cargamos de disco la lista con los paths de los documentos y\n",
    "        # el diccionario con las posiciones de los postings de cada termino.\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.PATHS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "            p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.dict_postings_pos = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "\n",
    "class DiskIndexBuilder(RAMIndexBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir = dir\n",
    "        self.index = DiskIndex(None)\n",
    "    \n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTje0viiM9I"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxuclLwpaM-"
   },
   "source": [
    "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
    "\n",
    "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "id": "e3uq565SpfSA"
   },
   "outputs": [],
   "source": [
    "class ProximitySearcher(Searcher):\n",
    "    # Your new code here (exercise 4*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        raise NotImplementedError\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4hWTstiOIT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPPWV7pepf85"
   },
   "source": [
    "# Ejercicio 5: Índice posicional* (1pt)\n",
    "\n",
    "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "# índice posicional sobre RAM es más fácil. Utilizamos el tercer campo del posting en el que se almacena\n",
    "# la posición del término en el texto. El tercer elemento de la tupla será una lista de posiciones. \n",
    "# Podemos mantener la frecuencia del posting o no (ya que la frecuencia es la longitud de la lista de posiciones)\n",
    "# Método positionalPostings devuelve lista de posiciones\n",
    "# Método postings devuelve como antes doc_id y frecuencia. \n",
    "# Se puede hacer el ejercicio 4 sin hacer el 5. Si se hace el ejercicio 5, el ejercicio 4 prueba que se hace bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "id": "zg8MIMpipih1"
   },
   "outputs": [],
   "source": [
    "class PositionalIndex(RAMIndex):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Note that it may be better to inherit from a different class\n",
    "    # if your index extends a particular type of index\n",
    "    # For example: PositionalIndex(RAMIndex)\n",
    "        \n",
    "    def postings(self, term):\n",
    "        postings=[]\n",
    "        if term in self.dict_postings:\n",
    "            for posting in self.dict_postings[term]:\n",
    "                postings.append((posting[0],posting[1]))\n",
    "            return postings\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        \n",
    "    def positional_postings(self, term):\n",
    "        return self.dict_postings[term] if term in self.dict_postings else []\n",
    "        \n",
    "    \n",
    "    def add_posting(self, term, doc_id, freq, position_list):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq, position_list))\n",
    "        \n",
    "        \n",
    "class PositionalIndexBuilder(RAMIndexBuilder):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Same note as for PositionalIndex\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir=dir\n",
    "        self.index=PositionalIndex(None)\n",
    "\n",
    "        \n",
    "    def index_document(self, path, text):\n",
    "        text_terms=self.parser.parse(text)\n",
    "\n",
    "        self.index.add_doc(path)\n",
    "        doc_id=self.index.ndocs()-1\n",
    "        \n",
    "        \n",
    "        array_terms=np.array(text_terms)\n",
    "        for term in set(text_terms):\n",
    "            position_list=np.where(array_terms == term)[0]\n",
    "            self.index.add_posting(term, doc_id, len(position_list), list(position_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1gukouXiPV3"
   },
   "source": [
    "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HbYGn8pjKZ"
   },
   "source": [
    "# Ejercicio 6: PageRank (1pt)\n",
    "\n",
    "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
    "\n",
    "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "id": "_aQE7SBgpk1S"
   },
   "outputs": [],
   "source": [
    "class PagerankDocScorer():\n",
    "    def __init__(self, graphfile, r, n_iter):\n",
    "        # Format of graphfile:\n",
    "        #  node1 node2\n",
    "\n",
    "        # Diccionario con los documentos/nodos como claves y como valor una lista de nodos\n",
    "        # a los que apunta, que representan las conexiones.\n",
    "        self.dict_connections = {}\n",
    "\n",
    "        # Set que contendra todos los nodos del grafo\n",
    "        self.all_nodes = set()\n",
    "\n",
    "        self.graphfile = graphfile\n",
    "        self.r = r\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.load_graphfile()\n",
    "\n",
    "        # Diccionario con los documentos como claves y como valor su pagerank\n",
    "        self.pagerank_scores = {}\n",
    "        self.calculate_pagerank()\n",
    "\n",
    "    def load_graphfile(self):\n",
    "        \"\"\"\n",
    "        Crea el grafo con todas las conexiones a partir de un fichero.\n",
    "        El formato del fichero es: node1 node2\n",
    "\n",
    "        El grafo estará contenido en el diccionario self.dict_connections, donde las claves son los nodos\n",
    "        y el valor es una lista de nodos, donde cada elemento en dicha lista representa una conexion\n",
    "        desde el nodo usado como clave al nodo que esta en la lista.\n",
    "        \"\"\"\n",
    "        with open(self.graphfile, 'r') as f:\n",
    "            for line in f:\n",
    "                mod_line = line.rstrip('\\n')\n",
    "                node1, node2 = mod_line.split('\\t')\n",
    "\n",
    "                # Add nodes to the set of all nodes\n",
    "                self.all_nodes.add(node1)\n",
    "                self.all_nodes.add(node2)\n",
    "\n",
    "                # Add node2 to the list of connections of node1\n",
    "                if node1 not in self.dict_connections:\n",
    "                    self.dict_connections[node1] = [node2]\n",
    "                else:\n",
    "                    self.dict_connections[node1].append(node2)\n",
    "\n",
    "            # A sink is a node that has no outgoing connections.\n",
    "            sinks = self.all_nodes - set(self.dict_connections.keys())\n",
    "\n",
    "            # Connect sinks to all the nodes.\n",
    "            for sink in sinks:\n",
    "                self.dict_connections[sink] = list(self.all_nodes)\n",
    "\n",
    "    def calculate_pagerank(self):\n",
    "        \"\"\"\n",
    "        Calcula el pagerank de cada nodo del grafo y lo guarda en un diccionario,\n",
    "        donde las claves son los nodos y el valor es su pagerank.\n",
    "\n",
    "        Este método usa el número de iteraciones especificado en el constructor.\n",
    "        \"\"\"\n",
    "        # Initialize pagerank scores\n",
    "        for node in self.all_nodes:\n",
    "            self.pagerank_scores[node] = 1 / len(self.all_nodes)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # Calculate new pagerank scores\n",
    "            new_pagerank_scores = {}\n",
    "            for node in self.all_nodes:\n",
    "                new_pagerank_scores[node] = self.r / len(self.all_nodes)\n",
    "\n",
    "            # Iterate through all the connections\n",
    "            for node1, connections in self.dict_connections.items():\n",
    "                for node2 in connections:\n",
    "                    new_pagerank_scores[node2] += (1 - self.r) * self.pagerank_scores[node1] / len(connections)\n",
    "\n",
    "            # Update pagerank scores\n",
    "            self.pagerank_scores = new_pagerank_scores\n",
    "\n",
    "    def rank(self, cutoff):\n",
    "        \"\"\"\n",
    "        Devuelve un ranking de cutoff documentos, ordenados por su pagerank.\n",
    "        \"\"\"\n",
    "        # Create SearchRanking\n",
    "        ranking = SearchRanking(cutoff)\n",
    "\n",
    "        for node, score in self.pagerank_scores.items():\n",
    "            ranking.push(node, score)\n",
    "\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ZT7seCiQiT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zXhPtFzon72"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que esta celda funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "id": "fTdDacCRn0u6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.01503300666809082 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.01457977294921875 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.017156600952148438 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.001100301742553711 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0015900135040283203 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0015914440155029297 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0006151199340820312 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0004930496215820312 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.00046443939208984375 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 9.608268737792969e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0003254413604736328 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 5.698204040527344e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa dd\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0010383129119873047 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0009768009185791016 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0006818771362304688 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0008635520935058594 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00027751922607421875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0006287097930908203 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.000339508056640625 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007531642913818359 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002663135528564453 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 7.987022399902344e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.5789947509765625e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007221698760986328 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002384185791015625 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 7.891654968261719e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.793571472167969e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0029268264770507812 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.002132415771484375 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0017507076263427734 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007140636444091797 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002033710479736328 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004203319549560547 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00029206275939941406 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004620552062988281 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00017690658569335938 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.413459777832031e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.078315734863281e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005891323089599609 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00021457672119140625 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.937980651855469e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.054473876953125e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.013918399810791016 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.14094281196594238 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.01156926155090332 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0008683204650878906 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0016303062438964844 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0022978782653808594 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0005614757537841797 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0005881786346435547 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0004413127899169922 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 6.008148193359375e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.00034165382385253906 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 5.745887756347656e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.002533435821533203 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0013234615325927734 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.001378774642944336 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004725456237792969 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002987384796142578 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005986690521240234 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.000396728515625 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008566379547119141 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005676746368408203 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00013136863708496094 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.988380432128906e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006511211395263672 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00028324127197265625 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 6.747245788574219e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 5.054473876953125e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.002141237258911133 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0016083717346191406 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0013191699981689453 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008771419525146484 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005240440368652344 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004711151123046875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002777576446533203 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004086494445800781 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00025010108947753906 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 5.91278076171875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 4.506111145019531e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005390644073486328 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003960132598876953 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001373291015625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.678436279296875e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 2 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.014474153518676758 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.014185428619384766 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.011828184127807617 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0009241104125976562 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0012333393096923828 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.002321004867553711 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0006244182586669922 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0004870891571044922 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0004420280456542969 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 5.4836273193359375e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0003018379211425781 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 5.626678466796875e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0025298595428466797 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.004854440689086914 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0030176639556884766 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0014598369598388672 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00039649009704589844 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0009953975677490234 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00042176246643066406 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0012044906616210938 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005288124084472656 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00011277198791503906 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.843971252441406e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0010950565338134766 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004229545593261719 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00012111663818359375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.630752563476562e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00413966178894043 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0027692317962646484 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003258943557739258 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0012726783752441406 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003650188446044922 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0009696483612060547 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006651878356933594 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.002816915512084961 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005848407745361328 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00016307830810546875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00011920928955078125 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0014386177062988281 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00046753883361816406 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00012135505676269531 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.440017700195312e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.5578575134277344 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.3411457538604736 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 7.882311105728149 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.72666335105896 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.7008039951324463 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.438650608062744 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6063\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 25.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.017573833465576172 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6063\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 25.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.02081608772277832 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6063\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 25.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.018944501876831055 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5950\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 29 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.0002334117889404297 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5950\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 29 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.0004992485046386719 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5950\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 29 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.000244140625 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for information probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0034668445587158203 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0040438175201416016 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0034058094024658203 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00074005126953125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003228187561035156 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0017635822296142578 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003275871276855469 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.000652313232421875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003230571746826172 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.939338684082031e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.151199340820312e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.000732421875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00035643577575683594 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02454872585909962 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01734627659968144 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259684 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.915496826171875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02454872585909962 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01734627659968144 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259684 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.365776062011719e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for probability information\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003281116485595703 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00327301025390625 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0038483142852783203 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007505416870117188 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00032258033752441406 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0017788410186767578 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003342628479003906 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006077289581298828 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003142356872558594 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.867813110351562e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.9604644775390625e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005817413330078125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003390312194824219 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02454872585909962 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01734627659968144 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259684 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.511543273925781e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02454872585909962 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01734627659968144 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259684 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.082389831542969e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for higher probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.885385770856463 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527726832868973 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.731024719683858 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00470280647277832 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "2.885385770856463 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527726832868973 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.731024719683858 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0034453868865966797 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "2.885385770856463 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527726832868973 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.731024719683858 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.007042646408081055 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0008993148803710938 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003833770751953125 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0021240711212158203 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005996227264404297 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006256103515625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003981590270996094 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.028102962822793218 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461795 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.796287536621094e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.028102962822793218 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461795 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.081031799316406e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.028102962822793218 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461795 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0010747909545898438 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.028102962822793218 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461795 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003662109375 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.02810296282279324 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461896 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766207 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.320808410644531e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.02810296282279324 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461896 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766207 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.413459777832031e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/urls.txt'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 2.731545925140381 seconds ---\n",
      "\tWhooshForwardIndex: 2.7536025047302246 seconds ---\n",
      "\tWhooshPositionalIndex: 2.7505621910095215 seconds ---\n",
      "\tRAMIndex: 2.055804491043091 seconds ---\n",
      "\tDiskIndex: 1.8024225234985352 seconds ---\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.002762317657470703 seconds ---\n",
      "\tWhooshForwardIndex: 0.0015835762023925781 seconds ---\n",
      "\tWhooshPositionalIndex: 0.0015916824340820312 seconds ---\n",
      "\tRAMIndex: 0.005717277526855469 seconds ---\n",
      "\tDiskIndex: 0.0013380050659179688 seconds ---\n",
      "  Disk space...\n",
      "\tWhooshIndex: 1090828 space ---\n",
      "\tWhooshForwardIndex: 1168122 space ---\n",
      "\tWhooshPositionalIndex: 1247960 space ---\n",
      "\tRAMIndex: 126677 space ---\n",
      "\tDiskIndex: 110237 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'information probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004117012023925781 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.006154298782348633 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.001043558120727539 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0010738372802734375 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00032639503479003906 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.00034427642822265625 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.914138793945312e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 8.988380432128906e-05 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005130767822265625 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.0005395412445068359 seconds ---\n",
      "DocBasedVSMSearcher still not implemented\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'probability information'\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.926100147768084 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763343927254799 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100579261943137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003470897674560547 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.0050508975982666016 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0009109973907470703 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0009326934814453125 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433210889496527 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017419565620326 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009595245350531557 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003190040588378906 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0003361701965332031 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.961822509765625e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 8.893013000488281e-05 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.0245487258590996 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.0173462765996813 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009528742278259566 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004329681396484375 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.0004467964172363281 seconds ---\n",
      "DocBasedVSMSearcher still not implemented\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'higher probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.885385770856463 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527726832868973 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.731024719683858 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0040357112884521484 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.005662202835083008 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0009894371032714844 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0010137557983398438 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027934257544690692 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012269943814119838 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005733099127812443 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005164146423339844 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0005397796630859375 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.028102962822793218 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461795 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.940696716308594e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.00012111663818359375 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.028102962822793218 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012181034608461795 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005693363957766137 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005059242248535156 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.0005233287811279297 seconds ---\n",
      "DocBasedVSMSearcher still not implemented\n",
      "----------------------------\n",
      "Testing PageRank\n",
      "0.3846153846153846 \t c\n",
      "0.3589743589743589 \t a\n",
      "0.2564102564102564 \t b\n",
      "\n",
      "--- Pagerank with toy_graph_1 0.000331878662109375 seconds ---\n",
      "0.368421052631579 \t b\n",
      "0.3157894736842105 \t c\n",
      "0.3157894736842105 \t a\n",
      "\n",
      "--- Pagerank with toy_graph_2 0.00027942657470703125 seconds ---\n",
      "0.033333748623865 \t clueweb09-en0000-06-11938.html\n",
      "0.032822385415072756 \t clueweb09-en0000-01-22977.html\n",
      "0.03264169111517307 \t clueweb09-en0000-06-11940.html\n",
      "0.03170456979307948 \t clueweb09-en0000-06-11932.html\n",
      "0.031598005967786835 \t clueweb09-en0000-01-27969.html\n",
      "\n",
      "--- Pagerank with simulated links for doc1k 0.3984851837158203 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # We now test building different implementations of an index\n",
    "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_paths)\n",
    "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_paths)\n",
    "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_paths)\n",
    "    try:\n",
    "        test_build(RAMIndexBuilder(index_path + \"ram\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(DiskIndexBuilder(index_path + \"disk\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"PositionalIndexBuilder still not implemented\")\n",
    "\n",
    "    def catch_index(func, name, *args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except NotImplementedError:\n",
    "            print(name + \" still not implemented (index)\")\n",
    "            return None\n",
    "\n",
    "    # We now inspect all the implementations\n",
    "    indices = [\n",
    "            WhooshIndex(index_path + \"whoosh\"),\n",
    "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
    "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
    "            catch_index(lambda: RAMIndex(index_path + \"ram\"), \"RAMIndex\"),\n",
    "            catch_index(lambda: DiskIndex(index_path + \"disk\"), \"DiskIndex\"),\n",
    "            catch_index(lambda: PositionalIndex(index_path + \"pos\"), \"PositionalIndex\"),\n",
    "            ]\n",
    "    for index in indices:\n",
    "        if index:\n",
    "            test_read(index, word)\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Checking search results for %s\" % (query))\n",
    "        # Whoosh searcher can only work with its own indices\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        try:\n",
    "            test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher still not implemented\")\n",
    "        for index in indices:\n",
    "            if index:\n",
    "                # our searchers should work with any other index\n",
    "                test_search(SlowVSMSearcher(index), index, query, 5)\n",
    "                try:\n",
    "                    test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"TermBasedVSMSearcher still not implemented\")\n",
    "                try:\n",
    "                    test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"DocBasedVSMSearcher still not implemented\")\n",
    "        try:\n",
    "            test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher or PositionalIndex still not implemented\")\n",
    "\n",
    "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
    "    indices = list()\n",
    "\n",
    "    if analyse_performance:\n",
    "        # let's analyse index performance\n",
    "        test_index_performance(collection_paths, index_path)\n",
    "        # let's analyse search performance\n",
    "        for query in queries:\n",
    "            test_search_performance(collection_paths, index_path, query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # this function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # when we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    # more tests\n",
    "    doc_id = 0\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, index, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def disk_space(index_path: str) -> int:\n",
    "    space = 0\n",
    "    if os.path.isdir(index_path):\n",
    "        for f in os.listdir(index_path):\n",
    "            p = os.path.join(index_path, f)\n",
    "            if os.path.isfile(p):\n",
    "                space += os.path.getsize(p)\n",
    "    return space\n",
    "\n",
    "def test_index_performance (collection_paths: list, base_index_path: str):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing index performance on \" + str(collection_paths) + \" document collection\")\n",
    "\n",
    "    print(\"  Build time...\")\n",
    "    start_time = time.time()\n",
    "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = RAMIndexBuilder(base_index_path + \"ram\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = DiskIndexBuilder(base_index_path + \"disk\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "\n",
    "    print(\"  Load time...\")\n",
    "    start_time = time.time()\n",
    "    WhooshIndex(base_index_path + \"whoosh\")\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        RAMIndex(base_index_path + \"ram\")\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        DiskIndex(base_index_path + \"disk\")\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "\n",
    "    print(\"  Disk space...\")\n",
    "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
    "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
    "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
    "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
    "    print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
    "\n",
    "\n",
    "def test_search_performance (collection_paths: list, base_index_path: str, query: str, cutoff: int):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing search performance on \" + str(collection_paths) + \" document collection with query: '\" + query + \"'\")\n",
    "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
    "    try:\n",
    "        ram_index = RAMIndex(base_index_path + \"ram\")\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "        ram_index = None\n",
    "    try:\n",
    "        disk_index = DiskIndex(base_index_path + \"disk\")\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "        disk_index = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
    "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # let's test some combinations of ranking + index implementations\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "        print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if ram_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
    "            print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DocBasedVSMSearcher still not implemented\")\n",
    "\n",
    "def test_pagerank(graphs_root_dir, cutoff):\n",
    "    print(\"----------------------------\")\n",
    "    # we separate this function because it cannot work with all the collections\n",
    "    print(\"Testing PageRank\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "test_collection ([collections_root_dir + \"toy1/\"], index_root_dir + \"toy1/\", \"cc\", [\"aa dd\", \"aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy2/\"], index_root_dir + \"toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy1/\", collections_root_dir + \"toy2/\"], index_root_dir + \"toys/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"toy2/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"three_collections/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs10k.zip\"], index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], False)\n",
    "test_pagerank(\"./collections/\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JhJJSFiSl5"
   },
   "source": [
    "### Resumen de coste y rendimiento\n",
    "\n",
    "Hay que analizar las **diferencias de rendimiento** observadas entre las diferentes implementaciones que se han creado y probado para cada componente.\n",
    "\n",
    "En concreto, hay que reportar tiempo de indexado, consumo máximo de RAM y espacio en disco al construir el índice, y el tiempo de carga y consumo máximo de RAM al cargar el índice para cada una de las colecciones utilizadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "|      | Construcción | del | índice | Carga del | índice |\n",
    "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
    "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
    "| toy1 | | | | | |\n",
    "| toy2 | | | | | |\n",
    "| toys | | | | | |\n",
    "| 1K | | | | | |\n",
    "| 10K | | | | | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
