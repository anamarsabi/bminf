{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eq_QfGIGXC_"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Motores de búsqueda e indexación**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 21 / jueves 23 de febrero\n",
    "* Entrega: martes 28 / jueves 30 de marzo (14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Xu Chen Xu <br>\n",
    "Ana Martínez Sabiote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYT0Qlrnoy7l"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFY_K6_pA_J"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
    "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
    "*\tLa implementación de un método de búsqueda proximal.\n",
    "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
    "*\tLa implementación del algoritmo PageRank.\n",
    "\n",
    "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
    "\n",
    "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
    "\n",
    "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El **único buscador que no será intercambiable es el de Whoosh**, que sólo funcionará con sus propios índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
    "\n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la *celda de prueba* así como otros tests adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p2-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
    "\n",
    "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la **celda de prueba** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (ten en cuenta que, aquellos ejercicios que no se hayan realizado, lanzan una excepción que se captura en dicha celda, por lo que no debería ser necesario modificarla).\n",
    "\n",
    "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjq_DVVpDEL"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
    "Las clases parten del código de la práctica anterior.\n",
    "Igual que en la práctica 1, la **celda de prueba** (al final del enunciado) implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
    "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
    "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
    "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería mantenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases genéricas ya implementadas\n",
    "\n",
    "En la siguiente celda de código, se encuentran ya implementadas las clases *Index* y *Builder* de manera que facilite la creación de otros índices a partir de las mismas. \n",
    "\n",
    "Estudia esta implementación y compara las **decisiones de diseño** tomadas con las vuestras en la práctica anterior.\n",
    "Ten en cuenta que las funciones de TF e IDF están **sin implementar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xAKBQZLLqVXR"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Config(object):\n",
    "  # variables de clase\n",
    "  NORMS_FILE = \"docnorms.dat\"\n",
    "  PATHS_FILE = \"docpaths.dat\"\n",
    "  INDEX_FILE = \"serialindex.dat\"\n",
    "  DICTIONARY_FILE = \"dictionary.dat\"\n",
    "  POSTINGS_FILE = \"postings.dat\"\n",
    "\n",
    "class BasicParser:\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "# Parámetro freq: frecuencia de un término\n",
    "def tf(freq):\n",
    "    if freq > 0:\n",
    "        tf = 1 + math.log(freq, 2)\n",
    "    else:\n",
    "        tf = 0\n",
    "\n",
    "    return tf \n",
    "\n",
    "# Parámetros\n",
    "#    df: doc_freq(term) frecuencia de un término\n",
    "#    n: ndocs() número total de documentos\n",
    "def idf(df, n):\n",
    "    idf = math.log(( (n+1) / (df+0.5)), 2)\n",
    "    \n",
    "    return idf \n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        self.index = index\n",
    "        self.parser = parser\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, dir=None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        if dir: self.open(dir)\n",
    "    def add_doc(self, path):\n",
    "        self.docmap.append(path)  # Assumed to come in order\n",
    "    def doc_path(self, docid):\n",
    "        return self.docmap[docid]\n",
    "    def doc_module(self, docid):\n",
    "        if docid in self.modulemap:\n",
    "            return self.modulemap[docid]\n",
    "        return None\n",
    "    def ndocs(self):\n",
    "        return len(self.docmap)\n",
    "    def doc_freq(self, term):\n",
    "        return len(self.postings(term))\n",
    "    def term_freq(self, term, docID):\n",
    "        post = self.postings(term)\n",
    "        if post is None: return 0\n",
    "        for posting in post:\n",
    "            if posting[0] == docID:\n",
    "                return posting[1]\n",
    "        return 0\n",
    "    def total_freq(self, term):\n",
    "        freq = 0\n",
    "        for posting in self.postings(term):\n",
    "            freq += posting[1]\n",
    "        return freq\n",
    "    def postings(self, term):\n",
    "        # used in more efficient implementations\n",
    "        return list()\n",
    "    def positional_postings(self, term):\n",
    "        # used in positional implementations\n",
    "        return list()\n",
    "    def all_terms(self):\n",
    "        return list()\n",
    "    def save(self, dir):\n",
    "        if not self.modulemap: self.compute_modules()\n",
    "        p = os.path.join(dir, Config.NORMS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.modulemap, f)        \n",
    "    def open(self, dir):\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.NORMS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.modulemap = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "    def compute_modules(self):\n",
    "        for term in self.all_terms():\n",
    "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
    "            post = self.postings(term)\n",
    "            if post is None: continue\n",
    "            for docid, freq in post:\n",
    "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
    "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
    "        for docid in range(self.ndocs()):\n",
    "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
    "\n",
    "import shutil\n",
    "class Builder:\n",
    "    def __init__(self, dir, parser=BasicParser()):\n",
    "        if os.path.exists(dir): shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "        self.parser = parser\n",
    "    def build(self, path):\n",
    "        if zipfile.is_zipfile(path):\n",
    "            self.index_zip(path)\n",
    "        elif os.path.isdir(path):\n",
    "            self.index_dir(path)\n",
    "        else:\n",
    "            self.index_url_file(path)\n",
    "    def index_zip(self, filename):\n",
    "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
    "        for name in sorted(file.namelist()):\n",
    "            with file.open(name, \"r\", force_zip64=True) as f:\n",
    "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "        file.close()\n",
    "    def index_dir(self, dir):\n",
    "        for subdir, dirs, files in os.walk(dir):\n",
    "            for file in sorted(files):\n",
    "                path = os.path.join(dir, file)\n",
    "                with open(path, \"r\") as f:\n",
    "                    self.index_document(path, f.read())\n",
    "    def index_url_file(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            self.index_urls(line.rstrip('\\n') for line in f)\n",
    "    def index_urls(self, urls):\n",
    "        for url in urls:\n",
    "            self.index_document(url, BeautifulSoup(urlopen(url).read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "    def index_document(self, path, text):\n",
    "        raise NotImplementedError # to be implemented by child class\n",
    "    def commit(self):\n",
    "        raise NotImplementedError # to be implemented by child class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de buscador\n",
    "\n",
    "En la siguiente celda se encuentra una implementación de un buscador basado en coseno que es relativamente lento. En los siguientes ejercicios veremos formas de acelerar el proceso (sin cambiar los resultados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yMoae4N7y38C"
   },
   "outputs": [],
   "source": [
    "# from previous lab\n",
    "class SlowVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for docid in range(self.index.ndocs()):\n",
    "            score = self.score(docid, qterms)\n",
    "            if score:\n",
    "                ranking.push(self.index.doc_path(docid), score)\n",
    "        return ranking\n",
    "\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases Whoosh\n",
    "\n",
    "En la siguiente celda podrás encontrar la adaptación a nuestras interfaces de los índices de Whoosh, en concreto, de tres variantes que permite usar la librería (observa los distintos Schema's usados y qué metodos se han reimplementado en cada caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "I-7gj9Rxx6LD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import whoosh\n",
    "except ModuleNotFoundError:\n",
    "  !pip install whoosh\n",
    "  import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "SimpleDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False))\n",
    "ForwardDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False,vector=Format))\n",
    "PositionalDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=True))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def __init__(self, dir, schema=SimpleDocument):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
    "        self.dir = dir\n",
    "\n",
    "    def index_document(self, p, text):\n",
    "        self.whoosh_writer.add_document(path=p, content=text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshForwardBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, ForwardDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshForwardIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshPositionalBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, PositionalDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshPositionalIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
    "    def total_freq(self, term):\n",
    "        return self.whoosh_reader.frequency(\"content\", term)\n",
    "    def doc_freq(self, term):\n",
    "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_reader.stored_fields(docid)['path']\n",
    "    def ndocs(self):\n",
    "        return self.whoosh_reader.doc_count()\n",
    "    def all_terms(self):\n",
    "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
    "    def postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshForwardIndex(WhooshIndex):\n",
    "    def term_freq(self, term, docID) -> int:\n",
    "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
    "            v = self.whoosh_reader.vector(docID, \"content\")\n",
    "            v.skip_to(term)\n",
    "            if v.id() == term:\n",
    "                return v.value_as(\"frequency\")\n",
    "        return 0\n",
    "\n",
    "class WhooshPositionalIndex(WhooshIndex):\n",
    "    def positional_postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, dir):\n",
    "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
    "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
    "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
    "    def search(self, query, cutoff):\n",
    "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
    "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_index.reader().stored_fields(docid)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3jRLNZmpEk_"
   },
   "source": [
    "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
    "\n",
    "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
    "\n",
    "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be3vDQNxdWbo"
   },
   "source": [
    "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
    "\n",
    "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ppr9PtZmduql"
   },
   "outputs": [],
   "source": [
    "class TermBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.1) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "        \n",
    "    def search(self, query, cutoff):\n",
    "        scores={}\n",
    "        query_terms=self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            for doc_id, freq in self.index.postings(term):\n",
    "                if doc_id not in scores:\n",
    "                    scores[doc_id]=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                else:\n",
    "                    scores[doc_id]+=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                    \n",
    "        for doc_id, freq in scores.items():\n",
    "            mod = self.index.doc_module(doc_id)\n",
    "\n",
    "            if mod:\n",
    "                scores[doc_id]=freq/mod\n",
    "            if scores[doc_id]:\n",
    "                ranking.push(self.index.doc_path(doc_id), scores[doc_id])\n",
    "\n",
    "        return ranking\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YGEGm7haop"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**TermBasedVSMSearcher**: Clase que hereda de Searcher. Implementa el coseno como función de ránking según la búsqueda orientada a términos.\n",
    "**Métodos:**\n",
    "* **search(query, cutoff)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *query*: string que contiene la consulta a buscar.\n",
    "    * *cutoff*: número de resultados que queremos que devuelva el buscador\n",
    "<br/><br/>\n",
    "Hemos implementado el TermBasedVSMSearcher utilizando un diccionario. Hemos iterado sobre los términos de la consulta y para cada término hemos iterado en su lista de postings (que almacena la frecuencia del término en cada documento, identificado por su doc_id) para calcular el score de dicho término asociado a cada documento. La clave del diccionario es el doc_id y el valor es el score. Cuando se calcula el score del documento, si el doc_id no está en el diccionario se añade a él, sino, se incrementa el valor de la clave doc:id ya existente. De esta manera, en el diccionario tenemos el acumulador de score de cada documento tal y como es necesario para la búsqueda orientada a términos. \n",
    "Finalmente, iteramos en el diccionario y obtenemos el score final para cada documento dividiendo por el módulo de cada documento y finalmente añadiéndolos al SearchRanking que se devuelve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ti8qGedgNB"
   },
   "source": [
    "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
    "\n",
    "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "wzZ-6OG0dvwX"
   },
   "outputs": [],
   "source": [
    "class DocBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.2*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        heap_postings=[]\n",
    "        query_terms=self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "\n",
    "        # Diccionario que contendrá los postings de cada término\n",
    "        # ordenados por doc_id. La clave es el término y el valor\n",
    "        # es una lista de tuplas (doc_id, tf-idf).\n",
    "        terms_postings_dict={}\n",
    "\n",
    "        ndocs_index = self.index.ndocs()\n",
    "\n",
    "        # Primero calculamos el tf-idf para cada término y cada documento\n",
    "        # y los guardamos en el diccionario, ordenando los postings por\n",
    "        # doc_id.\n",
    "        for term in query_terms:\n",
    "            postings = self.index.postings(term)\n",
    "\n",
    "            doc_freq_of_term = self.index.doc_freq(term)\n",
    "            score_postings = []\n",
    "\n",
    "            # Calculamos el tf-idf para cada termino y cada documento\n",
    "            for posting in postings:\n",
    "                doc_id = posting[0]\n",
    "                freq = posting[1]\n",
    "                score = tf(freq) * idf(doc_freq_of_term, ndocs_index)\n",
    "                score_postings.append((doc_id, score))\n",
    "\n",
    "            # Guardamos la lista de postings ordenada por doc_id\n",
    "            # en el diccionario.\n",
    "            terms_postings_dict[term] = sorted(score_postings, key=lambda x: x[0])\n",
    "\n",
    "        # Inicializamos el heap con los primeros elementos de cada\n",
    "        # lista de postings. El heap contendrá tuplas (doc_id, score, term).\n",
    "        # El term es necesario para saber qué término ha sido el que ha\n",
    "        # introducido el elemento en el heap y asi poder pushear el siguiente\n",
    "        # elemento de la lista de postings correspondiente.\n",
    "        for term in query_terms:\n",
    "            elem = terms_postings_dict[term].pop(0)\n",
    "            elem = (elem[0], elem[1], term)\n",
    "            heapq.heappush(heap_postings, elem)\n",
    "\n",
    "        # Primero procesamos el primer elemento del heap para inicializar\n",
    "        # la variable current_doc_id.\n",
    "        posting = heapq.heappop(heap_postings)\n",
    "        current_doc_id = posting[0]\n",
    "\n",
    "        # El score es el tf-idf del documento\n",
    "        doc_score = posting[1]\n",
    "        term = posting[2]\n",
    "\n",
    "        if len(terms_postings_dict[term]) > 0:\n",
    "            elem = terms_postings_dict[term].pop(0)\n",
    "            elem = (elem[0], elem[1], term)\n",
    "\n",
    "            heapq.heappush(heap_postings, elem)\n",
    "\n",
    "        # Vamos sacando elementos del heap y procesandolos hasta\n",
    "        # que el heap esté vacío.\n",
    "        while len(heap_postings) > 0:\n",
    "            posting = heapq.heappop(heap_postings)\n",
    "\n",
    "            aux_doc_id = posting[0]\n",
    "            aux_score = posting[1]\n",
    "            aux_term = posting[2]\n",
    "\n",
    "            # Si el documento del posting que acabamos de sacar del heap\n",
    "            # es el mismo que el que estamos procesando, sumamos su score.\n",
    "            if aux_doc_id == current_doc_id:\n",
    "                doc_score += aux_score\n",
    "\n",
    "            # Si no es el mismo, dividimos el score calculado por el modulo\n",
    "            # para obtener el coseno y lo insertamos en el ranking.\n",
    "            else:\n",
    "                mod = self.index.doc_module(current_doc_id)\n",
    "\n",
    "                if mod:\n",
    "                    doc_score=doc_score/mod\n",
    "                if score:\n",
    "                    ranking.push(self.index.doc_path(current_doc_id), doc_score)\n",
    "\n",
    "                current_doc_id = aux_doc_id\n",
    "                doc_score = aux_score\n",
    "\n",
    "            # Si quedan postings en el término que acabamos de procesar,\n",
    "            # insertamos el siguiente en el heap.\n",
    "            if len(terms_postings_dict[aux_term]) > 0:\n",
    "                elem = terms_postings_dict[aux_term].pop(0)\n",
    "                elem = (elem[0], elem[1], aux_term)\n",
    "\n",
    "                heapq.heappush(heap_postings, elem)\n",
    "\n",
    "        # Insertamos en el ranking el ultimo documento que estabamos procesando\n",
    "        mod = self.index.doc_module(current_doc_id)\n",
    "\n",
    "        if mod:\n",
    "            doc_score=doc_score/mod\n",
    "        if score:\n",
    "            ranking.push(self.index.doc_path(current_doc_id), doc_score)\n",
    "\n",
    "        return ranking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xYd4hzhukr"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**DocBasedVSMSearcher**: Clase que hereda de Searcher. Implementa el coseno como función de ránking según la búsqueda orientada a documentos.\n",
    "**Métodos:**\n",
    "* **search(query, cutoff)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *query*: string que contiene la consulta a buscar.\n",
    "    * *cutoff*: número de resultados que queremos que devuelva el buscador\n",
    "<br/><br/>\n",
    "Para la búsqueda basada en documentos hemos usado un heap de postings.\n",
    "\n",
    "En primer lugar lo que hacemos es crear un diccionario, en el que la clave es un término de la consulta y el valor es una lista de postings, en el que cada posting es una tupla (doc_id, tf-idf). Esto lo hacemos para poder iterar cosecuencialmente en las listas de postings de cada término de la consulta.\n",
    "\n",
    "Después, metemos en el heap de postings un elemento para cada término de la consulta. Los elementos del heap son tuplas (doc_id, tf-idf, term). Necesitamos guardar también el término del cual procede el postings para poder meter en el heap los siguientes postings del mismo término.\n",
    "\n",
    "A continuación, vamos sacando elementos del heap de postings y sumando los tf-idf de cada documento. Cuando sacamos un elemento del heap, si el siguiente elemento del heap es del mismo documento, sumamos el tf-idf. Si es de un documento distinto, querrá decir que ya hemos terminado de calcular el score de ese documento, por lo que calculamos su módulo y lo metemos en el Ranking. Al sacar un elemento del heap, metemos en el heap el siguiente elemento de la lista de postings del mismo término (si no quedan más no metemos nada). Y así sucesivamente hasta que el heap esté vacío."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXHr18Cdl2Q"
   },
   "source": [
    "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
    "\n",
    "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)), es decir, que permita almacenar un **número limitado de documentos** en memoria y su puntuación asociada. \n",
    "\n",
    "Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "MOfT2yZGpMNi"
   },
   "outputs": [],
   "source": [
    "class SearchRanking:\n",
    "    def __init__(self, cutoff):\n",
    "        self.cutoff = cutoff\n",
    "        self.ranking = list()\n",
    "\n",
    "    def push(self, docid, score):\n",
    "        if len(self.ranking) < self.cutoff:\n",
    "            heapq.heappush(self.ranking, (score, docid))\n",
    "        else:\n",
    "            heapq.heappushpop(self.ranking, (score, docid))\n",
    "\n",
    "    def __iter__(self):\n",
    "        ## sort ranking\n",
    "        orderedRanking = sorted(self.ranking, reverse=True)\n",
    "\n",
    "        # Invertimos la tupla para que el docid sea el primer elemento y el score el segundo\n",
    "        orderedRanking = [(x[1], x[0]) for x in orderedRanking]\n",
    "        return iter(orderedRanking)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDzjUp-hwNZ"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**SearchRanking**: Clase que implementa el heap de ranking\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(cutoff)**: Constructor de la clase.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *cutoff*: número de resultados del ranking.\n",
    "<br/><br/>\n",
    "Para implementar el heap de ránking nos ayudamos del módulo heapq de Python. SearchRanking almacena el cutoff y el ranking en sí: una lista de tuplas score y su respectivo doc_id. El heap tiene el tamaño del cutoff.\n",
    "\n",
    "\n",
    "* **push(docid, score)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *docid*: id del documento que queremos añadir al ranking\n",
    "    * *score*: score del docid pasado como parámetros.\n",
    "<br/><br/>\n",
    " La función push añade una tupla (score, doc_id) al heap. Se añade con el score como primer elemento de la tupla para que el heap use el score para ordenar. Si el heap no está lleno, se añade la tupla (heappush) conservando la propiedad del heap. Si el heap está lleno, a la hora de hacer push de una tupla, usamos la función heappushpop que es equivalente a hacer un heappush seguido de un heappop.\n",
    "\n",
    " * **__iter__()**:\n",
    "<br/><br/>\n",
    "Ordena el heap por score de forma descendente y devuelve un iterador en el que cada elemento es una tupla (doc_id,score). Se vuelve a dar la vuelta a la tupla para tener el doc_id como primer elemento y el score como segundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNkPcUjMpNRn"
   },
   "source": [
    "# Ejercicio 2: Índice en RAM (3pt)\n",
    "\n",
    "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
    "\n",
    "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
    "\n",
    "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVzsIg0Zev7a"
   },
   "source": [
    "## Ejercicio 2.1: Estructura de índice\n",
    "\n",
    "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
    "\n",
    "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "VqSKneeSe2bN"
   },
   "outputs": [],
   "source": [
    "class RAMIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        # Diccionario que contendrá los postings de cada término.\n",
    "        # La clave será el término y el valor será una lista de postings,\n",
    "        # donde cada elemento de la lista es una tupla (doc_id, freq).\n",
    "        self.dict_postings = {}\n",
    "\n",
    "        # El constructor del super llamará a open si dir no es None.\n",
    "        super().__init__(dir)\n",
    "\n",
    "    def postings(self, term):\n",
    "        return self.dict_postings[term] if term in self.dict_postings else []\n",
    "\n",
    "    def all_terms(self):\n",
    "        return list(self.dict_postings)\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq))\n",
    "\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        # Guardamos la lista con los paths de los documentos.\n",
    "        p = os.path.join(dir, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "\n",
    "        # Guardamos el diccionario con los postings.\n",
    "        p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings, f)\n",
    "\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "        # Cargamos de disco la lista con los paths de los documentos y\n",
    "        # el diccionario con los postings.\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.PATHS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "            p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.dict_postings = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ucORmwfCh4Um"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**RAMIndex**: Clase que hereda de Index. Construye el índice en RAM, para ello construimos las listas de postings.\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(index_path)**: Constructor de la clase. Recibe como parámetros la ruta donde se encuentra el índice. El índice almacena el diccionario que contendrá los postings de cada término. La clave será el término y el valor será una lista de postings, donde cada elemento de la lista es una tupla (doc_id, freq)\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *index_path*: Ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **postings(term)**: Devuelve la lista de postings del término pasado como parámetro. Cada posting es una tupla (doc_id, freq). Si el término no está en el diccionario, se devuelve una lista vacía.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **all_terms()**: Método que devuelve una lista con todos los términos del índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **add_postings(term,doc_id,freq)**: Método que añade un elemento a la lista de postings del término pasado como parámetro. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "    * *doc_id*: id del documento al cual corresponde el posting.\n",
    "    * *freq*: frecuencia del término en el documento identificado por doc_id.\n",
    "<br/><br/>\n",
    "Si el término no está en el diccionario, primero se añade al diccionario y se inicializa su valor a una lista vacía. Después, se añade a la lista de postings del término, la tupla formada por docid y frequencia pasadas como parámetros.\n",
    "\n",
    "\n",
    "* **save(dir)**: Método que guarda en disco los archivos generados por el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se guardarán los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el save de la clase padre Index. Además, guardamos la lista con los paths de los documentos y guardamos el diccionario con las listas de postings.\n",
    "\n",
    "* **open(dir)**: Método que carga de disco la lista con los paths de los documentos el diccionario con los postings. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde están almacenados los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el open de la clase padre Index. Además, leemos el archivo que contiene la lista con los paths de los documentos y leemos el diccionario con las listas de postings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbc9ng8e28p"
   },
   "source": [
    "## Ejercicio 2.2: Construcción del índice\n",
    "\n",
    "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "tHQ4UCf5pTw8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class RAMIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 2.2) #\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir=dir\n",
    "        self.index=RAMIndex(None)\n",
    "\n",
    "    def index_document(self, path, text):\n",
    "        text_terms=self.parser.parse(text)\n",
    "\n",
    "        self.index.add_doc(path)\n",
    "        doc_id=self.index.ndocs()-1\n",
    "\n",
    "        term_freq=Counter(text_terms)\n",
    "        for term, freq in term_freq.items():\n",
    "            self.index.add_posting(term, doc_id, freq)\n",
    "\n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "C_mxRswZh74N"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**RAMIndexBuilder**: Clase que hereda de Builder. Permite construir un índice RAMIndex.\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(dir)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "* **index_document(path, text)**: Método que añade un documento al índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *path*: Ruta del documento que queremos indexar.\n",
    "    * *text*: contenido del documento que queremos indexar.\n",
    "1. Primero aplicamos el parser al texto del documento para obtener la lista de términos del documento.\n",
    "2. Después, añadimos el path del documento con la función add_doc del index. \n",
    "3. Hallamos la frecuencia de cada término en el documento, utilizando la clase Counter del módulo collections de Python.\n",
    "4. Añadimos a la lista de postings cada término con su frecuencia en el documento, con la función add_posting del RAMIndex.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **commit()**: Método que guarda de forma definitiva el índice en el disco. Para ello llama a la función save del RAMIndex\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOWgbqZpV01"
   },
   "source": [
    "# Ejercicio 3: Índice en disco* (1pt)\n",
    "\n",
    "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
    "\n",
    "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
    "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
    "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
    "\n",
    "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
    "\n",
    "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación igual que indice RAM, tenemos que jugar con guardarlo en disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "br7yqFrnpZYl"
   },
   "outputs": [],
   "source": [
    "class DiskIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        # Diccionario que contendrá los postings de cada término.\n",
    "        # La clave será el término y el valor será una lista de postings,\n",
    "        self.dict_postings = {}\n",
    "\n",
    "        # Diccionario que contendrá las posiciones de los postings de cada término.\n",
    "        # La clave será el término y el valor será la posicion del posting en el archivo.\n",
    "        self.dict_postings_pos = {}\n",
    "\n",
    "        self.dir = dir\n",
    "        super().__init__(dir)\n",
    "\n",
    "    def postings(self, term):\n",
    "        if self.dict_postings_pos:\n",
    "            postings_list = []\n",
    "\n",
    "            if term in self.dict_postings_pos:\n",
    "                p = os.path.join(self.dir, Config.POSTINGS_FILE)\n",
    "\n",
    "                with open(p, 'rb') as f:\n",
    "                    # Nos colocamos en la posición del posting.\n",
    "                    f.seek(self.dict_postings_pos[term])\n",
    "\n",
    "                    # Leemos el posting.\n",
    "                    postings_list = pickle.load(f)\n",
    "\n",
    "            return postings_list\n",
    "        else:\n",
    "            return self.dict_postings[term] if term in self.dict_postings else []\n",
    "\n",
    "    def all_terms(self):\n",
    "        if self.dict_postings_pos:\n",
    "            return list(self.dict_postings_pos)\n",
    "        else:\n",
    "            return list(self.dict_postings)\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq))\n",
    "\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        # Guardamos la lista con los paths de los documentos.\n",
    "        p = os.path.join(dir, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "\n",
    "        # Creamos el fichero con los postings y nos guardamos la posicion del posting.\n",
    "        p = os.path.join(dir, Config.POSTINGS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            for term, postings in self.dict_postings.items():\n",
    "                # Obtenemos la posicion del posting en el fichero y la guardamos en el diccionario.\n",
    "                self.dict_postings_pos[term] = f.tell()\n",
    "\n",
    "                # Guardamos el posting en el fichero.\n",
    "                pickle.dump(postings, f)\n",
    "\n",
    "        # Guardamos el diccionario con las posiciones de los postings.\n",
    "        p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings_pos, f)\n",
    "\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "\n",
    "        # Cargamos de disco la lista con los paths de los documentos y\n",
    "        # el diccionario con las posiciones de los postings de cada termino.\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.PATHS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "            p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.dict_postings_pos = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "\n",
    "class DiskIndexBuilder(RAMIndexBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir = dir\n",
    "        self.index = DiskIndex(None)\n",
    "\n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTje0viiM9I"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**DiskIndex**: Clase que hereda de Index. Construye el índice en RAM, y guarda el índice en disco dato a dato. Se supone siempre que antes de empezar a usar el índice, se ha guardado en disco con el método save.\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(index_path)**: Constructor de la clase. Recibe como parámetros la ruta donde se encuentra el índice. El índice almacena :\n",
    "- dict_postings: diccionario que contendrá los postings de cada término. Es el diccionario que se usa al crear el índice. La clave será el término y el valor será una lista de postings, donde cada elemento de la lista es una tupla (doc_id, freq)\n",
    "- dict_postings_pos: diccionario que contendrá las posiciones de los postings de cada término. La clave será el término y el valor será la posicion del posting en el archivo de postings.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *index_path*: Ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **postings(term)**: Devuelve la lista de postings del término pasado como parámetro. Cada posting es una tupla (doc_id, freq). Si el término no está en el diccionario, se devuelve una lista vacía.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **all_terms()**: Método que devuelve una lista con todos los términos del índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **add_postings(term,doc_id,freq)**: Método que añade un elemento a la lista de postings del término pasado como parámetro. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "    * *doc_id*: id del documento al cual corresponde el posting.\n",
    "    * *freq*: frecuencia del término en el documento identificado por doc_id.\n",
    "<br/><br/>\n",
    "Si el término no está en el diccionario, primero se añade al diccionario y se inicializa su valor a una lista vacía. Después, se añade a la lista de postings del término, la tupla formada por docid y frecuencia pasadas como parámetros.\n",
    "<br/><br/>\n",
    "\n",
    "* **save(dir)**: Método que guarda en disco los archivos generados por el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se guardarán los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el save de la clase padre Index. Además, guardamos la lista con los paths de los documentos y guardamos el diccionario con las posiciones de los postings.\n",
    "\n",
    "* **open(dir)**: Método que carga de disco la lista con los paths de los documentos y el diccionario con las posiciones de los postings de cada termino.. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde están almacenados los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el open de la clase padre Index. Además, leemos el archivo que contiene la lista con los paths de los documentos y leemos el archivo con el diccionario con las posiciones de los postings de cada termino.\n",
    "\n",
    "**DiskIndexBuilder**: Clase que hereda de Builder. Permite construir un índice DiskIndex.\n",
    "\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(dir)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "\n",
    "* **commit()**: Método que guarda de forma definitiva el índice en el disco. Para ello llama a la función save del DiskIndex\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxuclLwpaM-"
   },
   "source": [
    "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
    "\n",
    "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whoosh el positional_postings devuelve una lista de tuplas (doc_id, [posiciones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "e3uq565SpfSA"
   },
   "outputs": [],
   "source": [
    "class ProximitySearcher(Searcher):\n",
    "    # Your new code here (exercise 4*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4hWTstiOIT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPPWV7pepf85"
   },
   "source": [
    "# Ejercicio 5: Índice posicional* (1pt)\n",
    "\n",
    "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# índice posicional sobre RAM es más fácil. Utilizamos el tercer campo del posting en el que se almacena\n",
    "# la posición del término en el texto. El tercer elemento de la tupla será una lista de posiciones. \n",
    "# Podemos mantener la frecuencia del posting o no (ya que la frecuencia es la longitud de la lista de posiciones)\n",
    "# Método positionalPostings devuelve lista de posiciones\n",
    "# Método postings devuelve como antes doc_id y frecuencia. \n",
    "# Se puede hacer el ejercicio 4 sin hacer el 5. Si se hace el ejercicio 5, el ejercicio 4 prueba que se hace bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "zg8MIMpipih1"
   },
   "outputs": [],
   "source": [
    "class PositionalIndex(RAMIndex):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Note that it may be better to inherit from a different class\n",
    "    # if your index extends a particular type of index\n",
    "    # For example: PositionalIndex(RAMIndex)\n",
    "\n",
    "    def postings(self, term):\n",
    "        postings=[]\n",
    "        if term in self.dict_postings:\n",
    "            for posting in self.dict_postings[term]:\n",
    "                postings.append((posting[0],posting[1]))\n",
    "            return postings\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "    def positional_postings(self, term):\n",
    "        if term in self.dict_postings:\n",
    "            postings = self.dict_postings[term]\n",
    "\n",
    "            # Eliminamos el elemento de la tupla que contiene la frecuencia.\n",
    "            return [(posting[0], posting[2]) for posting in postings]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq, position_list):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq, position_list))\n",
    "        \n",
    "        \n",
    "class PositionalIndexBuilder(RAMIndexBuilder):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Same note as for PositionalIndex\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir=dir\n",
    "        self.index=PositionalIndex(None)\n",
    "\n",
    "        \n",
    "    def index_document(self, path, text):\n",
    "        text_terms=self.parser.parse(text)\n",
    "\n",
    "        self.index.add_doc(path)\n",
    "        doc_id=self.index.ndocs()-1\n",
    "        \n",
    "        \n",
    "        array_terms=np.array(text_terms)\n",
    "        for term in set(text_terms):\n",
    "            position_list=np.where(array_terms == term)[0]\n",
    "            self.index.add_posting(term, doc_id, len(position_list), list(position_list))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H1gukouXiPV3"
   },
   "source": [
    "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
    "\n",
    "**PositionalIndex**: Clase que hereda de RAMIndex. Construye el índice en RAM, para ello construimos las listas de postings, en las que se incluyen las posiciones de los términos en los documentos. Cabe destacar que he hemos decidido que dict_postings sea ahora un diccionario cuya clave es diccionario que contendrá los postings de cada término. La clave será el término y el valor será una lista de postings, donde cada elemento de la lista es una tupla (doc_id, freq, position_list), donde position_list es la lista de posiciones del término en el documento. La longitud de esta lista coindice con el valor freq, es decir, la frecuencia del término dentro del documento.\n",
    "\n",
    "**Métodos:**\n",
    "\n",
    "* **postings(term)**: Devuelve la lista de postings del término pasado como parámetro. Cada posting es una tupla (doc_id, freq). Si el término no está en el diccionario, se devuelve una lista vacía.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **all_terms()**: Método que devuelve una lista con todos los términos del índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **add_postings(term,doc_id,freq)**: Método que añade un elemento a la lista de postings del término pasado como parámetro. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "    * *doc_id*: id del documento al cual corresponde el posting.\n",
    "    * *freq*: frecuencia del término en el documento identificado por doc_id.\n",
    "<br/><br/>\n",
    "\n",
    "**PositionalIndexBuilder**: Clase que hereda de RAMIndexBuilder. Permite construir un índice PositionalIndex.\n",
    "\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(dir)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice. Se llama al constructor de la clase padre.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "\n",
    "* **index_document()**: Método que añade un documento al índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *path*: Ruta del documento que queremos indexar.\n",
    "    * *text*: contenido del documento que queremos indexar.\n",
    "1. Primero aplicamos el parser al texto del documento para obtener la lista de términos del documento.\n",
    "2. Después, añadimos el path del documento con la función add_doc del index. \n",
    "3. Hallamos la frecuencia y posición de cada término en el documento. Para ello iteramos sobre los términos del documento, sin repetir términos. Para ello iteramos sobre el set de los términos del texto, que no tiene elementos repetidos. Usamos la función where de numpy para obtener la lista de posiciones del término en el array de términos del texto (text_terms que hemos transformado a numpy array). Así obtenemos una lista formada por las posiciones del término en el texto, la longitud de dicha lista es la frecuencia del término en el documento. \n",
    "4. Añadimos a la lista de postings cada término con su frecuencia en el documento y lista de posiciones, con la función add_posting del PositionalIndex.\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HbYGn8pjKZ"
   },
   "source": [
    "# Ejercicio 6: PageRank (1pt)\n",
    "\n",
    "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
    "\n",
    "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_aQE7SBgpk1S"
   },
   "outputs": [],
   "source": [
    "class PagerankDocScorer():\n",
    "    def __init__(self, graphfile, r, n_iter):\n",
    "        # Format of graphfile:\n",
    "        #  node1 node2\n",
    "\n",
    "        # Diccionario con los documentos/nodos como claves y como valor una lista de nodos\n",
    "        # a los que apunta, que representan las conexiones.\n",
    "        self.dict_connections = {}\n",
    "\n",
    "        # Set que contendra todos los nodos del grafo\n",
    "        self.all_nodes = set()\n",
    "\n",
    "        self.graphfile = graphfile\n",
    "        self.r = r\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.load_graphfile()\n",
    "\n",
    "        # Diccionario con los documentos como claves y como valor su pagerank\n",
    "        self.pagerank_scores = {}\n",
    "        self.calculate_pagerank()\n",
    "\n",
    "    def load_graphfile(self):\n",
    "        \"\"\"\n",
    "        Crea el grafo con todas las conexiones a partir de un fichero.\n",
    "        El formato del fichero es: node1 node2\n",
    "\n",
    "        El grafo estará contenido en el diccionario self.dict_connections, donde las claves son los nodos\n",
    "        y el valor es una lista de nodos, donde cada elemento en dicha lista representa una conexion\n",
    "        desde el nodo usado como clave al nodo que esta en la lista.\n",
    "        \"\"\"\n",
    "        with open(self.graphfile, 'r') as f:\n",
    "            for line in f:\n",
    "                mod_line = line.rstrip('\\n')\n",
    "                node1, node2 = mod_line.split('\\t')\n",
    "\n",
    "                # Add nodes to the set of all nodes\n",
    "                self.all_nodes.add(node1)\n",
    "                self.all_nodes.add(node2)\n",
    "\n",
    "                # Add node2 to the list of connections of node1\n",
    "                if node1 not in self.dict_connections:\n",
    "                    self.dict_connections[node1] = [node2]\n",
    "                else:\n",
    "                    self.dict_connections[node1].append(node2)\n",
    "\n",
    "            # A sink is a node that has no outgoing connections.\n",
    "            sinks = self.all_nodes - set(self.dict_connections.keys())\n",
    "\n",
    "            # Connect sinks to all the nodes.\n",
    "            for sink in sinks:\n",
    "                self.dict_connections[sink] = list(self.all_nodes)\n",
    "\n",
    "    def calculate_pagerank(self):\n",
    "        \"\"\"\n",
    "        Calcula el pagerank de cada nodo del grafo y lo guarda en un diccionario,\n",
    "        donde las claves son los nodos y el valor es su pagerank.\n",
    "\n",
    "        Este método usa el número de iteraciones especificado en el constructor.\n",
    "        \"\"\"\n",
    "        # Initialize pagerank scores\n",
    "        for node in self.all_nodes:\n",
    "            self.pagerank_scores[node] = 1 / len(self.all_nodes)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # Calculate new pagerank scores\n",
    "            new_pagerank_scores = {}\n",
    "            for node in self.all_nodes:\n",
    "                new_pagerank_scores[node] = self.r / len(self.all_nodes)\n",
    "\n",
    "            # Iterate through all the connections\n",
    "            for node1, connections in self.dict_connections.items():\n",
    "                for node2 in connections:\n",
    "                    new_pagerank_scores[node2] += (1 - self.r) * self.pagerank_scores[node1] / len(connections)\n",
    "\n",
    "            # Update pagerank scores\n",
    "            self.pagerank_scores = new_pagerank_scores\n",
    "\n",
    "    def rank(self, cutoff):\n",
    "        \"\"\"\n",
    "        Devuelve un ranking de cutoff documentos, ordenados por su pagerank.\n",
    "        \"\"\"\n",
    "        # Create SearchRanking\n",
    "        ranking = SearchRanking(cutoff)\n",
    "\n",
    "        for node, score in self.pagerank_scores.items():\n",
    "            ranking.push(node, score)\n",
    "\n",
    "        return ranking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ZT7seCiQiT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**PageRankDocScorer**: Clase que implementa el algoritmo PageRank\n",
    "\n",
    "**Métodos:**\n",
    "\n",
    "\n",
    "* **\\_\\_init\\_\\_(graphfile, r, n_iter)**: Constructor de la clase. Recibe como parámetros \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *graphfile*: Ruta donde se encuentra el índice.\n",
    "    * *r*: factor de teleportación.\n",
    "    * *n_iter*: número de iteraciones.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **load_graphfile()**: Crea el grafo con todas las conexiones a partir de un fichero. El formato del fichero es: node1 node2.\n",
    "\n",
    "El grafo estará contenido en el diccionario self.dict_connections, donde las claves son los nodos y el valor es una lista de nodos, donde cada elemento en dicha lista representa una conexion desde el nodo usado como clave al nodo que esta en la lista.\n",
    "<br/><br/>\n",
    "\n",
    "* **calculate_pagerank()**: Calcula el pagerank de cada nodo del grafo y lo guarda en un diccionario, donde las claves son los nodos y el valor es su pagerank. Este método usa el número de iteraciones especificado en el constructor.\n",
    "<br/><br/>\n",
    "\n",
    "* **rank(cutoff)**:  Devuelve un objeto Searchranking con el ranking de cutoff documentos, ordenados por su pagerank.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *cutoff*: número de resultados del ranking.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zXhPtFzon72"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que esta celda funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "fTdDacCRn0u6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.014702796936035156 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.01677536964416504 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.013969659805297852 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0007929801940917969 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0007431507110595703 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0012929439544677734 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0005710124969482422 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0004658699035644531 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.00042057037353515625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 5.340576171875e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0002295970916748047 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 5.6743621826171875e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa dd\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0006544589996337891 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0009667873382568359 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.000782012939453125 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0011506080627441406 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00032067298889160156 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00028324127197265625 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0008356571197509766 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005140304565429688 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004837512969970703 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0014221668243408203 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002617835998535156 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00024700164794921875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 7.271766662597656e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.340576171875e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.555152893066406e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004839897155761719 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00018548965454101562 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.000171661376953125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 7.62939453125e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.5789947509765625e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.7220458984375e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa dd'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0021424293518066406 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0019505023956298828 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.002718687057495117 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0008111000061035156 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00027370452880859375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00023603439331054688 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004489421844482422 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002887248992919922 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002231597900390625 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007739067077636719 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0003924369812011719 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004265308380126953 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00018358230590820312 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 8.296966552734375e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 8.487701416015625e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0003924369812011719 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0001838207244873047 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00014519691467285156 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0001049041748046875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 7.2479248046875e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 9.489059448242188e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.012402057647705078 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.01683783531188965 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.013199567794799805 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.000576019287109375 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0007281303405761719 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0010881423950195312 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.001524209976196289 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0009579658508300781 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0004405975341796875 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 5.4836273193359375e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.00022745132446289062 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 5.364418029785156e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0016427040100097656 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.001138925552368164 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011603832244873047 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00107574462890625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005404949188232422 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004057884216308594 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007002353668212891 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00042724609375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003025531768798828 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005710124969482422 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004436969757080078 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00042247772216796875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00010180473327636719 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.772445678710938e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.416175842285156e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004813671112060547 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002856254577636719 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002536773681640625 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00013756752014160156 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.606910705566406e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 9.107589721679688e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa cc'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0015025138854980469 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0019123554229736328 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0027627944946289062 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0020873546600341797 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.000629425048828125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005202293395996094 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008966922760009766 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005626678466796875 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004646778106689453 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0013892650604248047 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007662773132324219 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007281303405761719 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.000156402587890625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00011754035949707031 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00012421607971191406 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008296966552734375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00042176246643066406 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00033736228942871094 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00013589859008789062 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00011301040649414062 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001220703125 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'bb aa'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 2 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.025817394256591797 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.023413419723510742 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.01810002326965332 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0018930435180664062 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0023849010467529297 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.005088090896606445 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0008649826049804688 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0006330013275146484 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0005261898040771484 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 6.127357482910156e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0003592967987060547 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 8.702278137207031e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.006506919860839844 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0037338733673095703 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003549337387084961 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0012080669403076172 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003414154052734375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00029969215393066406 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008993148803710938 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00036334991455078125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00030803680419921875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0011477470397949219 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003609657287597656 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00031447410583496094 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001373291015625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00011301040649414062 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 9.1552734375e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006670951843261719 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003952980041503906 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003418922424316406 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00024580955505371094 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00014638900756835938 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00010204315185546875 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa cc'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.006238222122192383 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0024743080139160156 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0050373077392578125 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0013582706451416016 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003609657287597656 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00031447410583496094 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008680820465087891 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003414154052734375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00028133392333984375 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0011975765228271484 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003848075866699219 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002865791320800781 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 9.250640869140625e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 6.985664367675781e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.390975952148438e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006492137908935547 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002803802490234375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001614093780517578 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00010132789611816406 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.295608520507812e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.486343383789062e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'bb aa'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.5211169719696045 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.6838228702545166 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 4.431155443191528 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.128973960876465 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.984614610671997 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.9932124614715576 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6062\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.021392822265625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6062\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.024344205856323242 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6062\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.024736404418945312 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5949\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.0007045269012451172 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5949\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.00087738037109375 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5949\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.0005614757537841797 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for information probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.008400678634643555 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.009194374084472656 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0049304962158203125 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0014603137969970703 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005748271942138672 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003497600555419922 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0017764568328857422 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003936290740966797 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.000362396240234375 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006215572357177734 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00031566619873046875 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002799034118652344 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.152557373046875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.936622619628906e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.413459777832031e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003981590270996094 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00023937225341796875 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00015687942504882812 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02451982137907164 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017350091044968305 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009540509720222096 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.200241088867188e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02451982137907164 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017350091044968305 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009540509720222096 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.079673767089844e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02451982137907164 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017350091044968305 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009540509720222096 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.29425048828125e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'information probability'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for probability information\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0044403076171875 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.005105733871459961 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004166841506958008 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006756782531738281 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00029921531677246094 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002620220184326172 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0020568370819091797 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003445148468017578 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003712177276611328 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005931854248046875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003116130828857422 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002722740173339844 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.43865966796875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.888938903808594e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.318092346191406e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00040721893310546875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002307891845703125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00015401840209960938 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02451982137907164 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017350091044968305 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009540509720222096 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.581710815429688e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02451982137907164 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017350091044968305 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009540509720222096 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.008148193359375e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02451982137907164 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017350091044968305 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009540509720222096 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.318092346191406e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'probability information'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for higher probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.885515041393065 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5281365807265503 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7316404431205594 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004271268844604492 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "2.885515041393065 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5281365807265503 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7316404431205594 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0053730010986328125 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "2.885515041393065 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5281365807265503 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7316404431205594 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004323482513427734 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007367134094238281 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003077983856201172 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00027298927307128906 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.002252817153930664 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003719329833984375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004062652587890625 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006685256958007812 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003256797790527344 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00028061866760253906 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.867813110351562e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.413459777832031e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.556510925292969e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004024505615234375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002830028533935547 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00017523765563964844 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.028069873466860844 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217314023 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793651 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.058547973632812e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.028069873466860844 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217314023 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793651 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.318092346191406e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.028069873466860844 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217314023 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793651 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.580352783203125e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'higher probability'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/urls.txt'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 2.9643144607543945 seconds ---\n",
      "\tWhooshForwardIndex: 3.1930694580078125 seconds ---\n",
      "\tWhooshPositionalIndex: 3.7713799476623535 seconds ---\n",
      "\tRAMIndex: 1.6317036151885986 seconds ---\n",
      "\tDiskIndex: 1.651526689529419 seconds ---\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.00127410888671875 seconds ---\n",
      "\tWhooshForwardIndex: 0.000946044921875 seconds ---\n",
      "\tWhooshPositionalIndex: 0.0011358261108398438 seconds ---\n",
      "\tRAMIndex: 0.003884553909301758 seconds ---\n",
      "\tDiskIndex: 0.0013401508331298828 seconds ---\n",
      "  Disk space...\n",
      "\tWhooshIndex: 1090654 space ---\n",
      "\tWhooshForwardIndex: 1167952 space ---\n",
      "\tWhooshPositionalIndex: 1248002 space ---\n",
      "\tRAMIndex: 126678 space ---\n",
      "\tDiskIndex: 222596 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'information probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003936052322387695 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.005434513092041016 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.000789642333984375 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0008101463317871094 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003247261047363281 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.00034236907958984375 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.82012939453125e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 9.72747802734375e-05 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002827644348144531 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.0002970695495605469 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00015997886657714844 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.00017595291137695312 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'probability information'\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.92627179982283 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763523908656511 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.10100372975772 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.005326032638549805 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.007060050964355469 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0012412071228027344 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0012714862823486328 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02430272915045772 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017423445017086134 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009606550624010472 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003726482391357422 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.000392913818359375 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.225440979003906e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.00010514259338378906 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003123283386230469 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.00032639503479003906 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.024519821379071636 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735009104496815 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00954050972022197 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002009868621826172 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.0002243518829345703 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'higher probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.885515041393065 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.5281365807265503 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7316404431205594 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0037839412689208984 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.00515294075012207 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0008082389831542969 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0008289813995361328 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.027900528394734402 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012272676372515206 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005739853958059494 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00034236907958984375 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0003726482391357422 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.106231689453125e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.00011277198791503906 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00034236907958984375 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.00035643577575683594 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.02806987346686084 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012183713217313916 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005700394930793575 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00016808509826660156 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.0001823902130126953 seconds ---\n",
      "----------------------------\n",
      "Testing PageRank\n",
      "0.3846153846153846 \t c\n",
      "0.3589743589743589 \t a\n",
      "0.2564102564102564 \t b\n",
      "\n",
      "--- Pagerank with toy_graph_1 0.0057926177978515625 seconds ---\n",
      "0.368421052631579 \t b\n",
      "0.3157894736842105 \t c\n",
      "0.3157894736842105 \t a\n",
      "\n",
      "--- Pagerank with toy_graph_2 0.0005297660827636719 seconds ---\n",
      "0.033333748623865 \t clueweb09-en0000-06-11938.html\n",
      "0.032822385415072756 \t clueweb09-en0000-01-22977.html\n",
      "0.03264169111517307 \t clueweb09-en0000-06-11940.html\n",
      "0.03170456979307948 \t clueweb09-en0000-06-11932.html\n",
      "0.031598005967786835 \t clueweb09-en0000-01-27969.html\n",
      "\n",
      "--- Pagerank with simulated links for doc1k 0.4265782833099365 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # We now test building different implementations of an index\n",
    "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_paths)\n",
    "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_paths)\n",
    "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_paths)\n",
    "    try:\n",
    "        test_build(RAMIndexBuilder(index_path + \"ram\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(DiskIndexBuilder(index_path + \"disk\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"PositionalIndexBuilder still not implemented\")\n",
    "\n",
    "    def catch_index(func, name, *args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except NotImplementedError:\n",
    "            print(name + \" still not implemented (index)\")\n",
    "            return None\n",
    "\n",
    "    # We now inspect all the implementations\n",
    "    indices = [\n",
    "            WhooshIndex(index_path + \"whoosh\"),\n",
    "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
    "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
    "            catch_index(lambda: RAMIndex(index_path + \"ram\"), \"RAMIndex\"),\n",
    "            catch_index(lambda: DiskIndex(index_path + \"disk\"), \"DiskIndex\"),\n",
    "            catch_index(lambda: PositionalIndex(index_path + \"pos\"), \"PositionalIndex\"),\n",
    "            ]\n",
    "    for index in indices:\n",
    "        if index:\n",
    "            test_read(index, word)\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Checking search results for %s\" % (query))\n",
    "        # Whoosh searcher can only work with its own indices\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        try:\n",
    "            test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher still not implemented\")\n",
    "        for index in indices:\n",
    "            if index:\n",
    "                # our searchers should work with any other index\n",
    "                test_search(SlowVSMSearcher(index), index, query, 5)\n",
    "                try:\n",
    "                    test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"TermBasedVSMSearcher still not implemented\")\n",
    "                try:\n",
    "                    test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"DocBasedVSMSearcher still not implemented\")\n",
    "        try:\n",
    "            test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher or PositionalIndex still not implemented\")\n",
    "\n",
    "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
    "    indices = list()\n",
    "\n",
    "    if analyse_performance:\n",
    "        # let's analyse index performance\n",
    "        test_index_performance(collection_paths, index_path)\n",
    "        # let's analyse search performance\n",
    "        for query in queries:\n",
    "            test_search_performance(collection_paths, index_path, query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # this function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # when we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    # more tests\n",
    "    doc_id = 0\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, index, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def disk_space(index_path: str) -> int:\n",
    "    space = 0\n",
    "    if os.path.isdir(index_path):\n",
    "        for f in os.listdir(index_path):\n",
    "            p = os.path.join(index_path, f)\n",
    "            if os.path.isfile(p):\n",
    "                space += os.path.getsize(p)\n",
    "    return space\n",
    "\n",
    "def test_index_performance (collection_paths: list, base_index_path: str):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing index performance on \" + str(collection_paths) + \" document collection\")\n",
    "\n",
    "    print(\"  Build time...\")\n",
    "    start_time = time.time()\n",
    "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = RAMIndexBuilder(base_index_path + \"ram\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = DiskIndexBuilder(base_index_path + \"disk\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "\n",
    "    print(\"  Load time...\")\n",
    "    start_time = time.time()\n",
    "    WhooshIndex(base_index_path + \"whoosh\")\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        RAMIndex(base_index_path + \"ram\")\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        DiskIndex(base_index_path + \"disk\")\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "\n",
    "    print(\"  Disk space...\")\n",
    "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
    "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
    "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
    "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
    "    print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
    "\n",
    "\n",
    "def test_search_performance (collection_paths: list, base_index_path: str, query: str, cutoff: int):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing search performance on \" + str(collection_paths) + \" document collection with query: '\" + query + \"'\")\n",
    "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
    "    try:\n",
    "        ram_index = RAMIndex(base_index_path + \"ram\")\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "        ram_index = None\n",
    "    try:\n",
    "        disk_index = DiskIndex(base_index_path + \"disk\")\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "        disk_index = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
    "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # let's test some combinations of ranking + index implementations\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "        print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if ram_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
    "            print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DocBasedVSMSearcher still not implemented\")\n",
    "\n",
    "def test_pagerank(graphs_root_dir, cutoff):\n",
    "    print(\"----------------------------\")\n",
    "    # we separate this function because it cannot work with all the collections\n",
    "    print(\"Testing PageRank\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "test_collection ([collections_root_dir + \"toy1/\"], index_root_dir + \"toy1/\", \"cc\", [\"aa dd\", \"aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy2/\"], index_root_dir + \"toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy1/\", collections_root_dir + \"toy2/\"], index_root_dir + \"toys/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"toy2/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"three_collections/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs10k.zip\"], index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], False)\n",
    "test_pagerank(\"./collections/\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JhJJSFiSl5"
   },
   "source": [
    "### Resumen de coste y rendimiento\n",
    "\n",
    "Hay que analizar las **diferencias de rendimiento** observadas entre las diferentes implementaciones que se han creado y probado para cada componente.\n",
    "\n",
    "En concreto, hay que reportar tiempo de indexado, consumo máximo de RAM y espacio en disco al construir el índice, y el tiempo de carga y consumo máximo de RAM al cargar el índice para cada una de las colecciones utilizadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "|      | Construcción | del | índice | Carga del | índice |\n",
    "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
    "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
    "| toy1 | | | | | |\n",
    "| toy2 | | | | | |\n",
    "| toys | | | | | |\n",
    "| 1K | | | | | |\n",
    "| 10K | | | | | |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
