{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eq_QfGIGXC_"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Motores de búsqueda e indexación**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 21 / jueves 23 de febrero\n",
    "* Entrega: martes 28 / jueves 30 de marzo (14:00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Xu Chen Xu <br>\n",
    "Ana Martínez Sabiote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYT0Qlrnoy7l"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFY_K6_pA_J"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
    "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
    "*\tLa implementación de un método de búsqueda proximal.\n",
    "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
    "*\tLa implementación del algoritmo PageRank.\n",
    "\n",
    "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
    "\n",
    "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
    "\n",
    "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El **único buscador que no será intercambiable es el de Whoosh**, que sólo funcionará con sus propios índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
    "\n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la *celda de prueba* así como otros tests adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p2-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
    "\n",
    "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la **celda de prueba** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (ten en cuenta que, aquellos ejercicios que no se hayan realizado, lanzan una excepción que se captura en dicha celda, por lo que no debería ser necesario modificarla).\n",
    "\n",
    "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjq_DVVpDEL"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
    "Las clases parten del código de la práctica anterior.\n",
    "Igual que en la práctica 1, la **celda de prueba** (al final del enunciado) implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
    "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
    "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
    "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería mantenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases genéricas ya implementadas\n",
    "\n",
    "En la siguiente celda de código, se encuentran ya implementadas las clases *Index* y *Builder* de manera que facilite la creación de otros índices a partir de las mismas. \n",
    "\n",
    "Estudia esta implementación y compara las **decisiones de diseño** tomadas con las vuestras en la práctica anterior.\n",
    "Ten en cuenta que las funciones de TF e IDF están **sin implementar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "xAKBQZLLqVXR"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Config(object):\n",
    "  # variables de clase\n",
    "  NORMS_FILE = \"docnorms.dat\"\n",
    "  PATHS_FILE = \"docpaths.dat\"\n",
    "  INDEX_FILE = \"serialindex.dat\"\n",
    "  DICTIONARY_FILE = \"dictionary.dat\"\n",
    "  POSTINGS_FILE = \"postings.dat\"\n",
    "\n",
    "class BasicParser:\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "# Parámetro freq: frecuencia de un término\n",
    "def tf(freq):\n",
    "    if freq > 0:\n",
    "        tf = 1 + math.log(freq, 2)\n",
    "    else:\n",
    "        tf = 0\n",
    "\n",
    "    return tf \n",
    "\n",
    "# Parámetros\n",
    "#    df: doc_freq(term) frecuencia de un término\n",
    "#    n: ndocs() número total de documentos\n",
    "def idf(df, n):\n",
    "    idf = math.log(( (n+1) / (df+0.5)), 2)\n",
    "    \n",
    "    return idf \n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        self.index = index\n",
    "        self.parser = parser\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, dir=None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        if dir: self.open(dir)\n",
    "    def add_doc(self, path):\n",
    "        self.docmap.append(path)  # Assumed to come in order\n",
    "    def doc_path(self, docid):\n",
    "        return self.docmap[docid]\n",
    "    def doc_module(self, docid):\n",
    "        if docid in self.modulemap:\n",
    "            return self.modulemap[docid]\n",
    "        return None\n",
    "    def ndocs(self):\n",
    "        return len(self.docmap)\n",
    "    def doc_freq(self, term):\n",
    "        return len(self.postings(term))\n",
    "    def term_freq(self, term, docID):\n",
    "        post = self.postings(term)\n",
    "        if post is None: return 0\n",
    "        for posting in post:\n",
    "            if posting[0] == docID:\n",
    "                return posting[1]\n",
    "        return 0\n",
    "    def total_freq(self, term):\n",
    "        freq = 0\n",
    "        for posting in self.postings(term):\n",
    "            freq += posting[1]\n",
    "        return freq\n",
    "    def postings(self, term):\n",
    "        # used in more efficient implementations\n",
    "        return list()\n",
    "    def positional_postings(self, term):\n",
    "        # used in positional implementations\n",
    "        return list()\n",
    "    def all_terms(self):\n",
    "        return list()\n",
    "    def save(self, dir):\n",
    "        if not self.modulemap: self.compute_modules()\n",
    "        p = os.path.join(dir, Config.NORMS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.modulemap, f)        \n",
    "    def open(self, dir):\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.NORMS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.modulemap = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "    def compute_modules(self):\n",
    "        for term in self.all_terms():\n",
    "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
    "            post = self.postings(term)\n",
    "            if post is None: continue\n",
    "            for docid, freq in post:\n",
    "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
    "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
    "        for docid in range(self.ndocs()):\n",
    "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
    "\n",
    "import shutil\n",
    "class Builder:\n",
    "    def __init__(self, dir, parser=BasicParser()):\n",
    "        if os.path.exists(dir): shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "        self.parser = parser\n",
    "    def build(self, path):\n",
    "        if zipfile.is_zipfile(path):\n",
    "            self.index_zip(path)\n",
    "        elif os.path.isdir(path):\n",
    "            self.index_dir(path)\n",
    "        else:\n",
    "            self.index_url_file(path)\n",
    "    def index_zip(self, filename):\n",
    "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
    "        for name in sorted(file.namelist()):\n",
    "            with file.open(name, \"r\", force_zip64=True) as f:\n",
    "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "        file.close()\n",
    "    def index_dir(self, dir):\n",
    "        for subdir, dirs, files in os.walk(dir):\n",
    "            for file in sorted(files):\n",
    "                path = os.path.join(dir, file)\n",
    "                with open(path, \"r\") as f:\n",
    "                    self.index_document(path, f.read())\n",
    "    def index_url_file(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            self.index_urls(line.rstrip('\\n') for line in f)\n",
    "    def index_urls(self, urls):\n",
    "        for url in urls:\n",
    "            self.index_document(url, BeautifulSoup(urlopen(url).read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "    def index_document(self, path, text):\n",
    "        raise NotImplementedError # to be implemented by child class\n",
    "    def commit(self):\n",
    "        raise NotImplementedError # to be implemented by child class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de buscador\n",
    "\n",
    "En la siguiente celda se encuentra una implementación de un buscador basado en coseno que es relativamente lento. En los siguientes ejercicios veremos formas de acelerar el proceso (sin cambiar los resultados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "yMoae4N7y38C"
   },
   "outputs": [],
   "source": [
    "# from previous lab\n",
    "class SlowVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for docid in range(self.index.ndocs()):\n",
    "            score = self.score(docid, qterms)\n",
    "            if score:\n",
    "                ranking.push(self.index.doc_path(docid), score)\n",
    "        return ranking\n",
    "\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases Whoosh\n",
    "\n",
    "En la siguiente celda podrás encontrar la adaptación a nuestras interfaces de los índices de Whoosh, en concreto, de tres variantes que permite usar la librería (observa los distintos Schema's usados y qué metodos se han reimplementado en cada caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "I-7gj9Rxx6LD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import whoosh\n",
    "except ModuleNotFoundError:\n",
    "  !pip install whoosh\n",
    "  import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "SimpleDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False))\n",
    "ForwardDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False,vector=Format))\n",
    "PositionalDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=True))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def __init__(self, dir, schema=SimpleDocument):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
    "        self.dir = dir\n",
    "\n",
    "    def index_document(self, p, text):\n",
    "        self.whoosh_writer.add_document(path=p, content=text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshForwardBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, ForwardDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshForwardIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshPositionalBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, PositionalDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshPositionalIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
    "    def total_freq(self, term):\n",
    "        return self.whoosh_reader.frequency(\"content\", term)\n",
    "    def doc_freq(self, term):\n",
    "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_reader.stored_fields(docid)['path']\n",
    "    def ndocs(self):\n",
    "        return self.whoosh_reader.doc_count()\n",
    "    def all_terms(self):\n",
    "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
    "    def postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshForwardIndex(WhooshIndex):\n",
    "    def term_freq(self, term, docID) -> int:\n",
    "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
    "            v = self.whoosh_reader.vector(docID, \"content\")\n",
    "            v.skip_to(term)\n",
    "            if v.id() == term:\n",
    "                return v.value_as(\"frequency\")\n",
    "        return 0\n",
    "\n",
    "class WhooshPositionalIndex(WhooshIndex):\n",
    "    def positional_postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, dir):\n",
    "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
    "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
    "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
    "    def search(self, query, cutoff):\n",
    "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
    "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_index.reader().stored_fields(docid)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3jRLNZmpEk_"
   },
   "source": [
    "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
    "\n",
    "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
    "\n",
    "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be3vDQNxdWbo"
   },
   "source": [
    "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
    "\n",
    "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "ppr9PtZmduql"
   },
   "outputs": [],
   "source": [
    "class TermBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.1) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "        \n",
    "    def search(self, query, cutoff):\n",
    "        scores={}\n",
    "        query_terms=self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            for doc_id, freq in self.index.postings(term):\n",
    "                if doc_id not in scores:\n",
    "                    scores[doc_id]=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                else:\n",
    "                    scores[doc_id]+=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                    \n",
    "        for doc_id, freq in scores.items():\n",
    "            mod = self.index.doc_module(doc_id)\n",
    "            if mod:\n",
    "                scores[doc_id]=freq/mod\n",
    "            if scores[doc_id]:\n",
    "                ranking.push(self.index.doc_path(doc_id), scores[doc_id])\n",
    "                \n",
    "        return ranking\n",
    "\n",
    "        #dic.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YGEGm7haop"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ti8qGedgNB"
   },
   "source": [
    "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
    "\n",
    "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "wzZ-6OG0dvwX"
   },
   "outputs": [],
   "source": [
    "class DocBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.2*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        raise NotImplementedError\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xYd4hzhukr"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXHr18Cdl2Q"
   },
   "source": [
    "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
    "\n",
    "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)), es decir, que permita almacenar un **número limitado de documentos** en memoria y su puntuación asociada. \n",
    "\n",
    "Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "MOfT2yZGpMNi"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class SearchRanking:\n",
    "    def __init__(self, cutoff):\n",
    "        self.cutoff = cutoff\n",
    "        self.ranking = list()\n",
    "\n",
    "    def push(self, docid, score):\n",
    "        if len(self.ranking) < self.cutoff:\n",
    "            heapq.heappush(self.ranking, (score, docid))\n",
    "        else:\n",
    "            heapq.heappushpop(self.ranking, (score, docid))\n",
    "\n",
    "    def __iter__(self):\n",
    "        ## sort ranking\n",
    "        orderedRanking = sorted(self.ranking, reverse=True)\n",
    "\n",
    "        # Invertimos la tupla para que el docid sea el primer elemento y el score el segundo\n",
    "        orderedRanking = [(x[1], x[0]) for x in orderedRanking]\n",
    "        return iter(orderedRanking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDzjUp-hwNZ"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNkPcUjMpNRn"
   },
   "source": [
    "# Ejercicio 2: Índice en RAM (3pt)\n",
    "\n",
    "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
    "\n",
    "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
    "\n",
    "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVzsIg0Zev7a"
   },
   "source": [
    "## Ejercicio 2.1: Estructura de índice\n",
    "\n",
    "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
    "\n",
    "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "VqSKneeSe2bN"
   },
   "outputs": [],
   "source": [
    "class RAMIndex(Index):\n",
    "    # Your new code here (exercise 2.1) #\n",
    "    def __init__(self, dir):\n",
    "        # Diccionario que contendrá los postings de cada término.\n",
    "        # La clave será el término y el valor será una lista de postings,\n",
    "        # donde cada elemento de la lista es una tupla (doc_id, freq).\n",
    "        self.dict_postings = {}\n",
    "\n",
    "        # El constructor del super llamará a open si dir no es None.\n",
    "        super().__init__(dir)\n",
    "\n",
    "    def postings(self, term):\n",
    "        return self.dict_postings[term] if term in self.dict_postings else []\n",
    "\n",
    "    def all_terms(self):\n",
    "        return list(self.dict_postings)\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq))\n",
    "\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        # Guardamos la lista con los paths de los documentos.\n",
    "        p = os.path.join(dir, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "\n",
    "        # Guardamos el diccionario con los postings.\n",
    "        p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings, f)\n",
    "\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "\n",
    "        # Cargamos de disco la lista con los paths de los documentos y\n",
    "        # el diccionario con los postings.\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.PATHS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "            p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.dict_postings = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucORmwfCh4Um"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbc9ng8e28p"
   },
   "source": [
    "## Ejercicio 2.2: Construcción del índice\n",
    "\n",
    "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "tHQ4UCf5pTw8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class RAMIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 2.2) #\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir=dir\n",
    "        self.index=RAMIndex(None)\n",
    "\n",
    "    def index_document(self, path, text):\n",
    "        text_terms=self.parser.parse(text)\n",
    "\n",
    "        self.index.add_doc(path)\n",
    "        doc_id=self.index.ndocs()-1\n",
    "\n",
    "        term_freq=Counter(text_terms)\n",
    "        for term, freq in term_freq.items():\n",
    "            self.index.add_posting(term, doc_id, freq)\n",
    "\n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_mxRswZh74N"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOWgbqZpV01"
   },
   "source": [
    "# Ejercicio 3: Índice en disco* (1pt)\n",
    "\n",
    "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
    "\n",
    "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
    "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
    "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
    "\n",
    "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
    "\n",
    "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "br7yqFrnpZYl"
   },
   "outputs": [],
   "source": [
    "class DiskIndex(Index):\n",
    "    # Your new code here (exercise 3*) #\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DiskIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 3*) #\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTje0viiM9I"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxuclLwpaM-"
   },
   "source": [
    "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
    "\n",
    "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "e3uq565SpfSA"
   },
   "outputs": [],
   "source": [
    "class ProximitySearcher(Searcher):\n",
    "    # Your new code here (exercise 4*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        raise NotImplementedError\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4hWTstiOIT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPPWV7pepf85"
   },
   "source": [
    "# Ejercicio 5: Índice posicional* (1pt)\n",
    "\n",
    "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "zg8MIMpipih1"
   },
   "outputs": [],
   "source": [
    "class PositionalIndex(Index):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Note that it may be better to inherit from a different class\n",
    "    # if your index extends a particular type of index\n",
    "    # For example: PositionalIndex(RAMIndex)\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class PositionalIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Same note as for PositionalIndex\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1gukouXiPV3"
   },
   "source": [
    "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HbYGn8pjKZ"
   },
   "source": [
    "# Ejercicio 6: PageRank (1pt)\n",
    "\n",
    "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
    "\n",
    "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "_aQE7SBgpk1S"
   },
   "outputs": [],
   "source": [
    "class PagerankDocScorer():\n",
    "    def __init__(self, graphfile, r, n_iter):\n",
    "        # Your new code here (exercise 6) #\n",
    "        # Format of graphfile:\n",
    "        #  node1 node2\n",
    "        # TODO #\n",
    "        raise NotImplementedError\n",
    "    def rank(self, cutoff):\n",
    "        # TODO #\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ZT7seCiQiT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zXhPtFzon72"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que esta celda funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "fTdDacCRn0u6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.012729167938232422 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.012476682662963867 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.010412454605102539 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0008931159973144531 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0006268024444580078 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0006563663482666016 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0005066394805908203 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 5.91278076171875e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa dd\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0007338523864746094 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0006945133209228516 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0006034374237060547 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007956027984619141 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002739429473876953 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007450580596923828 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00028133392333984375 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0006830692291259766 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00024437904357910156 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 7.128715515136719e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.221366882324219e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0019860267639160156 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.002519845962524414 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0015902519226074219 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005497932434082031 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0001678466796875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0003361701965332031 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0001685619354248047 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004887580871582031 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002791881561279297 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 9.34600830078125e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 7.033348083496094e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.010341882705688477 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.011183977127075195 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.009833812713623047 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0007123947143554688 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0004818439483642578 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0004324913024902344 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.00039768218994140625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 5.14984130859375e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011589527130126953 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0013117790222167969 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.001138925552368164 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004756450653076172 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002384185791015625 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00035762786865234375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002434253692626953 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003685951232910156 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002269744873046875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 5.459785461425781e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 4.2438507080078125e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.003093242645263672 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011157989501953125 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011916160583496094 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00044083595275878906 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00023698806762695312 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003523826599121094 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00024318695068359375 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004086494445800781 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00023818016052246094 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 5.5789947509765625e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 4.4345855712890625e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 2 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.010915994644165039 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.012633085250854492 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.013689756393432617 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0009186267852783203 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.000476837158203125 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0004336833953857422 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0005052089691162109 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 5.269050598144531e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.002619504928588867 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003607511520385742 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0024335384368896484 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0012924671173095703 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00040340423583984375 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0021064281463623047 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005199909210205078 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.002107381820678711 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006499290466308594 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001380443572998047 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00013327598571777344 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0039048194885253906 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.004301786422729492 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.005838632583618164 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0029566287994384766 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005643367767333984 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0010678768157958984 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003674030303955078 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0013728141784667969 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005633831024169922 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 9.989738464355469e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.128715515136719e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.61045503616333 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.603055477142334 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.8404057025909424 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.780747890472412 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6061\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.016872644424438477 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6061\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.019778966903686523 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6061\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.01955556869506836 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 5948\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 7\n",
      "  Total frequency of word \"wikipedia\" in the collection: 31 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 7), (1, 15)]\n",
      "Done ( 0.00021028518676757812 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for information probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003650188446044922 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004241466522216797 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.005106925964355469 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0012264251708984375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00045371055603027344 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0021250247955322266 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005056858062744141 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006272792816162109 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00030040740966796875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.02455066848200471 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735293253477276 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009534388699273736 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.152557373046875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.02455066848200471 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735293253477276 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009534388699273736 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.7697296142578125e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for probability information\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003688812255859375 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004248857498168945 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0037343502044677734 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007190704345703125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002968311309814453 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.001771688461303711 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005185604095458984 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005934238433837891 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00037479400634765625 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.02455066848200471 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735293253477276 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009534388699273736 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.104873657226562e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.02455066848200471 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735293253477276 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009534388699273736 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.841255187988281e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for higher probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004334688186645508 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0035600662231445312 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004088878631591797 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006716251373291016 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.000347137451171875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0017037391662597656 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00031638145446777344 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005619525909423828 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002918243408203125 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.02810518670437446 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01218570858994911 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005696737659032651 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.937980651855469e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.02810518670437446 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01218570858994911 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005696737659032651 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.6743621826171875e-05 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/urls.txt'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 2.801642417907715 seconds ---\n",
      "\tWhooshForwardIndex: 2.6960737705230713 seconds ---\n",
      "\tWhooshPositionalIndex: 2.676213264465332 seconds ---\n",
      "\tRAMIndex: 1.6754539012908936 seconds ---\n",
      "DiskIndexBuilder still not implemented\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.0014209747314453125 seconds ---\n",
      "\tWhooshForwardIndex: 0.0011434555053710938 seconds ---\n",
      "\tWhooshPositionalIndex: 0.0012722015380859375 seconds ---\n",
      "\tRAMIndex: 0.004573345184326172 seconds ---\n",
      "DiskIndex still not implemented\n",
      "  Disk space...\n",
      "\tWhooshIndex: 1090472 space ---\n",
      "\tWhooshForwardIndex: 1167751 space ---\n",
      "\tWhooshPositionalIndex: 1247566 space ---\n",
      "\tRAMIndex: 126661 space ---\n",
      "\tDiskIndex: 0 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'information probability'\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003974437713623047 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.00554203987121582 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0008111000061035156 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0008325576782226562 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00032973289489746094 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.00034689903259277344 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.02455066848200471 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735293253477276 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009534388699273736 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.343292236328125e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 9.298324584960938e-05 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'probability information'\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0047130584716796875 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.0068149566650390625 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0009005069732666016 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0009233951568603516 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004067420959472656 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0004279613494873047 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.02455066848200471 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01735293253477276 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009534388699273736 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 9.179115295410156e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.00011610984802246094 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'higher probability'\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0036110877990722656 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.00574803352355957 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0008780956268310547 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0008969306945800781 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003230571746826172 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0003399848937988281 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.02810518670437446 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01218570858994911 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005696737659032651 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.033348083496094e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 8.845329284667969e-05 seconds ---\n",
      "----------------------------\n",
      "Testing PageRank\n",
      "PagerankDocScorer still not implemented\n",
      "PagerankDocScorer still not implemented\n",
      "PagerankDocScorer still not implemented\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # We now test building different implementations of an index\n",
    "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_paths)\n",
    "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_paths)\n",
    "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_paths)\n",
    "    try:\n",
    "        test_build(RAMIndexBuilder(index_path + \"ram\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(DiskIndexBuilder(index_path + \"disk\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"PositionalIndexBuilder still not implemented\")\n",
    "\n",
    "    def catch_index(func, name, *args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except NotImplementedError:\n",
    "            print(name + \" still not implemented (index)\")\n",
    "            return None\n",
    "\n",
    "    # We now inspect all the implementations\n",
    "    indices = [\n",
    "            WhooshIndex(index_path + \"whoosh\"),\n",
    "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
    "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
    "            catch_index(lambda: RAMIndex(index_path + \"ram\"), \"RAMIndex\"),\n",
    "            catch_index(lambda: DiskIndex(index_path + \"disk\"), \"DiskIndex\"),\n",
    "            catch_index(lambda: PositionalIndex(index_path + \"pos\"), \"PositionalIndex\"),\n",
    "            ]\n",
    "    for index in indices:\n",
    "        if index:\n",
    "            test_read(index, word)\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Checking search results for %s\" % (query))\n",
    "        # Whoosh searcher can only work with its own indices\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        try:\n",
    "            test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher still not implemented\")\n",
    "        for index in indices:\n",
    "            if index:\n",
    "                # our searchers should work with any other index\n",
    "                test_search(SlowVSMSearcher(index), index, query, 5)\n",
    "                try:\n",
    "                    test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"TermBasedVSMSearcher still not implemented\")\n",
    "                try:\n",
    "                    test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"DocBasedVSMSearcher still not implemented\")\n",
    "        try:\n",
    "            test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher or PositionalIndex still not implemented\")\n",
    "\n",
    "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
    "    indices = list()\n",
    "\n",
    "    if analyse_performance:\n",
    "        # let's analyse index performance\n",
    "        test_index_performance(collection_paths, index_path)\n",
    "        # let's analyse search performance\n",
    "        for query in queries:\n",
    "            test_search_performance(collection_paths, index_path, query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # this function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # when we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    # more tests\n",
    "    doc_id = 0\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, index, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def disk_space(index_path: str) -> int:\n",
    "    space = 0\n",
    "    if os.path.isdir(index_path):\n",
    "        for f in os.listdir(index_path):\n",
    "            p = os.path.join(index_path, f)\n",
    "            if os.path.isfile(p):\n",
    "                space += os.path.getsize(p)\n",
    "    return space\n",
    "\n",
    "def test_index_performance (collection_paths: list, base_index_path: str):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing index performance on \" + str(collection_paths) + \" document collection\")\n",
    "\n",
    "    print(\"  Build time...\")\n",
    "    start_time = time.time()\n",
    "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = RAMIndexBuilder(base_index_path + \"ram\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = DiskIndexBuilder(base_index_path + \"disk\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "\n",
    "    print(\"  Load time...\")\n",
    "    start_time = time.time()\n",
    "    WhooshIndex(base_index_path + \"whoosh\")\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        RAMIndex(base_index_path + \"ram\")\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        DiskIndex(base_index_path + \"disk\")\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "\n",
    "    print(\"  Disk space...\")\n",
    "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
    "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
    "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
    "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
    "    print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
    "\n",
    "\n",
    "def test_search_performance (collection_paths: list, base_index_path: str, query: str, cutoff: int):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing search performance on \" + str(collection_paths) + \" document collection with query: '\" + query + \"'\")\n",
    "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
    "    try:\n",
    "        ram_index = RAMIndex(base_index_path + \"ram\")\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "        ram_index = None\n",
    "    try:\n",
    "        disk_index = DiskIndex(base_index_path + \"disk\")\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "        disk_index = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
    "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # let's test some combinations of ranking + index implementations\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "        print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if ram_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
    "            print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DocBasedVSMSearcher still not implemented\")\n",
    "\n",
    "def test_pagerank(graphs_root_dir, cutoff):\n",
    "    print(\"----------------------------\")\n",
    "    # we separate this function because it cannot work with all the collections\n",
    "    print(\"Testing PageRank\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "test_collection ([collections_root_dir + \"toy1/\"], index_root_dir + \"toy1/\", \"cc\", [\"aa dd\", \"aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy2/\"], index_root_dir + \"toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy1/\", collections_root_dir + \"toy2/\"], index_root_dir + \"toys/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"toy2/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"three_collections/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs10k.zip\"], index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], False)\n",
    "test_pagerank(\"./collections/\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JhJJSFiSl5"
   },
   "source": [
    "### Resumen de coste y rendimiento\n",
    "\n",
    "Hay que analizar las **diferencias de rendimiento** observadas entre las diferentes implementaciones que se han creado y probado para cada componente.\n",
    "\n",
    "En concreto, hay que reportar tiempo de indexado, consumo máximo de RAM y espacio en disco al construir el índice, y el tiempo de carga y consumo máximo de RAM al cargar el índice para cada una de las colecciones utilizadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "|      | Construcción | del | índice | Carga del | índice |\n",
    "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
    "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
    "| toy1 | | | | | |\n",
    "| toy2 | | | | | |\n",
    "| toys | | | | | |\n",
    "| 1K | | | | | |\n",
    "| 10K | | | | | |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
