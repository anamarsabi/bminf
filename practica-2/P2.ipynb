{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eq_QfGIGXC_"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Motores de búsqueda e indexación**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 21 / jueves 23 de febrero\n",
    "* Entrega: martes 28 / jueves 30 de marzo (14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Xu Chen\n",
    "Ana Martínez Sabiote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYT0Qlrnoy7l"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFY_K6_pA_J"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
    "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
    "*\tLa implementación de un método de búsqueda proximal.\n",
    "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
    "*\tLa implementación del algoritmo PageRank.\n",
    "\n",
    "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
    "\n",
    "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
    "\n",
    "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El **único buscador que no será intercambiable es el de Whoosh**, que sólo funcionará con sus propios índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
    "\n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la *celda de prueba* así como otros tests adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p2-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
    "\n",
    "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la **celda de prueba** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (ten en cuenta que, aquellos ejercicios que no se hayan realizado, lanzan una excepción que se captura en dicha celda, por lo que no debería ser necesario modificarla).\n",
    "\n",
    "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjq_DVVpDEL"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
    "Las clases parten del código de la práctica anterior.\n",
    "Igual que en la práctica 1, la **celda de prueba** (al final del enunciado) implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
    "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
    "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
    "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería mantenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases genéricas ya implementadas\n",
    "\n",
    "En la siguiente celda de código, se encuentran ya implementadas las clases *Index* y *Builder* de manera que facilite la creación de otros índices a partir de las mismas. \n",
    "\n",
    "Estudia esta implementación y compara las **decisiones de diseño** tomadas con las vuestras en la práctica anterior.\n",
    "Ten en cuenta que las funciones de TF e IDF están **sin implementar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xAKBQZLLqVXR"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Config(object):\n",
    "  # variables de clase\n",
    "  NORMS_FILE = \"docnorms.dat\"\n",
    "  PATHS_FILE = \"docpaths.dat\"\n",
    "  INDEX_FILE = \"serialindex.dat\"\n",
    "  DICTIONARY_FILE = \"dictionary.dat\"\n",
    "  POSTINGS_FILE = \"postings.dat\"\n",
    "\n",
    "class BasicParser:\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "# Parámetro freq: frecuencia de un término\n",
    "def tf(freq):\n",
    "    if freq > 0:\n",
    "        tf = 1 + math.log(freq, 2)\n",
    "    else:\n",
    "        tf = 0\n",
    "\n",
    "    return tf \n",
    "\n",
    "# Parámetros\n",
    "#    df: doc_freq(term) frecuencia de un término\n",
    "#    n: ndocs() número total de documentos\n",
    "def idf(df, n):\n",
    "    idf = math.log(( (n+1) / (df+0.5)), 2)\n",
    "    \n",
    "    return idf \n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        self.index = index\n",
    "        self.parser = parser\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, dir=None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        if dir: self.open(dir)\n",
    "    def add_doc(self, path):\n",
    "        self.docmap.append(path)  # Assumed to come in order\n",
    "    def doc_path(self, docid):\n",
    "        return self.docmap[docid]\n",
    "    def doc_module(self, docid):\n",
    "        if docid in self.modulemap:\n",
    "            return self.modulemap[docid]\n",
    "        return None\n",
    "    def ndocs(self):\n",
    "        return len(self.docmap)\n",
    "    def doc_freq(self, term):\n",
    "        return len(self.postings(term))\n",
    "    def term_freq(self, term, docID):\n",
    "        post = self.postings(term)\n",
    "        if post is None: return 0\n",
    "        for posting in post:\n",
    "            if posting[0] == docID:\n",
    "                return posting[1]\n",
    "        return 0\n",
    "    def total_freq(self, term):\n",
    "        freq = 0\n",
    "        for posting in self.postings(term):\n",
    "            freq += posting[1]\n",
    "        return freq\n",
    "    def postings(self, term):\n",
    "        # used in more efficient implementations\n",
    "        return list()\n",
    "    def positional_postings(self, term):\n",
    "        # used in positional implementations\n",
    "        return list()\n",
    "    def all_terms(self):\n",
    "        return list()\n",
    "    def save(self, dir):\n",
    "        if not self.modulemap: self.compute_modules()\n",
    "        p = os.path.join(dir, Config.NORMS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.modulemap, f)        \n",
    "    def open(self, dir):\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.NORMS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.modulemap = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "    def compute_modules(self):\n",
    "        for term in self.all_terms():\n",
    "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
    "            post = self.postings(term)\n",
    "            if post is None: continue\n",
    "            for docid, freq in post:\n",
    "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
    "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
    "        for docid in range(self.ndocs()):\n",
    "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
    "\n",
    "import shutil\n",
    "class Builder:\n",
    "    def __init__(self, dir, parser=BasicParser()):\n",
    "        if os.path.exists(dir): shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "        self.parser = parser\n",
    "    def build(self, path):\n",
    "        if zipfile.is_zipfile(path):\n",
    "            self.index_zip(path)\n",
    "        elif os.path.isdir(path):\n",
    "            self.index_dir(path)\n",
    "        else:\n",
    "            self.index_url_file(path)\n",
    "    def index_zip(self, filename):\n",
    "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
    "        for name in sorted(file.namelist()):\n",
    "            with file.open(name, \"r\", force_zip64=True) as f:\n",
    "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "        file.close()\n",
    "    def index_dir(self, dir):\n",
    "        for subdir, dirs, files in os.walk(dir):\n",
    "            for file in sorted(files):\n",
    "                path = os.path.join(dir, file)\n",
    "                with open(path, \"r\") as f:\n",
    "                    self.index_document(path, f.read())\n",
    "    def index_url_file(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            self.index_urls(line.rstrip('\\n') for line in f)\n",
    "    def index_urls(self, urls):\n",
    "        for url in urls:\n",
    "            self.index_document(url, BeautifulSoup(urlopen(url).read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "    def index_document(self, path, text):\n",
    "        raise NotImplementedError # to be implemented by child class\n",
    "    def commit(self):\n",
    "        raise NotImplementedError # to be implemented by child class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de buscador\n",
    "\n",
    "En la siguiente celda se encuentra una implementación de un buscador basado en coseno que es relativamente lento. En los siguientes ejercicios veremos formas de acelerar el proceso (sin cambiar los resultados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yMoae4N7y38C"
   },
   "outputs": [],
   "source": [
    "# from previous lab\n",
    "class SlowVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for docid in range(self.index.ndocs()):\n",
    "            score = self.score(docid, qterms)\n",
    "            if score:\n",
    "                ranking.push(self.index.doc_path(docid), score)\n",
    "        return ranking\n",
    "\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases Whoosh\n",
    "\n",
    "En la siguiente celda podrás encontrar la adaptación a nuestras interfaces de los índices de Whoosh, en concreto, de tres variantes que permite usar la librería (observa los distintos Schema's usados y qué metodos se han reimplementado en cada caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I-7gj9Rxx6LD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import whoosh\n",
    "except ModuleNotFoundError:\n",
    "  !pip install whoosh\n",
    "  import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "SimpleDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False))\n",
    "ForwardDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False,vector=Format))\n",
    "PositionalDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=True))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def __init__(self, dir, schema=SimpleDocument):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
    "        self.dir = dir\n",
    "\n",
    "    def index_document(self, p, text):\n",
    "        self.whoosh_writer.add_document(path=p, content=text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshForwardBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, ForwardDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshForwardIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshPositionalBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, PositionalDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshPositionalIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
    "    def total_freq(self, term):\n",
    "        return self.whoosh_reader.frequency(\"content\", term)\n",
    "    def doc_freq(self, term):\n",
    "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_reader.stored_fields(docid)['path']\n",
    "    def ndocs(self):\n",
    "        return self.whoosh_reader.doc_count()\n",
    "    def all_terms(self):\n",
    "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
    "    def postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshForwardIndex(WhooshIndex):\n",
    "    def term_freq(self, term, docID) -> int:\n",
    "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
    "            v = self.whoosh_reader.vector(docID, \"content\")\n",
    "            v.skip_to(term)\n",
    "            if v.id() == term:\n",
    "                return v.value_as(\"frequency\")\n",
    "        return 0\n",
    "\n",
    "class WhooshPositionalIndex(WhooshIndex):\n",
    "    def positional_postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, dir):\n",
    "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
    "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
    "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
    "    def search(self, query, cutoff):\n",
    "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
    "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_index.reader().stored_fields(docid)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3jRLNZmpEk_"
   },
   "source": [
    "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
    "\n",
    "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
    "\n",
    "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be3vDQNxdWbo"
   },
   "source": [
    "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
    "\n",
    "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ppr9PtZmduql"
   },
   "outputs": [],
   "source": [
    "class TermBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.1) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "        \n",
    "    def search(self, query, cutoff):\n",
    "        scores={}\n",
    "        query_terms=self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            for doc_id, freq in self.index.postings(term):\n",
    "                if doc_id not in scores:\n",
    "                    scores[doc_id]=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                else:\n",
    "                    scores[doc_id]+=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                    \n",
    "        for doc_id, freq in scores.items():\n",
    "            mod = self.index.doc_module(doc_id)\n",
    "            if mod:\n",
    "                scores[doc_id]=freq/mod\n",
    "            if scores[doc_id]:\n",
    "                ranking.push(self.index.doc_path(doc_id), scores[doc_id])\n",
    "                \n",
    "        return ranking\n",
    "\n",
    "        #dic.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YGEGm7haop"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ti8qGedgNB"
   },
   "source": [
    "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
    "\n",
    "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wzZ-6OG0dvwX"
   },
   "outputs": [],
   "source": [
    "class DocBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.2*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        raise NotImplementedError\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xYd4hzhukr"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXHr18Cdl2Q"
   },
   "source": [
    "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
    "\n",
    "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)), es decir, que permita almacenar un **número limitado de documentos** en memoria y su puntuación asociada. \n",
    "\n",
    "Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MOfT2yZGpMNi"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class SearchRanking:\n",
    "    # TODO: to be implemented as heap (exercise 1.3) #\n",
    "    def __init__(self, cutoff):\n",
    "        self.cutoff = cutoff\n",
    "        self.ranking = list()\n",
    "\n",
    "    def push(self, docid, score):\n",
    "        if len(self.ranking) < self.cutoff:\n",
    "            heapq.heappush(self.ranking, (score, docid))\n",
    "        else:\n",
    "            heapq.heappushpop(self.ranking, (score, docid))\n",
    "\n",
    "    def __iter__(self):\n",
    "        min_l = min(len(self.ranking), self.cutoff)\n",
    "        ## sort ranking\n",
    "        orderedRanking = sorted(self.ranking, reverse=True)\n",
    "\n",
    "        # Invertimos la tupla para que el docid sea el primer elemento y el score el segundo\n",
    "        orderedRanking = [(x[1], x[0]) for x in orderedRanking]\n",
    "        return iter(orderedRanking[0:min_l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDzjUp-hwNZ"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNkPcUjMpNRn"
   },
   "source": [
    "# Ejercicio 2: Índice en RAM (3pt)\n",
    "\n",
    "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
    "\n",
    "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
    "\n",
    "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVzsIg0Zev7a"
   },
   "source": [
    "## Ejercicio 2.1: Estructura de índice\n",
    "\n",
    "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
    "\n",
    "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VqSKneeSe2bN"
   },
   "outputs": [],
   "source": [
    "class RAMIndex(Index):\n",
    "    # Your new code here (exercise 2.1) #\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucORmwfCh4Um"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbc9ng8e28p"
   },
   "source": [
    "## Ejercicio 2.2: Construcción del índice\n",
    "\n",
    "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tHQ4UCf5pTw8"
   },
   "outputs": [],
   "source": [
    "class RAMIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 2.2) #\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_mxRswZh74N"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOWgbqZpV01"
   },
   "source": [
    "# Ejercicio 3: Índice en disco* (1pt)\n",
    "\n",
    "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
    "\n",
    "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
    "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
    "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
    "\n",
    "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
    "\n",
    "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "br7yqFrnpZYl"
   },
   "outputs": [],
   "source": [
    "class DiskIndex(Index):\n",
    "    # Your new code here (exercise 3*) #\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DiskIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 3*) #\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTje0viiM9I"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxuclLwpaM-"
   },
   "source": [
    "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
    "\n",
    "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e3uq565SpfSA"
   },
   "outputs": [],
   "source": [
    "class ProximitySearcher(Searcher):\n",
    "    # Your new code here (exercise 4*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        raise NotImplementedError\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4hWTstiOIT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPPWV7pepf85"
   },
   "source": [
    "# Ejercicio 5: Índice posicional* (1pt)\n",
    "\n",
    "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zg8MIMpipih1"
   },
   "outputs": [],
   "source": [
    "class PositionalIndex(Index):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Note that it may be better to inherit from a different class\n",
    "    # if your index extends a particular type of index\n",
    "    # For example: PositionalIndex(RAMIndex)\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class PositionalIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Same note as for PositionalIndex\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1gukouXiPV3"
   },
   "source": [
    "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HbYGn8pjKZ"
   },
   "source": [
    "# Ejercicio 6: PageRank (1pt)\n",
    "\n",
    "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
    "\n",
    "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_aQE7SBgpk1S"
   },
   "outputs": [],
   "source": [
    "class PagerankDocScorer():\n",
    "    def __init__(self, graphfile, r, n_iter):\n",
    "        # Your new code here (exercise 6) #\n",
    "        # Format of graphfile:\n",
    "        #  node1 node2\n",
    "        # TODO #\n",
    "        raise NotImplementedError\n",
    "    def rank(self, cutoff):\n",
    "        # TODO #\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ZT7seCiQiT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zXhPtFzon72"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que esta celda funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fTdDacCRn0u6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.01540231704711914 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.012642621994018555 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.015662670135498047 seconds )\n",
      "\n",
      "RAMIndexBuilder still not implemented\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "RAMIndex still not implemented (index)\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.000949859619140625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0008108615875244141 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0004987716674804688 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa dd\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0013289451599121094 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0012257099151611328 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0012502670288085938 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0008556842803955078 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002644062042236328 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.000629425048828125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00044798851013183594 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0013117790222167969 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005548000335693359 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003287076950073242 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.004136562347412109 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0033795833587646484 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0011470317840576172 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0003528594970703125 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005893707275390625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00030803680419921875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007040500640869141 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002655982971191406 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.012802362442016602 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.014726877212524414 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.013265609741210938 seconds )\n",
      "\n",
      "RAMIndexBuilder still not implemented\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "RAMIndex still not implemented (index)\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0006253719329833984 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0005261898040771484 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0004792213439941406 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0021092891693115234 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0015015602111816406 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0017590522766113281 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006129741668701172 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003838539123535156 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.000446319580078125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00028061866760253906 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00044608116149902344 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002620220184326172 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0012922286987304688 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.001634359359741211 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0013005733489990234 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0009026527404785156 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003859996795654297 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007243156433105469 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003733634948730469 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00046062469482421875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00028705596923828125 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 2 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.012562274932861328 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.020199298858642578 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.016687631607055664 seconds )\n",
      "\n",
      "RAMIndexBuilder still not implemented\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "RAMIndex still not implemented (index)\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0007493495941162109 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0006628036499023438 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0010998249053955078 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003261566162109375 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00471806526184082 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0054857730865478516 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0014731884002685547 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00037741661071777344 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00118255615234375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00041484832763671875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.001453399658203125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003948211669921875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0035164356231689453 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.004416465759277344 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003217458724975586 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0012814998626708984 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003707408905029297 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00096893310546875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004737377166748047 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.001299142837524414 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005083084106445312 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.079268455505371 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.2981746196746826 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.8653676509857178 seconds )\n",
      "\n",
      "RAMIndexBuilder still not implemented\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "RAMIndex still not implemented (index)\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6061\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.07518124580383301 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6061\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.04292488098144531 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6061\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.02906012535095215 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for information probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0046482086181640625 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0045049190521240234 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004405975341796875 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0013916492462158203 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005586147308349609 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.002181529998779297 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004131793975830078 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007481575012207031 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003771781921386719 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for probability information\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0036668777465820312 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004635334014892578 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.006196498870849609 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0015859603881835938 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003876686096191406 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.002184152603149414 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00038123130798339844 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00069427490234375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003399848937988281 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for higher probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00632023811340332 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.006783485412597656 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00672602653503418 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0013239383697509766 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00038433074951171875 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003271341323852539 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0005147457122802734 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0009591579437255859 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003426074981689453 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/urls.txt'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 2.8339836597442627 seconds ---\n",
      "\tWhooshForwardIndex: 3.219876766204834 seconds ---\n",
      "\tWhooshPositionalIndex: 2.915175437927246 seconds ---\n",
      "RAMIndexBuilder still not implemented\n",
      "DiskIndexBuilder still not implemented\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.002147197723388672 seconds ---\n",
      "\tWhooshForwardIndex: 0.0012242794036865234 seconds ---\n",
      "\tWhooshPositionalIndex: 0.001529693603515625 seconds ---\n",
      "RAMIndex still not implemented\n",
      "DiskIndex still not implemented\n",
      "  Disk space...\n",
      "\tWhooshIndex: 1090475 space ---\n",
      "\tWhooshForwardIndex: 1167751 space ---\n",
      "\tWhooshPositionalIndex: 1247569 space ---\n",
      "\tRAMIndex: 0 space ---\n",
      "\tDiskIndex: 0 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'information probability'\n",
      "RAMIndex still not implemented\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.009491443634033203 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.010713338851928711 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0008234977722167969 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0008535385131835938 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003323554992675781 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.00034880638122558594 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'probability information'\n",
      "RAMIndex still not implemented\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9260926766599984 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.763336093697813 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.100560793424447 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0031938552856445312 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.0051767826080322266 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0015714168548583984 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0016155242919921875 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.02433442081241171 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01742636905300994 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.00960095693372997 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0008826255798339844 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0009222030639648438 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'higher probability'\n",
      "RAMIndex still not implemented\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.8853801442592566 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.527709001336885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7309979294567825 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0043506622314453125 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.007404327392578125 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0010623931884765625 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0010933876037597656 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.0279369117206049 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012274735996576885 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005736511763076787 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003304481506347656 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.00034928321838378906 seconds ---\n",
      "=================================================================\n",
      "Testing indices and search on 3 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 98.99917030334473 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 95.6464307308197 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 97.89071393013 seconds )\n",
      "\n",
      "RAMIndexBuilder still not implemented\n",
      "DiskIndexBuilder still not implemented\n",
      "PositionalIndexBuilder still not implemented\n",
      "RAMIndex still not implemented (index)\n",
      "DiskIndex still not implemented (index)\n",
      "PositionalIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 120084\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.36414098739624023 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 120084\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.34790706634521484 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 120084\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.34276509284973145 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for obama family tree\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.51918010541621 \t clueweb09-en0010-79-2218.html\n",
      "15.922331961045302 \t clueweb09-en0010-57-32937.html\n",
      "15.843320371669872 \t clueweb09-en0001-02-21241.html\n",
      "15.647245630689188 \t clueweb09-en0008-45-29117.html\n",
      "15.591036189822873 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.06075263023376465 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "16.51918010541621 \t clueweb09-en0010-79-2218.html\n",
      "15.922331961045302 \t clueweb09-en0010-57-32937.html\n",
      "15.843320371669872 \t clueweb09-en0001-02-21241.html\n",
      "15.647245630689188 \t clueweb09-en0008-45-29117.html\n",
      "15.591036189822873 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.04693937301635742 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "16.51918010541621 \t clueweb09-en0010-79-2218.html\n",
      "15.922331961045302 \t clueweb09-en0010-57-32937.html\n",
      "15.843320371669872 \t clueweb09-en0001-02-21241.html\n",
      "15.647245630689188 \t clueweb09-en0008-45-29117.html\n",
      "15.591036189822873 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.06639480590820312 seconds )\n",
      "\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 1.489673137664795 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.020963668823242188 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.6809432506561279 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.014969587326049805 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 3.6994240283966064 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.026972293853759766 seconds )\n",
      "\n",
      "DocBasedVSMSearcher still not implemented\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/toy2/', './collections/urls.txt', './collections/docs1k.zip'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 91.45126175880432 seconds ---\n",
      "\tWhooshForwardIndex: 86.55278468132019 seconds ---\n",
      "\tWhooshPositionalIndex: 90.91233682632446 seconds ---\n",
      "RAMIndexBuilder still not implemented\n",
      "DiskIndexBuilder still not implemented\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.009949207305908203 seconds ---\n",
      "\tWhooshForwardIndex: 0.008159160614013672 seconds ---\n",
      "\tWhooshPositionalIndex: 0.012596607208251953 seconds ---\n",
      "RAMIndex still not implemented\n",
      "DiskIndex still not implemented\n",
      "  Disk space...\n",
      "\tWhooshIndex: 23219684 space ---\n",
      "\tWhooshForwardIndex: 29053167 space ---\n",
      "\tWhooshPositionalIndex: 31777119 space ---\n",
      "\tRAMIndex: 0 space ---\n",
      "\tDiskIndex: 0 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/toy2/', './collections/urls.txt', './collections/docs1k.zip'] document collection with query: 'obama family tree'\n",
      "RAMIndex still not implemented\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.51918010541621 \t clueweb09-en0010-79-2218.html\n",
      "15.922331961045302 \t clueweb09-en0010-57-32937.html\n",
      "15.843320371669872 \t clueweb09-en0001-02-21241.html\n",
      "15.647245630689188 \t clueweb09-en0008-45-29117.html\n",
      "15.591036189822873 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.049938201904296875 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.06633186340332031 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 1.857024908065796 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 1.8570492267608643 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2856696452168881 \t clueweb09-en0010-79-2218.html\n",
      "0.22713452416497795 \t clueweb09-en0009-30-2768.html\n",
      "0.22566699652302635 \t clueweb09-en0001-02-21241.html\n",
      "0.22464823189431904 \t clueweb09-en0009-30-2441.html\n",
      "0.2242752393107756 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.015761375427246094 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.01578068733215332 seconds ---\n",
      "----------------------------\n",
      "Testing PageRank\n",
      "PagerankDocScorer still not implemented\n",
      "PagerankDocScorer still not implemented\n",
      "PagerankDocScorer still not implemented\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # We now test building different implementations of an index\n",
    "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_paths)\n",
    "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_paths)\n",
    "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_paths)\n",
    "    try:\n",
    "        test_build(RAMIndexBuilder(index_path + \"ram\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(DiskIndexBuilder(index_path + \"disk\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"PositionalIndexBuilder still not implemented\")\n",
    "\n",
    "    def catch_index(func, name, *args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except NotImplementedError:\n",
    "            print(name + \" still not implemented (index)\")\n",
    "            return None\n",
    "\n",
    "    # We now inspect all the implementations\n",
    "    indices = [\n",
    "            WhooshIndex(index_path + \"whoosh\"),\n",
    "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
    "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
    "            catch_index(lambda: RAMIndex(index_path + \"ram\"), \"RAMIndex\"),\n",
    "            catch_index(lambda: DiskIndex(index_path + \"disk\"), \"DiskIndex\"),\n",
    "            catch_index(lambda: PositionalIndex(index_path + \"pos\"), \"PositionalIndex\"),\n",
    "            ]\n",
    "    for index in indices:\n",
    "        if index:\n",
    "            test_read(index, word)\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Checking search results for %s\" % (query))\n",
    "        # Whoosh searcher can only work with its own indices\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        try:\n",
    "            test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher still not implemented\")\n",
    "        for index in indices:\n",
    "            if index:\n",
    "                # our searchers should work with any other index\n",
    "                test_search(SlowVSMSearcher(index), index, query, 5)\n",
    "                try:\n",
    "                    test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"TermBasedVSMSearcher still not implemented\")\n",
    "                try:\n",
    "                    test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"DocBasedVSMSearcher still not implemented\")\n",
    "        try:\n",
    "            test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher or PositionalIndex still not implemented\")\n",
    "\n",
    "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
    "    indices = list()\n",
    "\n",
    "    if analyse_performance:\n",
    "        # let's analyse index performance\n",
    "        test_index_performance(collection_paths, index_path)\n",
    "        # let's analyse search performance\n",
    "        for query in queries:\n",
    "            test_search_performance(collection_paths, index_path, query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # this function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # when we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    # more tests\n",
    "    doc_id = 0\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, index, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def disk_space(index_path: str) -> int:\n",
    "    space = 0\n",
    "    if os.path.isdir(index_path):\n",
    "        for f in os.listdir(index_path):\n",
    "            p = os.path.join(index_path, f)\n",
    "            if os.path.isfile(p):\n",
    "                space += os.path.getsize(p)\n",
    "    return space\n",
    "\n",
    "def test_index_performance (collection_paths: list, base_index_path: str):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing index performance on \" + str(collection_paths) + \" document collection\")\n",
    "\n",
    "    print(\"  Build time...\")\n",
    "    start_time = time.time()\n",
    "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = RAMIndexBuilder(base_index_path + \"ram\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = DiskIndexBuilder(base_index_path + \"disk\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "\n",
    "    print(\"  Load time...\")\n",
    "    start_time = time.time()\n",
    "    WhooshIndex(base_index_path + \"whoosh\")\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        RAMIndex(base_index_path + \"ram\")\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        DiskIndex(base_index_path + \"disk\")\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "\n",
    "    print(\"  Disk space...\")\n",
    "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
    "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
    "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
    "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
    "    print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
    "\n",
    "\n",
    "def test_search_performance (collection_paths: list, base_index_path: str, query: str, cutoff: int):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing search performance on \" + str(collection_paths) + \" document collection with query: '\" + query + \"'\")\n",
    "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
    "    try:\n",
    "        ram_index = RAMIndex(base_index_path + \"ram\")\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "        ram_index = None\n",
    "    try:\n",
    "        disk_index = DiskIndex(base_index_path + \"disk\")\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "        disk_index = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
    "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # let's test some combinations of ranking + index implementations\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "        print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if ram_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
    "            print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DocBasedVSMSearcher still not implemented\")\n",
    "\n",
    "def test_pagerank(graphs_root_dir, cutoff):\n",
    "    print(\"----------------------------\")\n",
    "    # we separate this function because it cannot work with all the collections\n",
    "    print(\"Testing PageRank\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "test_collection ([collections_root_dir + \"toy1/\"], index_root_dir + \"toy1/\", \"cc\", [\"aa dd\", \"aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy2/\"], index_root_dir + \"toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy1/\", collections_root_dir + \"toy2/\"], index_root_dir + \"toys/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
    "test_collection ([collections_root_dir + \"toy2/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"three_collections/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs10k.zip\"], index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], False)\n",
    "test_pagerank(\"./collections/\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JhJJSFiSl5"
   },
   "source": [
    "### Resumen de coste y rendimiento\n",
    "\n",
    "Hay que analizar las **diferencias de rendimiento** observadas entre las diferentes implementaciones que se han creado y probado para cada componente.\n",
    "\n",
    "En concreto, hay que reportar tiempo de indexado, consumo máximo de RAM y espacio en disco al construir el índice, y el tiempo de carga y consumo máximo de RAM al cargar el índice para cada una de las colecciones utilizadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "|      | Construcción | del | índice | Carga del | índice |\n",
    "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
    "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
    "| toy1 | | | | | |\n",
    "| toy2 | | | | | |\n",
    "| toys | | | | | |\n",
    "| 1K | | | | | |\n",
    "| 10K | | | | | |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
