{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eq_QfGIGXC_"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Motores de búsqueda e indexación**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 21 / jueves 23 de febrero\n",
    "* Entrega: martes 28 / jueves 30 de marzo (14:00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Xu Chen Xu <br>\n",
    "Ana Martínez Sabiote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYT0Qlrnoy7l"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFY_K6_pA_J"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
    "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
    "*\tLa implementación de un método de búsqueda proximal.\n",
    "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
    "*\tLa implementación del algoritmo PageRank.\n",
    "\n",
    "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
    "\n",
    "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
    "\n",
    "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El **único buscador que no será intercambiable es el de Whoosh**, que sólo funcionará con sus propios índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
    "\n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la *celda de prueba* así como otros tests adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p2-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
    "\n",
    "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la **celda de prueba** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (ten en cuenta que, aquellos ejercicios que no se hayan realizado, lanzan una excepción que se captura en dicha celda, por lo que no debería ser necesario modificarla).\n",
    "\n",
    "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjq_DVVpDEL"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
    "Las clases parten del código de la práctica anterior.\n",
    "Igual que en la práctica 1, la **celda de prueba** (al final del enunciado) implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
    "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
    "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
    "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería mantenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases genéricas ya implementadas\n",
    "\n",
    "En la siguiente celda de código, se encuentran ya implementadas las clases *Index* y *Builder* de manera que facilite la creación de otros índices a partir de las mismas. \n",
    "\n",
    "Estudia esta implementación y compara las **decisiones de diseño** tomadas con las vuestras en la práctica anterior.\n",
    "Ten en cuenta que las funciones de TF e IDF están **sin implementar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xAKBQZLLqVXR"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Config(object):\n",
    "  # variables de clase\n",
    "  NORMS_FILE = \"docnorms.dat\"\n",
    "  PATHS_FILE = \"docpaths.dat\"\n",
    "  INDEX_FILE = \"serialindex.dat\"\n",
    "  DICTIONARY_FILE = \"dictionary.dat\"\n",
    "  POSTINGS_FILE = \"postings.dat\"\n",
    "\n",
    "class BasicParser:\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "# Parámetro freq: frecuencia de un término\n",
    "def tf(freq):\n",
    "    if freq > 0:\n",
    "        tf = 1 + math.log(freq, 2)\n",
    "    else:\n",
    "        tf = 0\n",
    "\n",
    "    return tf \n",
    "\n",
    "# Parámetros\n",
    "#    df: doc_freq(term) frecuencia de un término\n",
    "#    n: ndocs() número total de documentos\n",
    "def idf(df, n):\n",
    "    idf = math.log(( (n+1) / (df+0.5)), 2)\n",
    "    \n",
    "    return idf \n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        self.index = index\n",
    "        self.parser = parser\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, dir=None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        if dir: self.open(dir)\n",
    "    def add_doc(self, path):\n",
    "        self.docmap.append(path)  # Assumed to come in order\n",
    "    def doc_path(self, docid):\n",
    "        return self.docmap[docid]\n",
    "    def doc_module(self, docid):\n",
    "        if docid in self.modulemap:\n",
    "            return self.modulemap[docid]\n",
    "        return None\n",
    "    def ndocs(self):\n",
    "        return len(self.docmap)\n",
    "    def doc_freq(self, term):\n",
    "        return len(self.postings(term))\n",
    "    def term_freq(self, term, docID):\n",
    "        post = self.postings(term)\n",
    "        if post is None: return 0\n",
    "        for posting in post:\n",
    "            if posting[0] == docID:\n",
    "                return posting[1]\n",
    "        return 0\n",
    "    def total_freq(self, term):\n",
    "        freq = 0\n",
    "        for posting in self.postings(term):\n",
    "            freq += posting[1]\n",
    "        return freq\n",
    "    def postings(self, term):\n",
    "        # used in more efficient implementations\n",
    "        return list()\n",
    "    def positional_postings(self, term):\n",
    "        # used in positional implementations\n",
    "        return list()\n",
    "    def all_terms(self):\n",
    "        return list()\n",
    "    def save(self, dir):\n",
    "        if not self.modulemap: self.compute_modules()\n",
    "        p = os.path.join(dir, Config.NORMS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.modulemap, f)        \n",
    "    def open(self, dir):\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.NORMS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.modulemap = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "    def compute_modules(self):\n",
    "        for term in self.all_terms():\n",
    "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
    "            post = self.postings(term)\n",
    "            if post is None: continue\n",
    "            for docid, freq in post:\n",
    "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
    "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
    "        for docid in range(self.ndocs()):\n",
    "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
    "\n",
    "import shutil\n",
    "class Builder:\n",
    "    def __init__(self, dir, parser=BasicParser()):\n",
    "        if os.path.exists(dir): shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "        self.parser = parser\n",
    "    def build(self, path):\n",
    "        if zipfile.is_zipfile(path):\n",
    "            self.index_zip(path)\n",
    "        elif os.path.isdir(path):\n",
    "            self.index_dir(path)\n",
    "        else:\n",
    "            self.index_url_file(path)\n",
    "    def index_zip(self, filename):\n",
    "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
    "        for name in sorted(file.namelist()):\n",
    "            with file.open(name, \"r\", force_zip64=True) as f:\n",
    "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "        file.close()\n",
    "    def index_dir(self, dir):\n",
    "        for subdir, dirs, files in os.walk(dir):\n",
    "            for file in sorted(files):\n",
    "                path = os.path.join(dir, file)\n",
    "                with open(path, \"r\") as f:\n",
    "                    self.index_document(path, f.read())\n",
    "    def index_url_file(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            self.index_urls(line.rstrip('\\n') for line in f)\n",
    "    def index_urls(self, urls):\n",
    "        for url in urls:\n",
    "            self.index_document(url, BeautifulSoup(urlopen(url).read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "    def index_document(self, path, text):\n",
    "        raise NotImplementedError # to be implemented by child class\n",
    "    def commit(self):\n",
    "        raise NotImplementedError # to be implemented by child class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de buscador\n",
    "\n",
    "En la siguiente celda se encuentra una implementación de un buscador basado en coseno que es relativamente lento. En los siguientes ejercicios veremos formas de acelerar el proceso (sin cambiar los resultados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "yMoae4N7y38C"
   },
   "outputs": [],
   "source": [
    "# from previous lab\n",
    "class SlowVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for docid in range(self.index.ndocs()):\n",
    "            score = self.score(docid, qterms)\n",
    "            if score:\n",
    "                ranking.push(self.index.doc_path(docid), score)\n",
    "        return ranking\n",
    "\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases Whoosh\n",
    "\n",
    "En la siguiente celda podrás encontrar la adaptación a nuestras interfaces de los índices de Whoosh, en concreto, de tres variantes que permite usar la librería (observa los distintos Schema's usados y qué metodos se han reimplementado en cada caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "I-7gj9Rxx6LD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import whoosh\n",
    "except ModuleNotFoundError:\n",
    "  !pip install whoosh\n",
    "  import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "SimpleDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False))\n",
    "ForwardDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False,vector=Format))\n",
    "PositionalDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=True))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def __init__(self, dir, schema=SimpleDocument):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
    "        self.dir = dir\n",
    "\n",
    "    def index_document(self, p, text):\n",
    "        self.whoosh_writer.add_document(path=p, content=text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshForwardBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, ForwardDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshForwardIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshPositionalBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, PositionalDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshPositionalIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
    "    def total_freq(self, term):\n",
    "        return self.whoosh_reader.frequency(\"content\", term)\n",
    "    def doc_freq(self, term):\n",
    "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_reader.stored_fields(docid)['path']\n",
    "    def ndocs(self):\n",
    "        return self.whoosh_reader.doc_count()\n",
    "    def all_terms(self):\n",
    "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
    "    def postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshForwardIndex(WhooshIndex):\n",
    "    def term_freq(self, term, docID) -> int:\n",
    "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
    "            v = self.whoosh_reader.vector(docID, \"content\")\n",
    "            v.skip_to(term)\n",
    "            if v.id() == term:\n",
    "                return v.value_as(\"frequency\")\n",
    "        return 0\n",
    "\n",
    "class WhooshPositionalIndex(WhooshIndex):\n",
    "    def positional_postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, dir):\n",
    "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
    "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
    "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
    "    def search(self, query, cutoff):\n",
    "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
    "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_index.reader().stored_fields(docid)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3jRLNZmpEk_"
   },
   "source": [
    "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
    "\n",
    "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
    "\n",
    "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be3vDQNxdWbo"
   },
   "source": [
    "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
    "\n",
    "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ppr9PtZmduql"
   },
   "outputs": [],
   "source": [
    "class TermBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.1) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "        \n",
    "    def search(self, query, cutoff):\n",
    "        scores={}\n",
    "        query_terms=self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            for doc_id, freq in self.index.postings(term):\n",
    "                if doc_id not in scores:\n",
    "                    scores[doc_id]=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                else:\n",
    "                    scores[doc_id]+=tf(freq)*idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "                    \n",
    "        for doc_id, freq in scores.items():\n",
    "            mod = self.index.doc_module(doc_id)\n",
    "\n",
    "            if mod:\n",
    "                scores[doc_id]=freq/mod\n",
    "            if scores[doc_id]:\n",
    "                ranking.push(self.index.doc_path(doc_id), scores[doc_id])\n",
    "\n",
    "        return ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YGEGm7haop"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**TermBasedVSMSearcher**: Clase que hereda de Searcher. Implementa el coseno como función de ránking según la búsqueda orientada a términos.\n",
    "**Métodos:**\n",
    "* **search(query, cutoff)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *query*: string que contiene la consulta a buscar.\n",
    "    * *cutoff*: número de resultados que queremos que devuelva el buscador\n",
    "<br/><br/>\n",
    "Hemos implementado el TermBasedVSMSearcher utilizando un diccionario. Hemos iterado sobre los términos de la consulta y para cada término hemos iterado en su lista de postings (que almacena la frecuencia del término en cada documento, identificado por su doc_id) para calcular el score de dicho término asociado a cada documento. La clave del diccionario es el doc_id y el valor es el score. Cuando se calcula el score del documento, si el doc_id no está en el diccionario se añade a él, sino, se incrementa el valor de la clave doc:id ya existente. De esta manera, en el diccionario tenemos el acumulador de score de cada documento tal y como es necesario para la búsqueda orientada a términos. \n",
    "Finalmente, iteramos en el diccionario y obtenemos el score final para cada documento dividiendo por el módulo de cada documento y finalmente añadiéndolos al SearchRanking que se devuelve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ti8qGedgNB"
   },
   "source": [
    "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
    "\n",
    "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "wzZ-6OG0dvwX"
   },
   "outputs": [],
   "source": [
    "class DocBasedVSMSearcher(Searcher):\n",
    "    # Your new code here (exercise 1.2*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        heap_postings=[]\n",
    "        query_terms=self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "\n",
    "        # Diccionario que contendrá los postings de cada término\n",
    "        # ordenados por doc_id. La clave es el término y el valor\n",
    "        # es una lista de tuplas (doc_id, tf-idf).\n",
    "        terms_postings_dict={}\n",
    "\n",
    "        ndocs_index = self.index.ndocs()\n",
    "\n",
    "        # Primero calculamos el tf-idf para cada término y cada documento\n",
    "        # y los guardamos en el diccionario, ordenando los postings por\n",
    "        # doc_id.\n",
    "        for term in query_terms:\n",
    "            postings = self.index.postings(term)\n",
    "\n",
    "            doc_freq_of_term = self.index.doc_freq(term)\n",
    "            score_postings = []\n",
    "\n",
    "            # Calculamos el tf-idf para cada termino y cada documento\n",
    "            for posting in postings:\n",
    "                doc_id = posting[0]\n",
    "                freq = posting[1]\n",
    "                score = tf(freq) * idf(doc_freq_of_term, ndocs_index)\n",
    "                score_postings.append((doc_id, score))\n",
    "\n",
    "            # Guardamos la lista de postings ordenada por doc_id\n",
    "            # en el diccionario.\n",
    "            terms_postings_dict[term] = sorted(score_postings, key=lambda x: x[0])\n",
    "\n",
    "        # Inicializamos el heap con los primeros elementos de cada\n",
    "        # lista de postings. El heap contendrá tuplas (doc_id, score, term).\n",
    "        # El term es necesario para saber qué término ha sido el que ha\n",
    "        # introducido el elemento en el heap y asi poder pushear el siguiente\n",
    "        # elemento de la lista de postings correspondiente.\n",
    "        for term in query_terms:\n",
    "            elem = terms_postings_dict[term].pop(0)\n",
    "            elem = (elem[0], elem[1], term)\n",
    "            heapq.heappush(heap_postings, elem)\n",
    "\n",
    "        # Primero procesamos el primer elemento del heap para inicializar\n",
    "        # la variable current_doc_id.\n",
    "        posting = heapq.heappop(heap_postings)\n",
    "        current_doc_id = posting[0]\n",
    "\n",
    "        # El score es el tf-idf del documento\n",
    "        doc_score = posting[1]\n",
    "        term = posting[2]\n",
    "\n",
    "        if len(terms_postings_dict[term]) > 0:\n",
    "            elem = terms_postings_dict[term].pop(0)\n",
    "            elem = (elem[0], elem[1], term)\n",
    "\n",
    "            heapq.heappush(heap_postings, elem)\n",
    "\n",
    "        # Vamos sacando elementos del heap y procesandolos hasta\n",
    "        # que el heap esté vacío.\n",
    "        while len(heap_postings) > 0:\n",
    "            posting = heapq.heappop(heap_postings)\n",
    "\n",
    "            aux_doc_id = posting[0]\n",
    "            aux_score = posting[1]\n",
    "            aux_term = posting[2]\n",
    "\n",
    "            # Si el documento del posting que acabamos de sacar del heap\n",
    "            # es el mismo que el que estamos procesando, sumamos su score.\n",
    "            if aux_doc_id == current_doc_id:\n",
    "                doc_score += aux_score\n",
    "\n",
    "            # Si no es el mismo, dividimos el score calculado por el modulo\n",
    "            # para obtener el coseno y lo insertamos en el ranking.\n",
    "            else:\n",
    "                mod = self.index.doc_module(current_doc_id)\n",
    "\n",
    "                if mod:\n",
    "                    doc_score=doc_score/mod\n",
    "                if score:\n",
    "                    ranking.push(self.index.doc_path(current_doc_id), doc_score)\n",
    "\n",
    "                current_doc_id = aux_doc_id\n",
    "                doc_score = aux_score\n",
    "\n",
    "            # Si quedan postings en el término que acabamos de procesar,\n",
    "            # insertamos el siguiente en el heap.\n",
    "            if len(terms_postings_dict[aux_term]) > 0:\n",
    "                elem = terms_postings_dict[aux_term].pop(0)\n",
    "                elem = (elem[0], elem[1], aux_term)\n",
    "\n",
    "                heapq.heappush(heap_postings, elem)\n",
    "\n",
    "        # Insertamos en el ranking el ultimo documento que estabamos procesando\n",
    "        mod = self.index.doc_module(current_doc_id)\n",
    "\n",
    "        if mod:\n",
    "            doc_score=doc_score/mod\n",
    "        if score:\n",
    "            ranking.push(self.index.doc_path(current_doc_id), doc_score)\n",
    "\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xYd4hzhukr"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**DocBasedVSMSearcher**: Clase que hereda de Searcher. Implementa el coseno como función de ránking según la búsqueda orientada a documentos.\n",
    "**Métodos:**\n",
    "* **search(query, cutoff)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *query*: string que contiene la consulta a buscar.\n",
    "    * *cutoff*: número de resultados que queremos que devuelva el buscador\n",
    "<br/><br/>\n",
    "Para la búsqueda basada en documentos hemos usado un heap de postings.\n",
    "\n",
    "En primer lugar lo que hacemos es crear un diccionario, en el que la clave es un término de la consulta y el valor es una lista de postings, en el que cada posting es una tupla (doc_id, tf-idf). Esto lo hacemos para poder iterar cosecuencialmente en las listas de postings de cada término de la consulta.\n",
    "\n",
    "Después, metemos en el heap de postings un elemento para cada término de la consulta. Los elementos del heap son tuplas (doc_id, tf-idf, term). Necesitamos guardar también el término del cual procede el postings para poder meter en el heap los siguientes postings del mismo término.\n",
    "\n",
    "A continuación, vamos sacando elementos del heap de postings y sumando los tf-idf de cada documento. Cuando sacamos un elemento del heap, si el siguiente elemento del heap es del mismo documento, sumamos el tf-idf. Si es de un documento distinto, querrá decir que ya hemos terminado de calcular el score de ese documento, por lo que calculamos su módulo y lo metemos en el Ranking. Al sacar un elemento del heap, metemos en el heap el siguiente elemento de la lista de postings del mismo término (si no quedan más no metemos nada). Y así sucesivamente hasta que el heap esté vacío."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXHr18Cdl2Q"
   },
   "source": [
    "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
    "\n",
    "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)), es decir, que permita almacenar un **número limitado de documentos** en memoria y su puntuación asociada. \n",
    "\n",
    "Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "MOfT2yZGpMNi"
   },
   "outputs": [],
   "source": [
    "class SearchRanking:\n",
    "    def __init__(self, cutoff):\n",
    "        self.cutoff = cutoff\n",
    "        self.ranking = list()\n",
    "\n",
    "    def push(self, docid, score):\n",
    "        if len(self.ranking) < self.cutoff:\n",
    "            heapq.heappush(self.ranking, (score, docid))\n",
    "        else:\n",
    "            heapq.heappushpop(self.ranking, (score, docid))\n",
    "\n",
    "    def __iter__(self):\n",
    "        ## sort ranking\n",
    "        orderedRanking = sorted(self.ranking, reverse=True)\n",
    "\n",
    "        # Invertimos la tupla para que el docid sea el primer elemento y el score el segundo\n",
    "        orderedRanking = [(x[1], x[0]) for x in orderedRanking]\n",
    "        return iter(orderedRanking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDzjUp-hwNZ"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**SearchRanking**: Clase que implementa el heap de ranking\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(cutoff)**: Constructor de la clase.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *cutoff*: número de resultados del ranking.\n",
    "<br/><br/>\n",
    "Para implementar el heap de ránking nos ayudamos del módulo heapq de Python. SearchRanking almacena el cutoff y el ranking en sí: una lista de tuplas score y su respectivo doc_id. El heap tiene el tamaño del cutoff.\n",
    "\n",
    "\n",
    "* **push(docid, score)**: \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *docid*: id del documento que queremos añadir al ranking\n",
    "    * *score*: score del docid pasado como parámetros.\n",
    "<br/><br/>\n",
    " La función push añade una tupla (score, doc_id) al heap. Se añade con el score como primer elemento de la tupla para que el heap use el score para ordenar. Si el heap no está lleno, se añade la tupla (heappush) conservando la propiedad del heap. Si el heap está lleno, a la hora de hacer push de una tupla, usamos la función heappushpop que es equivalente a hacer un heappush seguido de un heappop.\n",
    "\n",
    " * **__iter__()**:\n",
    "<br/><br/>\n",
    "Ordena el heap por score de forma descendente y devuelve un iterador en el que cada elemento es una tupla (doc_id,score). Se vuelve a dar la vuelta a la tupla para tener el doc_id como primer elemento y el score como segundo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNkPcUjMpNRn"
   },
   "source": [
    "# Ejercicio 2: Índice en RAM (3pt)\n",
    "\n",
    "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
    "\n",
    "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
    "\n",
    "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVzsIg0Zev7a"
   },
   "source": [
    "## Ejercicio 2.1: Estructura de índice\n",
    "\n",
    "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
    "\n",
    "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "VqSKneeSe2bN"
   },
   "outputs": [],
   "source": [
    "class RAMIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        # Diccionario que contendrá los postings de cada término.\n",
    "        # La clave será el término y el valor será una lista de postings,\n",
    "        # donde cada elemento de la lista es una tupla (doc_id, freq).\n",
    "        self.dict_postings = {}\n",
    "\n",
    "        # El constructor del super llamará a open si dir no es None.\n",
    "        super().__init__(dir)\n",
    "\n",
    "    def postings(self, term):\n",
    "        return self.dict_postings[term] if term in self.dict_postings else []\n",
    "\n",
    "    def all_terms(self):\n",
    "        return list(self.dict_postings)\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq))\n",
    "\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        # Guardamos la lista con los paths de los documentos.\n",
    "        p = os.path.join(dir, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "\n",
    "        # Guardamos el diccionario con los postings.\n",
    "        p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings, f)\n",
    "\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "        # Cargamos de disco la lista con los paths de los documentos y\n",
    "        # el diccionario con los postings.\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.PATHS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "            p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.dict_postings = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucORmwfCh4Um"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**RAMIndex**: Clase que hereda de Index. Construye el índice en RAM, para ello construimos las listas de postings.\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(index_path)**: Constructor de la clase. Recibe como parámetros la ruta donde se encuentra el índice. El índice almacena el diccionario que contendrá los postings de cada término. La clave será el término y el valor será una lista de postings, donde cada elemento de la lista es una tupla (doc_id, freq)\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *index_path*: Ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **postings(term)**: Devuelve la lista de postings del término pasado como parámetro. Cada posting es una tupla (doc_id, freq). Si el término no está en el diccionario, se devuelve una lista vacía.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **all_terms()**: Método que devuelve una lista con todos los términos del índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **add_postings(term,doc_id,freq)**: Método que añade un elemento a la lista de postings del término pasado como parámetro. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "    * *doc_id*: id del documento al cual corresponde el posting.\n",
    "    * *freq*: frecuencia del término en el documento identificado por doc_id.\n",
    "<br/><br/>\n",
    "Si el término no está en el diccionario, primero se añade al diccionario y se inicializa su valor a una lista vacía. Después, se añade a la lista de postings del término, la tupla formada por docid y frequencia pasadas como parámetros.\n",
    "\n",
    "\n",
    "* **save(dir)**: Método que guarda en disco los archivos generados por el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se guardarán los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el save de la clase padre Index. Además, guardamos la lista con los paths de los documentos y guardamos el diccionario con las listas de postings.\n",
    "\n",
    "* **open(dir)**: Método que carga de disco la lista con los paths de los documentos el diccionario con los postings. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde están almacenados los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el open de la clase padre Index. Además, leemos el archivo que contiene la lista con los paths de los documentos y leemos el diccionario con las listas de postings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbc9ng8e28p"
   },
   "source": [
    "## Ejercicio 2.2: Construcción del índice\n",
    "\n",
    "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "tHQ4UCf5pTw8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class RAMIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 2.2) #\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir=dir\n",
    "        self.index=RAMIndex(None)\n",
    "\n",
    "    def index_document(self, path, text):\n",
    "        text_terms=self.parser.parse(text)\n",
    "\n",
    "        self.index.add_doc(path)\n",
    "        doc_id=self.index.ndocs()-1\n",
    "\n",
    "        term_freq=Counter(text_terms)\n",
    "        for term, freq in term_freq.items():\n",
    "            self.index.add_posting(term, doc_id, freq)\n",
    "\n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_mxRswZh74N"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**RAMIndexBuilder**: Clase que hereda de Builder. Permite construir un índice RAMIndex.\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(dir)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "* **index_document(path, text)**: Método que añade un documento al índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *path*: Ruta del documento que queremos indexar.\n",
    "    * *text*: contenido del documento que queremos indexar.\n",
    "1. Primero aplicamos el parser al texto del documento para obtener la lista de términos del documento.\n",
    "2. Después, añadimos el path del documento con la función add_doc del index. \n",
    "3. Hallamos la frecuencia de cada término en el documento, utilizando la clase Counter del módulo collections de Python.\n",
    "4. Añadimos a la lista de postings cada término con su frecuencia en el documento, con la función add_posting del RAMIndex.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **commit()**: Método que guarda de forma definitiva el índice en el disco. Para ello llama a la función save del RAMIndex\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOWgbqZpV01"
   },
   "source": [
    "# Ejercicio 3: Índice en disco* (1pt)\n",
    "\n",
    "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
    "\n",
    "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
    "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
    "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
    "\n",
    "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
    "\n",
    "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación igual que indice RAM, tenemos que jugar con guardarlo en disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "br7yqFrnpZYl"
   },
   "outputs": [],
   "source": [
    "class DiskIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        # Diccionario que contendrá los postings de cada término.\n",
    "        # La clave será el término y el valor será una lista de postings,\n",
    "        self.dict_postings = {}\n",
    "\n",
    "        # Diccionario que contendrá las posiciones de los postings de cada término.\n",
    "        # La clave será el término y el valor será la posicion del posting en el archivo.\n",
    "        self.dict_postings_pos = {}\n",
    "\n",
    "        self.dir = dir\n",
    "        super().__init__(dir)\n",
    "\n",
    "    def postings(self, term):\n",
    "        if self.dict_postings_pos:\n",
    "            postings_list = []\n",
    "\n",
    "            if term in self.dict_postings_pos:\n",
    "                p = os.path.join(self.dir, Config.POSTINGS_FILE)\n",
    "\n",
    "                with open(p, 'rb') as f:\n",
    "                    # Nos colocamos en la posición del posting.\n",
    "                    f.seek(self.dict_postings_pos[term])\n",
    "\n",
    "                    # Leemos el posting.\n",
    "                    postings_list = pickle.load(f)\n",
    "\n",
    "            return postings_list\n",
    "        else:\n",
    "            return self.dict_postings[term] if term in self.dict_postings else []\n",
    "\n",
    "    def all_terms(self):\n",
    "        if self.dict_postings_pos:\n",
    "            return list(self.dict_postings_pos)\n",
    "        else:\n",
    "            return list(self.dict_postings)\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq))\n",
    "\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        # Guardamos la lista con los paths de los documentos.\n",
    "        p = os.path.join(dir, Config.PATHS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "\n",
    "        # Creamos el fichero con los postings y nos guardamos la posicion del posting.\n",
    "        p = os.path.join(dir, Config.POSTINGS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            for term, postings in self.dict_postings.items():\n",
    "                # Obtenemos la posicion del posting en el fichero y la guardamos en el diccionario.\n",
    "                self.dict_postings_pos[term] = f.tell()\n",
    "\n",
    "                # Guardamos el posting en el fichero.\n",
    "                pickle.dump(postings, f)\n",
    "\n",
    "        # Guardamos el diccionario con las posiciones de los postings.\n",
    "        p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.dict_postings_pos, f)\n",
    "\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "\n",
    "        # Cargamos de disco la lista con los paths de los documentos y\n",
    "        # el diccionario con las posiciones de los postings de cada termino.\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.PATHS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "            p = os.path.join(dir, Config.DICTIONARY_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.dict_postings_pos = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "\n",
    "class DiskIndexBuilder(RAMIndexBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir = dir\n",
    "        self.index = DiskIndex(None)\n",
    "\n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTje0viiM9I"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**DiskIndex**: Clase que hereda de Index. Construye el índice en RAM, y guarda el índice en disco. Guarda el diccionario con las posiciones de los postings de cada término en un fichero, y en otro fichero los postings. Se supone siempre que antes de empezar a usar el índice, se ha guardado en disco con el método save.\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(index_path)**: Constructor de la clase. Recibe como parámetros la ruta donde se encuentra el índice. El índice almacena :\n",
    "- dict_postings: diccionario que contendrá los postings de cada término. Es el diccionario que se usa al crear el índice. La clave será el término y el valor será una lista de postings, donde cada elemento de la lista es una tupla (doc_id, freq)\n",
    "- dict_postings_pos: diccionario que contendrá las posiciones de los postings de cada término. La clave será el término y el valor será la posicion del posting en el archivo de postings.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *index_path*: Ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **postings(term)**: Devuelve la lista de postings del término pasado como parámetro. Cada posting es una tupla (doc_id, freq). Si el término no está en el diccionario, se devuelve una lista vacía. Este método obtiene del diccionario la posición de los postings de un término y con esa posición lee en el fichero de postings los postings del término.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **all_terms()**: Método que devuelve una lista con todos los términos del índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **add_postings(term,doc_id,freq)**: Método que añade un elemento a la lista de postings del término pasado como parámetro. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "    * *doc_id*: id del documento al cual corresponde el posting.\n",
    "    * *freq*: frecuencia del término en el documento identificado por doc_id.\n",
    "<br/><br/>\n",
    "Si el término no está en el diccionario, primero se añade al diccionario y se inicializa su valor a una lista vacía. Después, se añade a la lista de postings del término, la tupla formada por docid y frecuencia pasadas como parámetros.\n",
    "<br/><br/>\n",
    "\n",
    "* **save(dir)**: Método que guarda en disco los archivos generados por el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se guardarán los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el save de la clase padre Index. Además, guardamos la lista con los paths de los documentos, guardamos el diccionario con las posiciones de los postings en un fichero y los postings en otro.\n",
    "\n",
    "* **open(dir)**: Método que carga de disco la lista con los paths de los documentos y el diccionario con las posiciones de los postings de cada termino.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde están almacenados los archivos del índice.\n",
    "<br/><br/>\n",
    "Se ejecuta el open de la clase padre Index. Además, leemos el archivo que contiene la lista con los paths de los documentos y leemos el archivo con el diccionario con las posiciones de los postings de cada termino.\n",
    "\n",
    "**DiskIndexBuilder**: Clase que hereda de Builder. Permite construir un índice DiskIndex.\n",
    "\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(dir)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "\n",
    "* **commit()**: Método que guarda de forma definitiva el índice en el disco. Para ello llama a la función save del DiskIndex\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxuclLwpaM-"
   },
   "source": [
    "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
    "\n",
    "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whoosh el positional_postings devuelve una lista de tuplas (doc_id, [posiciones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "e3uq565SpfSA"
   },
   "outputs": [],
   "source": [
    "class ProximitySearcher(Searcher):\n",
    "    # Your new code here (exercise 4*) #\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4hWTstiOIT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPPWV7pepf85"
   },
   "source": [
    "# Ejercicio 5: Índice posicional* (1pt)\n",
    "\n",
    "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "zg8MIMpipih1"
   },
   "outputs": [],
   "source": [
    "class PositionalIndex(RAMIndex):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Note that it may be better to inherit from a different class\n",
    "    # if your index extends a particular type of index\n",
    "    # For example: PositionalIndex(RAMIndex)\n",
    "\n",
    "    def postings(self, term):\n",
    "        postings=[]\n",
    "        if term in self.dict_postings:\n",
    "            for posting in self.dict_postings[term]:\n",
    "                postings.append((posting[0],posting[1]))\n",
    "            return postings\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "    def positional_postings(self, term):\n",
    "        if term in self.dict_postings:\n",
    "            postings = self.dict_postings[term]\n",
    "\n",
    "            # Eliminamos el elemento de la tupla que contiene la frecuencia.\n",
    "            return [(posting[0], posting[2]) for posting in postings]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "    def add_posting(self, term, doc_id, freq, position_list):\n",
    "        # Si el término no está en el diccionario, creamos la lista que contendrá los postings.\n",
    "        if term not in self.dict_postings:\n",
    "            self.dict_postings[term] = []\n",
    "\n",
    "        self.dict_postings[term].append((doc_id, freq, position_list))\n",
    "        \n",
    "        \n",
    "class PositionalIndexBuilder(RAMIndexBuilder):\n",
    "    # Your new code here (exercise 5*) #\n",
    "    # Same note as for PositionalIndex\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.dir=dir\n",
    "        self.index=PositionalIndex(None)\n",
    "\n",
    "        \n",
    "    def index_document(self, path, text):\n",
    "        text_terms=self.parser.parse(text)\n",
    "\n",
    "        self.index.add_doc(path)\n",
    "        doc_id=self.index.ndocs()-1\n",
    "        \n",
    "        \n",
    "        array_terms=np.array(text_terms)\n",
    "        for term in set(text_terms):\n",
    "            position_list=np.where(array_terms == term)[0]\n",
    "            self.index.add_posting(term, doc_id, len(position_list), list(position_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1gukouXiPV3"
   },
   "source": [
    "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
    "\n",
    "**PositionalIndex**: Clase que hereda de RAMIndex. Construye el índice en RAM, para ello construimos las listas de postings, en las que se incluyen las posiciones de los términos en los documentos. Cabe destacar que he hemos decidido que dict_postings sea ahora un diccionario que contendrá los postings de cada término. La clave será el término y el valor será una lista de postings, donde cada elemento de la lista es una tupla (doc_id, freq, position_list), donde position_list es la lista de posiciones del término en el documento. La longitud de esta lista coindice con el valor freq, es decir, la frecuencia del término dentro del documento.\n",
    "\n",
    "**Métodos:**\n",
    "\n",
    "* **postings(term)**: Devuelve la lista de postings del término pasado como parámetro. Cada posting es una tupla (doc_id, freq). Si el término no está en el diccionario, se devuelve una lista vacía.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **positional_postings(term)**: Devuelve una lista con los postings posicionales de un término. Cada postings es una tupla (doc_id, position_list). Si el término no está en el diccionario, se devuelve una lista vacía.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "<br/><br/>\n",
    "\n",
    "* **all_terms()**: Método que devuelve una lista con todos los términos del índice.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "* **add_postings(term,doc_id,freq)**: Método que añade un elemento a la lista de postings del término pasado como parámetro. \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *term*\n",
    "    * *doc_id*: id del documento al cual corresponde el posting.\n",
    "    * *freq*: frecuencia del término en el documento identificado por doc_id.\n",
    "<br/><br/>\n",
    "\n",
    "**PositionalIndexBuilder**: Clase que hereda de RAMIndexBuilder. Permite construir un índice PositionalIndex.\n",
    "\n",
    "\n",
    "**Métodos:**\n",
    "* **\\_\\_init\\_\\_(dir)**: Constructor de la clase. Recibe como parámetros la ruta donde se guardará el índice. Se llama al constructor de la clase padre.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *dir*: Ruta donde se encuentra el índice.\n",
    "<br/><br/>\n",
    "\n",
    "* **index_document()**: Método que añade un documento al índice.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *path*: Ruta del documento que queremos indexar.\n",
    "    * *text*: contenido del documento que queremos indexar.\n",
    "1. Primero aplicamos el parser al texto del documento para obtener la lista de términos del documento.\n",
    "2. Después, añadimos el path del documento con la función add_doc del index. \n",
    "3. Hallamos la frecuencia y posición de cada término en el documento. Para ello iteramos sobre los términos del documento, sin repetir términos. Para ello iteramos sobre el set de los términos del texto, que no tiene elementos repetidos. Usamos la función where de numpy para obtener la lista de posiciones del término en el array de términos del texto (text_terms que hemos transformado a numpy array). Así obtenemos una lista formada por las posiciones del término en el texto, la longitud de dicha lista es la frecuencia del término en el documento. \n",
    "4. Añadimos a la lista de postings cada término con su frecuencia en el documento y lista de posiciones, con la función add_posting del PositionalIndex.\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HbYGn8pjKZ"
   },
   "source": [
    "# Ejercicio 6: PageRank (1pt)\n",
    "\n",
    "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
    "\n",
    "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_aQE7SBgpk1S"
   },
   "outputs": [],
   "source": [
    "class PagerankDocScorer():\n",
    "    def __init__(self, graphfile, r, n_iter):\n",
    "        # Format of graphfile:\n",
    "        #  node1 node2\n",
    "\n",
    "        # Diccionario con los documentos/nodos como claves y como valor una lista de nodos\n",
    "        # a los que apunta, que representan las conexiones.\n",
    "        self.dict_connections = {}\n",
    "\n",
    "        # Set que contendra todos los nodos del grafo\n",
    "        self.all_nodes = set()\n",
    "\n",
    "        self.graphfile = graphfile\n",
    "        self.r = r\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.load_graphfile()\n",
    "\n",
    "        # Diccionario con los documentos como claves y como valor su pagerank\n",
    "        self.pagerank_scores = {}\n",
    "        self.calculate_pagerank()\n",
    "\n",
    "    def load_graphfile(self):\n",
    "        \"\"\"\n",
    "        Crea el grafo con todas las conexiones a partir de un fichero.\n",
    "        El formato del fichero es: node1 node2\n",
    "\n",
    "        El grafo estará contenido en el diccionario self.dict_connections, donde las claves son los nodos\n",
    "        y el valor es una lista de nodos, donde cada elemento en dicha lista representa una conexion\n",
    "        desde el nodo usado como clave al nodo que esta en la lista.\n",
    "        \"\"\"\n",
    "        with open(self.graphfile, 'r') as f:\n",
    "            for line in f:\n",
    "                mod_line = line.rstrip('\\n')\n",
    "                node1, node2 = mod_line.split('\\t')\n",
    "\n",
    "                # Add nodes to the set of all nodes\n",
    "                self.all_nodes.add(node1)\n",
    "                self.all_nodes.add(node2)\n",
    "\n",
    "                # Add node2 to the list of connections of node1\n",
    "                if node1 not in self.dict_connections:\n",
    "                    self.dict_connections[node1] = [node2]\n",
    "                else:\n",
    "                    self.dict_connections[node1].append(node2)\n",
    "\n",
    "            # A sink is a node that has no outgoing connections.\n",
    "            sinks = self.all_nodes - set(self.dict_connections.keys())\n",
    "\n",
    "            # Connect sinks to all the nodes.\n",
    "            for sink in sinks:\n",
    "                self.dict_connections[sink] = list(self.all_nodes)\n",
    "\n",
    "    def calculate_pagerank(self):\n",
    "        \"\"\"\n",
    "        Calcula el pagerank de cada nodo del grafo y lo guarda en un diccionario,\n",
    "        donde las claves son los nodos y el valor es su pagerank.\n",
    "\n",
    "        Este método usa el número de iteraciones especificado en el constructor.\n",
    "        \"\"\"\n",
    "        # Initialize pagerank scores\n",
    "        for node in self.all_nodes:\n",
    "            self.pagerank_scores[node] = 1 / len(self.all_nodes)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # Calculate new pagerank scores\n",
    "            new_pagerank_scores = {}\n",
    "            for node in self.all_nodes:\n",
    "                new_pagerank_scores[node] = self.r / len(self.all_nodes)\n",
    "\n",
    "            # Iterate through all the connections\n",
    "            for node1, connections in self.dict_connections.items():\n",
    "                for node2 in connections:\n",
    "                    new_pagerank_scores[node2] += (1 - self.r) * self.pagerank_scores[node1] / len(connections)\n",
    "\n",
    "            # Update pagerank scores\n",
    "            self.pagerank_scores = new_pagerank_scores\n",
    "\n",
    "    def rank(self, cutoff):\n",
    "        \"\"\"\n",
    "        Devuelve un ranking de cutoff documentos, ordenados por su pagerank.\n",
    "        \"\"\"\n",
    "        # Create SearchRanking\n",
    "        ranking = SearchRanking(cutoff)\n",
    "\n",
    "        for node, score in self.pagerank_scores.items():\n",
    "            ranking.push(node, score)\n",
    "\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ZT7seCiQiT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "**PageRankDocScorer**: Clase que implementa el algoritmo PageRank\n",
    "\n",
    "**Métodos:**\n",
    "\n",
    "\n",
    "* **\\_\\_init\\_\\_(graphfile, r, n_iter)**: Constructor de la clase. Recibe como parámetros \n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *graphfile*: Ruta donde se encuentra el archivo que indica los enlaces entre páginas. Cada línea del archivo tiene el formato \"node1 node2\" e indica que dichos nodos están conectados por un enlace.\n",
    "    * *r*: factor de teleportación.\n",
    "    * *n_iter*: número de iteraciones.\n",
    "<br/><br/>\n",
    "\n",
    "El constructor de la clase inicializa las estructuras que almacenarán el grafo indicado en *graphfile*. Para ello se utiliza:\n",
    "- Un diccionario **dict_connections** con los documentos/nodos como claves y como valor una lista de nodos a los que apunta, que representan las conexiones.\n",
    "\n",
    "- Un set **all_nodes** que contendrá todos los nodos del grafo (sin repeticiones).\n",
    "\n",
    "El contructor llama a la función *load_graphfile*, que carga el fichero *graphfile* y elabora **dict_connections** y **all_nodes**. A continuación, calcula el pagerank del grafo con la función *calculate_pagerank* y almacena los scores de cada nodo en un diccionario con los documentos como claves y como valor su pagerank.\n",
    "\n",
    "* **load_graphfile()**: Función que carga el fichero graphfile y crea el grafo con todas las conexiones a partir de él. El formato del fichero es: node1 node2. \n",
    "\n",
    "El grafo estará contenido en el diccionario self.dict_connections, donde las claves son los nodos y el valor es una lista de nodos, donde cada elemento en dicha lista representa una conexion desde el nodo usado como clave al nodo que esta en la lista.\n",
    "Se iteran las líneas del fichero, quitamos el salto de página de cada línea y nos quedamos con los dos nodos: node1 node2. Ambos se añadirán al set **all_nodes** (si se intenta añadir un nodo que ya está en el set, no se repetirá). A continuación, se añade node2 a la lista de conexiones de node1. \n",
    "\n",
    "Por último, para el tratamiento de sumideros primero identificamos los nodos sumidero, haciendo una resta de conjuntos. Los nodos sumideros son aquellos que están en **all_nodes** pero no están como clave en el diccionario **dict_connections**. Una vez hemos hallado la lista de nodos sumideros, los añadimos uno a uno al diccionario **dict_connections** y los conectamos con todos los nodos, es decir, el valor de cada nodo sumidero en el diccionario es la lista **all_nodes**.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "* **calculate_pagerank()**: Calcula el pagerank de cada nodo del grafo y lo guarda en un diccionario, donde las claves son los nodos y el valor es su pagerank. Este método iterativo usa el número de iteraciones especificado **n_iter**.\n",
    "\n",
    "Primero inicializamos los valores del diccionario **pagerank_scores** a 1/N donde N es el número de nodos del grafo. A continuación iniciamos el algoritmo iterativo. Calculamos los scores de la siguiente iteración y los guardamos en el diccionario **new_pagerank_scores**, según la fórmula estudiada en teoría, para ello iteramos sobre la lista de conexiones de cada nodo y utilizamos el score pagerank de la iteración precedente. Por la manera que hemos tratado los sumideros (añadiéndo las conexiones extra), los tenemos en cuenta en este cálculo. Por último, se van actualizando **pagerank_scores** hasta que se realizan **n_iter** iteraciones.\n",
    "<br/><br/>\n",
    "\n",
    "* **rank(cutoff)**:  Devuelve un objeto Searchranking con el ranking de cutoff documentos, ordenados por su pagerank.\n",
    "<br/><br/>\n",
    "Parámetros:\n",
    "    * *cutoff*: número de resultados del ranking.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zXhPtFzon72"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que esta celda funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "fTdDacCRn0u6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.02106928825378418 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.013645172119140625 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.01372218132019043 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0008907318115234375 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0016963481903076172 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0026590824127197266 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0009860992431640625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0010204315185546875 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0008246898651123047 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.00010633468627929688 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0003402233123779297 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 9.894371032714844e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa dd\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.001667022705078125 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0013759136199951172 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0012073516845703125 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0009737014770507812 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0003161430358886719 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00031280517578125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0009808540344238281 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0006344318389892578 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00037670135498046875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0008935928344726562 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00031113624572753906 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002911090850830078 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 8.559226989746094e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.4849853515625e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.651878356933594e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005939006805419922 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0001933574676513672 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00018858909606933594 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 8.654594421386719e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.461143493652344e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.651878356933594e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa dd'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0023560523986816406 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0015969276428222656 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.001554727554321289 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0009298324584960938 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00047516822814941406 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002841949462890625 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00038433074951171875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00018739700317382812 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0001678466796875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004527568817138672 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00027823448181152344 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0001704692840576172 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.270408630371094e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 4.6253204345703125e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 4.673004150390625e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002856254577636719 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00011658668518066406 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 9.441375732421875e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 5.745887756347656e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 4.410743713378906e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 4.553794860839844e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.009680747985839844 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.011949300765991211 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.012732505798339844 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.001211404800415039 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0008282661437988281 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.001325845718383789 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0009517669677734375 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0005154609680175781 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.00041294097900390625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 5.269050598144531e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0002243518829345703 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 5.054473876953125e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011570453643798828 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0012822151184082031 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0012793540954589844 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00048732757568359375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00025534629821777344 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002396106719970703 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00046181678771972656 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004086494445800781 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003056526184082031 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004875659942626953 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007824897766113281 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007479190826416016 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001735687255859375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001583099365234375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00019979476928710938 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0009012222290039062 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006198883056640625 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00035691261291503906 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.2479248046875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 5.793571472167969e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 5.8650970458984375e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa cc'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0011806488037109375 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.001651763916015625 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0024139881134033203 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0011126995086669922 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005600452423095703 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00041961669921875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00041294097900390625 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002818107604980469 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.000240325927734375 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006415843963623047 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002734661102294922 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00024199485778808594 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 5.7220458984375e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 4.6253204345703125e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 4.8160552978515625e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0014405250549316406 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00045943260192871094 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002548694610595703 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 9.584426879882812e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.296966552734375e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.02556544296910982 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.606910705566406e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'bb aa'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 2 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.015790224075317383 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.024372577667236328 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.01711893081665039 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0017311573028564453 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0021185874938964844 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.002421140670776367 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0005612373352050781 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.00052642822265625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0004451274871826172 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 5.745887756347656e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.00020885467529296875 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 5.459785461425781e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0028319358825683594 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0038919448852539062 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0037908554077148438 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0019092559814453125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003542900085449219 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00029349327087402344 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0010585784912109375 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006070137023925781 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00030922889709472656 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0010488033294677734 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003452301025390625 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002865791320800781 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 9.059906005859375e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.081031799316406e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.414817810058594e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007190704345703125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004215240478515625 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0003008842468261719 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00015878677368164062 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00013518333435058594 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00014019012451171875 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa cc'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003153085708618164 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.002643108367919922 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0026214122772216797 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.002192258834838867 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006496906280517578 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005304813385009766 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0014972686767578125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007874965667724609 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005974769592285156 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0021071434020996094 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006964206695556641 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.05996034747142527 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005209445953369141 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00014901161193847656 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00010967254638671875 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00012612342834472656 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0010974407196044922 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0004582405090332031 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.04168561889723996 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00030922889709472656 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001842975616455078 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001404285430908203 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.041685618897239964 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001537799835205078 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'bb aa'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.813121795654297 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.894395112991333 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.97564697265625 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.7418415546417236 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 1.830883264541626 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.327561855316162 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6119\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.018148183822631836 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6119\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.019091367721557617 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6119\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.020460844039916992 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6009\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.000244140625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6009\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.000396728515625 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6009\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 0.00025272369384765625 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for information probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003731966018676758 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0039010047912597656 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0037055015563964844 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0010530948638916016 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006461143493652344 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00035762786865234375 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0019254684448242188 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004551410675048828 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003199577331542969 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006582736968994141 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003437995910644531 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002906322479248047 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.534027099609375e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.246566772460938e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.05718994140625e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00038170814514160156 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002415180206298828 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0001614093780517578 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02345565075487026 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01740086527172651 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110315 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 9.942054748535156e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02345565075487026 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01740086527172651 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110315 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002148151397705078 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0.02345565075487026 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01740086527172651 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110315 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00019025802612304688 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'information probability'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for probability information\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.005154609680175781 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.007767438888549805 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0042459964752197266 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006735324859619141 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003008842468261719 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002646446228027344 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0020585060119628906 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003521442413330078 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00029397010803222656 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006353855133056641 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003390312194824219 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003218650817871094 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 9.799003601074219e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.747245788574219e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.295608520507812e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004115104675292969 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00029587745666503906 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00017452239990234375 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02345565075487026 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01740086527172651 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110315 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 8.463859558105469e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02345565075487026 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01740086527172651 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110315 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.937980651855469e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0.02345565075487026 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01740086527172651 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110315 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 9.1552734375e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'probability information'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "------------------------------\n",
      "Checking search results for higher probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00341033935546875 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0032837390899658203 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004907369613647461 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006799697875976562 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003814697265625 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00028443336486816406 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0019180774688720703 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00034427642822265625 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003113746643066406 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.000568389892578125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003466606140136719 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00027108192443847656 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.081031799316406e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.91278076171875e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.318092346191406e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00035572052001953125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00022363662719726562 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00014829635620117188 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.02685162907973089 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950256 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659118 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.961822509765625e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.02685162907973089 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950256 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659118 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 5.9604644775390625e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0.02685162907973089 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950256 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659118 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.222724914550781e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'higher probability'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/urls.txt'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 2.7111611366271973 seconds ---\n",
      "\tWhooshForwardIndex: 2.9289495944976807 seconds ---\n",
      "\tWhooshPositionalIndex: 2.9953372478485107 seconds ---\n",
      "\tRAMIndex: 1.9956238269805908 seconds ---\n",
      "\tDiskIndex: 1.7437536716461182 seconds ---\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.001544952392578125 seconds ---\n",
      "\tWhooshForwardIndex: 0.001241445541381836 seconds ---\n",
      "\tWhooshPositionalIndex: 0.0009868144989013672 seconds ---\n",
      "\tRAMIndex: 0.004050016403198242 seconds ---\n",
      "\tDiskIndex: 0.001489400863647461 seconds ---\n",
      "  Disk space...\n",
      "\tWhooshIndex: 1101097 space ---\n",
      "\tWhooshForwardIndex: 1179269 space ---\n",
      "\tWhooshPositionalIndex: 1260189 space ---\n",
      "\tRAMIndex: 128095 space ---\n",
      "\tDiskIndex: 225039 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'information probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0031528472900390625 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.004872322082519531 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006988048553466797 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0007171630859375 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.000301361083984375 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0003170967102050781 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.914138793945312e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 8.797645568847656e-05 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00027680397033691406 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.0002913475036621094 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'information probability'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002257823944091797 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.0002422332763671875 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'probability information'\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003680706024169922 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.005074977874755859 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007379055023193359 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0007574558258056641 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.01747928247142733 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003192424774169922 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.000335693359375 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 7.152557373046875e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 9.083747863769531e-05 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002799034118652344 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.0002944469451904297 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'probability information'\n",
      "0.023455650754870248 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017400865271726365 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009575341629110203 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0001652240753173828 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.00018024444580078125 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'higher probability'\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0035152435302734375 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.004867076873779297 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0011680126190185547 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.001188516616821289 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916736 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449961 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00037598609924316406 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.00039696693420410156 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 6.937980651855469e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 8.7738037109375e-05 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0002849102020263672 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.00029850006103515625 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'higher probability'\n",
      "0.026851629079730877 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012219368281950154 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005721206778659051 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00015616416931152344 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.0001704692840576172 seconds ---\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 85.30813884735107 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 87.56075620651245 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 87.1441719532013 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 57.6436333656311 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 57.320109367370605 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 92.56069231033325 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 118003\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.3851501941680908 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 118003\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.36133599281311035 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 118003\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.3284125328063965 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 111590\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.005509853363037109 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 111590\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.0035321712493896484 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 998\n",
      "Vocabulary size: 111590\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(0, 28), (9, 4)]\n",
      "Done ( 0.0058765411376953125 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for obama family tree\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n",
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.04884982109069824 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n",
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.0481564998626709 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n",
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.0755915641784668 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 1.6068594455718994 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.014896154403686523 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.006964445114135742 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.6408932209014893 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.01693248748779297 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.007126808166503906 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 3.2833027839660645 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.018845796585083008 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.009252548217773438 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.03733253479003906 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.0018930435180664062 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.001994609832763672 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.41459059715270996 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.05144810676574707 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.002028226852416992 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598442 \t clueweb09-en0001-02-21241.html\n",
      "0.22362309586160156 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.26608753204345703 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598442 \t clueweb09-en0001-02-21241.html\n",
      "0.22362309586160156 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.0433955192565918 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598442 \t clueweb09-en0001-02-21241.html\n",
      "0.22362309586160156 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.004361629486083984 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'obama family tree'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/docs1k.zip'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 83.29247045516968 seconds ---\n",
      "\tWhooshForwardIndex: 91.78028845787048 seconds ---\n",
      "\tWhooshPositionalIndex: 87.97744059562683 seconds ---\n",
      "\tRAMIndex: 57.36173725128174 seconds ---\n",
      "\tDiskIndex: 57.85594940185547 seconds ---\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.009197473526000977 seconds ---\n",
      "\tWhooshForwardIndex: 0.007818937301635742 seconds ---\n",
      "\tWhooshPositionalIndex: 0.012026071548461914 seconds ---\n",
      "\tRAMIndex: 0.21604251861572266 seconds ---\n",
      "\tDiskIndex: 0.06230449676513672 seconds ---\n",
      "  Disk space...\n",
      "\tWhooshIndex: 22819862 space ---\n",
      "\tWhooshForwardIndex: 28575696 space ---\n",
      "\tWhooshPositionalIndex: 31237568 space ---\n",
      "\tRAMIndex: 5967296 space ---\n",
      "\tDiskIndex: 7863727 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/docs1k.zip'] document collection with query: 'obama family tree'\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.485499731264426 \t clueweb09-en0010-79-2218.html\n",
      "15.887791854254225 \t clueweb09-en0010-57-32937.html\n",
      "15.808952372971135 \t clueweb09-en0001-02-21241.html\n",
      "15.611476287511962 \t clueweb09-en0008-45-29117.html\n",
      "15.554087101921635 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.03964424133300781 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.04811429977416992 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 1.8005146980285645 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 1.8005383014678955 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2842178936437617 \t clueweb09-en0010-79-2218.html\n",
      "0.22631966869496395 \t clueweb09-en0009-30-2768.html\n",
      "0.224801913947326 \t clueweb09-en0001-02-21241.html\n",
      "0.22386305596546352 \t clueweb09-en0009-30-2441.html\n",
      "0.22349064069479566 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.017030715942382812 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.017052650451660156 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.0016863346099853516 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.0017137527465820312 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.04673194885253906 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.04674863815307617 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2855798492804466 \t clueweb09-en0010-79-2218.html\n",
      "0.23011119798598417 \t clueweb09-en0001-02-21241.html\n",
      "0.2236230958616016 \t clueweb09-en0009-30-2768.html\n",
      "0.22359113984813714 \t clueweb09-en0009-30-2441.html\n",
      "0.22328915854098813 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.002252817153930664 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.002276182174682617 seconds ---\n",
      "=================================================================\n",
      "Testing indices and search on 3 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 84.8165295124054 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 89.70510029792786 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 89.38644528388977 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 59.65569186210632 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 59.161354303359985 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Collection: ./collections/urls.txt\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 95.471853017807 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 120110\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.3912041187286377 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 120110\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.40338945388793945 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 120110\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.3901336193084717 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 113480\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.005372047424316406 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 113480\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.003444671630859375 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 1003\n",
      "Vocabulary size: 113480\n",
      "  Frequency of word \"seat\" in document 0 - ./collections/toy2/example.txt: 0\n",
      "  Total frequency of word \"seat\" in the collection: 1406 occurrences over 119 documents\n",
      "  Docs containing the word 'seat': 119\n",
      "    First two documents: [(5, 28), (14, 4)]\n",
      "Done ( 0.007592439651489258 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for obama family tree\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.51921326001622 \t clueweb09-en0010-79-2218.html\n",
      "15.922414828779061 \t clueweb09-en0010-57-32937.html\n",
      "15.843401520993417 \t clueweb09-en0001-02-21241.html\n",
      "15.647364701456898 \t clueweb09-en0008-45-29117.html\n",
      "15.59118889548364 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.049900054931640625 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "16.51921326001622 \t clueweb09-en0010-79-2218.html\n",
      "15.922414828779061 \t clueweb09-en0010-57-32937.html\n",
      "15.843401520993417 \t clueweb09-en0001-02-21241.html\n",
      "15.647364701456898 \t clueweb09-en0008-45-29117.html\n",
      "15.59118889548364 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.052849769592285156 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "16.51921326001622 \t clueweb09-en0010-79-2218.html\n",
      "15.922414828779061 \t clueweb09-en0010-57-32937.html\n",
      "15.843401520993417 \t clueweb09-en0001-02-21241.html\n",
      "15.647364701456898 \t clueweb09-en0008-45-29117.html\n",
      "15.59118889548364 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.08449411392211914 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 1.744396686553955 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.020441055297851562 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.2856796867661231 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.008752107620239258 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.7776284217834473 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.01569366455078125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.2856796867661231 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.006509542465209961 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 3.3080766201019287 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.016575336456298828 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.2856796867661231 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.008625984191894531 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.2310268870385333 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.03833436965942383 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.2310268870385333 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.0032401084899902344 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.23102688703853336 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.002282381057739258 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.2310268870385333 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.380540132522583 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.2310268870385333 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.04709911346435547 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.23102688703853336 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.002548694610595703 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.23102688703853314 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.2529478073120117 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.23102688703853314 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.03332042694091797 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.2310268870385332 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.0029625892639160156 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'obama family tree'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/toy2/', './collections/urls.txt', './collections/docs1k.zip'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 90.42452836036682 seconds ---\n",
      "\tWhooshForwardIndex: 97.02529573440552 seconds ---\n",
      "\tWhooshPositionalIndex: 86.55951428413391 seconds ---\n",
      "\tRAMIndex: 56.60266709327698 seconds ---\n",
      "\tDiskIndex: 57.21701502799988 seconds ---\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.00870823860168457 seconds ---\n",
      "\tWhooshForwardIndex: 0.008454322814941406 seconds ---\n",
      "\tWhooshPositionalIndex: 0.011075019836425781 seconds ---\n",
      "\tRAMIndex: 0.20332837104797363 seconds ---\n",
      "\tDiskIndex: 0.04253792762756348 seconds ---\n",
      "  Disk space...\n",
      "\tWhooshIndex: 23224692 space ---\n",
      "\tWhooshForwardIndex: 29059069 space ---\n",
      "\tWhooshPositionalIndex: 31783833 space ---\n",
      "\tRAMIndex: 6045258 space ---\n",
      "\tDiskIndex: 7973766 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/toy2/', './collections/urls.txt', './collections/docs1k.zip'] document collection with query: 'obama family tree'\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "16.51921326001622 \t clueweb09-en0010-79-2218.html\n",
      "15.922414828779061 \t clueweb09-en0010-57-32937.html\n",
      "15.843401520993417 \t clueweb09-en0001-02-21241.html\n",
      "15.647364701456898 \t clueweb09-en0008-45-29117.html\n",
      "15.59118889548364 \t clueweb09-enwp01-59-16163.html\n",
      "\n",
      "Done ( 0.037073373794555664 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.04496335983276367 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 1.6831238269805908 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 1.6831457614898682 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.28567968676612304 \t clueweb09-en0010-79-2218.html\n",
      "0.22713454550370205 \t clueweb09-en0009-30-2768.html\n",
      "0.22566655536218813 \t clueweb09-en0001-02-21241.html\n",
      "0.2246480865666664 \t clueweb09-en0009-30-2441.html\n",
      "0.22427509440033008 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.015459060668945312 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.015477895736694336 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.2310268870385333 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.0016016960144042969 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.0016193389892578125 seconds ---\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.2310268870385333 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.04552793502807617 seconds )\n",
      "\n",
      "--- TermVSM on Disk 0.045545101165771484 seconds ---\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.2870428770884237 \t clueweb09-en0010-79-2218.html\n",
      "0.23102688703853336 \t clueweb09-en0001-02-21241.html\n",
      "0.22446170225014214 \t clueweb09-en0009-30-2768.html\n",
      "0.22437688888193916 \t clueweb09-en0009-30-2441.html\n",
      "0.22407406087410137 \t clueweb09-en0009-30-2755.html\n",
      "\n",
      "Done ( 0.002179861068725586 seconds )\n",
      "\n",
      "--- DocVSM on Disk 0.002200603485107422 seconds ---\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 538.5100557804108 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 582.3709962368011 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 572.7762765884399 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 452.15492153167725 seconds )\n",
      "\n",
      "Building index with <class '__main__.DiskIndexBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 425.82387590408325 seconds )\n",
      "\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/docs10k.zip\n",
      "Done ( 621.1800606250763 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 9952\n",
      "Vocabulary size: 439173\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-00-01835.html: 0\n",
      "  Total frequency of word \"seat\" in the collection: 5919.0 occurrences over 997 documents\n",
      "  Docs containing the word 'seat': 997\n",
      "    First two documents: [(44, 2), (47, 28)]\n",
      "Done ( 1.631887435913086 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 9952\n",
      "Vocabulary size: 439173\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-00-01835.html: 0\n",
      "  Total frequency of word \"seat\" in the collection: 5919.0 occurrences over 997 documents\n",
      "  Docs containing the word 'seat': 997\n",
      "    First two documents: [(44, 2), (47, 28)]\n",
      "Done ( 1.6179187297821045 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 9952\n",
      "Vocabulary size: 439173\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-00-01835.html: 0\n",
      "  Total frequency of word \"seat\" in the collection: 5919.0 occurrences over 997 documents\n",
      "  Docs containing the word 'seat': 997\n",
      "    First two documents: [(44, 2), (47, 28)]\n",
      "Done ( 1.590752124786377 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 9952\n",
      "Vocabulary size: 363822\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-00-01835.html: 0\n",
      "  Total frequency of word \"seat\" in the collection: 5990 occurrences over 998 documents\n",
      "  Docs containing the word 'seat': 998\n",
      "    First two documents: [(44, 2), (47, 28)]\n",
      "Done ( 0.022858142852783203 seconds )\n",
      "\n",
      "Reading index with <class '__main__.DiskIndex'>\n",
      "Collection size: 9952\n",
      "Vocabulary size: 363822\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-00-01835.html: 0\n",
      "  Total frequency of word \"seat\" in the collection: 5990 occurrences over 998 documents\n",
      "  Docs containing the word 'seat': 998\n",
      "    First two documents: [(44, 2), (47, 28)]\n",
      "Done ( 0.01817464828491211 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 9952\n",
      "Vocabulary size: 363822\n",
      "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-00-01835.html: 0\n",
      "  Total frequency of word \"seat\" in the collection: 5990 occurrences over 998 documents\n",
      "  Docs containing the word 'seat': 998\n",
      "    First two documents: [(44, 2), (47, 28)]\n",
      "Done ( 0.024145126342773438 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for obama family tree\n",
      "  WhooshSearcher with index WhooshIndex for query 'obama family tree'\n",
      "19.97980968231021 \t clueweb09-en0010-79-02218.html\n",
      "19.00287927047043 \t clueweb09-en0010-57-32937.html\n",
      "18.90827733288588 \t clueweb09-en0001-02-21241.html\n",
      "18.85621589874382 \t clueweb09-en0010-57-32591.html\n",
      "18.834727190253766 \t clueweb09-en0010-57-32598.html\n",
      "\n",
      "Done ( 0.48697400093078613 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "19.97980968231021 \t clueweb09-en0010-79-02218.html\n",
      "19.00287927047043 \t clueweb09-en0010-57-32937.html\n",
      "18.90827733288588 \t clueweb09-en0001-02-21241.html\n",
      "18.85621589874382 \t clueweb09-en0010-57-32591.html\n",
      "18.834727190253766 \t clueweb09-en0010-57-32598.html\n",
      "\n",
      "Done ( 0.3956329822540283 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "19.97980968231021 \t clueweb09-en0010-79-02218.html\n",
      "19.00287927047043 \t clueweb09-en0010-57-32937.html\n",
      "18.90827733288588 \t clueweb09-en0001-02-21241.html\n",
      "18.85621589874382 \t clueweb09-en0010-57-32591.html\n",
      "18.834727190253766 \t clueweb09-en0010-57-32598.html\n",
      "\n",
      "Done ( 0.6127355098724365 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "ProximitySearcher still not implemented\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 85.28540205955505 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 0.09804296493530273 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 0.04844927787780762 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 6.485586166381836 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 0.10941815376281738 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 0.04694795608520508 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 131.7309958934784 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 0.10589861869812012 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'obama family tree'\n",
      "0.4364133184439472 \t clueweb09-en0002-89-01498.html\n",
      "0.423715741944431 \t clueweb09-en0007-13-05862.html\n",
      "0.4157382645418763 \t clueweb09-en0005-38-00576.html\n",
      "0.3983723368734467 \t clueweb09-en0006-84-27106.html\n",
      "0.38900530837750835 \t clueweb09-en0000-15-04138.html\n",
      "\n",
      "Done ( 0.0513150691986084 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.4239574690343873 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.4008683373191491 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 2.165990114212036 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.4239574690343873 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.4008683373191491 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 0.011284828186035156 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.4239574690343873 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.4008683373191491 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 0.0135040283203125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.4239574690343873 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.4008683373191491 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 14.056848764419556 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.4239574690343873 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.4008683373191491 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 1.1790320873260498 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index DiskIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.4239574690343873 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.4008683373191491 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 0.014744281768798828 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.42395746903438736 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.400868337319149 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 18.47615885734558 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.42395746903438736 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.400868337319149 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 1.5680484771728516 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'obama family tree'\n",
      "0.42604254103637584 \t clueweb09-en0002-89-01498.html\n",
      "0.42395746903438736 \t clueweb09-en0007-13-05862.html\n",
      "0.42313379057464995 \t clueweb09-en0005-38-00576.html\n",
      "0.400868337319149 \t clueweb09-en0000-15-04138.html\n",
      "0.39436535731535927 \t clueweb09-en0005-63-02535.html\n",
      "\n",
      "Done ( 0.033879995346069336 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'obama family tree'\n",
      "ProximitySearcher or PositionalIndex still not implemented\n",
      "----------------------------\n",
      "Testing PageRank\n",
      "0.3846153846153846 \t c\n",
      "0.3589743589743589 \t a\n",
      "0.2564102564102564 \t b\n",
      "\n",
      "--- Pagerank with toy_graph_1 0.0031859874725341797 seconds ---\n",
      "0.368421052631579 \t b\n",
      "0.3157894736842105 \t c\n",
      "0.3157894736842105 \t a\n",
      "\n",
      "--- Pagerank with toy_graph_2 0.0011150836944580078 seconds ---\n",
      "0.033333748623865 \t clueweb09-en0000-06-11938.html\n",
      "0.032822385415072756 \t clueweb09-en0000-01-22977.html\n",
      "0.03264169111517307 \t clueweb09-en0000-06-11940.html\n",
      "0.03170456979307948 \t clueweb09-en0000-06-11932.html\n",
      "0.031598005967786835 \t clueweb09-en0000-01-27969.html\n",
      "\n",
      "--- Pagerank with simulated links for doc1k 0.49329638481140137 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # We now test building different implementations of an index\n",
    "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_paths)\n",
    "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_paths)\n",
    "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_paths)\n",
    "    try:\n",
    "        test_build(RAMIndexBuilder(index_path + \"ram\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(DiskIndexBuilder(index_path + \"disk\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"PositionalIndexBuilder still not implemented\")\n",
    "\n",
    "    def catch_index(func, name, *args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except NotImplementedError:\n",
    "            print(name + \" still not implemented (index)\")\n",
    "            return None\n",
    "\n",
    "    # We now inspect all the implementations\n",
    "    indices = [\n",
    "            WhooshIndex(index_path + \"whoosh\"),\n",
    "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
    "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
    "            catch_index(lambda: RAMIndex(index_path + \"ram\"), \"RAMIndex\"),\n",
    "            catch_index(lambda: DiskIndex(index_path + \"disk\"), \"DiskIndex\"),\n",
    "            catch_index(lambda: PositionalIndex(index_path + \"pos\"), \"PositionalIndex\"),\n",
    "            ]\n",
    "    for index in indices:\n",
    "        if index:\n",
    "            test_read(index, word)\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Checking search results for %s\" % (query))\n",
    "        # Whoosh searcher can only work with its own indices\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        try:\n",
    "            test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher still not implemented\")\n",
    "        for index in indices:\n",
    "            if index:\n",
    "                # our searchers should work with any other index\n",
    "                test_search(SlowVSMSearcher(index), index, query, 5)\n",
    "                try:\n",
    "                    test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"TermBasedVSMSearcher still not implemented\")\n",
    "                try:\n",
    "                    test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"DocBasedVSMSearcher still not implemented\")\n",
    "        try:\n",
    "            test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher or PositionalIndex still not implemented\")\n",
    "\n",
    "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
    "    indices = list()\n",
    "\n",
    "    if analyse_performance:\n",
    "        # let's analyse index performance\n",
    "        test_index_performance(collection_paths, index_path)\n",
    "        # let's analyse search performance\n",
    "        for query in queries:\n",
    "            test_search_performance(collection_paths, index_path, query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # this function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # when we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    # more tests\n",
    "    doc_id = 0\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, index, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def disk_space(index_path: str) -> int:\n",
    "    space = 0\n",
    "    if os.path.isdir(index_path):\n",
    "        for f in os.listdir(index_path):\n",
    "            p = os.path.join(index_path, f)\n",
    "            if os.path.isfile(p):\n",
    "                space += os.path.getsize(p)\n",
    "    return space\n",
    "\n",
    "def test_index_performance (collection_paths: list, base_index_path: str):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing index performance on \" + str(collection_paths) + \" document collection\")\n",
    "\n",
    "    print(\"  Build time...\")\n",
    "    start_time = time.time()\n",
    "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = RAMIndexBuilder(base_index_path + \"ram\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = DiskIndexBuilder(base_index_path + \"disk\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "\n",
    "    print(\"  Load time...\")\n",
    "    start_time = time.time()\n",
    "    WhooshIndex(base_index_path + \"whoosh\")\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        RAMIndex(base_index_path + \"ram\")\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        DiskIndex(base_index_path + \"disk\")\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "\n",
    "    print(\"  Disk space...\")\n",
    "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
    "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
    "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
    "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
    "    print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
    "\n",
    "\n",
    "def test_search_performance (collection_paths: list, base_index_path: str, query: str, cutoff: int):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing search performance on \" + str(collection_paths) + \" document collection with query: '\" + query + \"'\")\n",
    "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
    "    try:\n",
    "        ram_index = RAMIndex(base_index_path + \"ram\")\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "        ram_index = None\n",
    "    try:\n",
    "        disk_index = DiskIndex(base_index_path + \"disk\")\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "        disk_index = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
    "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # let's test some combinations of ranking + index implementations\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "        print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if ram_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
    "            print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DocBasedVSMSearcher still not implemented\")\n",
    "\n",
    "def test_pagerank(graphs_root_dir, cutoff):\n",
    "    print(\"----------------------------\")\n",
    "    # we separate this function because it cannot work with all the collections\n",
    "    print(\"Testing PageRank\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "test_collection ([collections_root_dir + \"toy1/\"], index_root_dir + \"toy1/\", \"cc\", [\"aa dd\", \"aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy2/\"], index_root_dir + \"toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy1/\", collections_root_dir + \"toy2/\"], index_root_dir + \"toys/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], True)\n",
    "test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
    "test_collection ([collections_root_dir + \"toy2/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"three_collections/\", \"seat\", [\"obama family tree\"], True)\n",
    "test_collection ([collections_root_dir + \"docs10k.zip\"], index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], False)\n",
    "test_pagerank(\"./collections/\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JhJJSFiSl5"
   },
   "source": [
    "### Resumen de coste y rendimiento\n",
    "\n",
    "Hay que analizar las **diferencias de rendimiento** observadas entre las diferentes implementaciones que se han creado y probado para cada componente.\n",
    "\n",
    "En concreto, hay que reportar tiempo de indexado, consumo máximo de RAM y espacio en disco al construir el índice, y el tiempo de carga y consumo máximo de RAM al cargar el índice para cada una de las colecciones utilizadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "|      | Construcción | del | índice | Carga del | índice |\n",
    "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
    "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
    "| toy1 | | | | | |\n",
    "| toy2 | | | | | |\n",
    "| toys | | | | | |\n",
    "| 1K | | | | | |\n",
    "| 10K | | | | | |\n",
    "\n",
    "### Respuesta\n",
    "Vamos a analizar las diferencias de rendimiento observadas entre las implementaciones de RAMIndex y DiskIndex, que son los más interesantes por la diferente forma en la que guardan y cargan el índice.\n",
    "\n",
    "Vamos a comenzar primero con la colección más grande que hemos testeado, para que se vean más claramente las diferencias.\n",
    "\n",
    "**Rendimiento con la colección de documentos toy2, urls.txt y docs1k.zip**\n",
    "|      | Construcción del | índice | Carga del índice |\n",
    "|------|--------------------|------------------|-----------------|\n",
    "|      | Tiempo de indexado | Espacio en disco | Tiempo de carga |\n",
    "| RAMIndex |56.60266709327698 |6045258| 0.20332837104797363|\n",
    "| DiskIndex |57.21701502799988|7973766| 0.04253792762756348|\n",
    "\n",
    "Observamos que el tiempo de indexado de DiskIndex es ligeramente mayor, esto es debido a que DiskIndex, para guardar los datos, primero tiene que guardar los postings de cada término uno a uno, y guardarse la posición, y después en un fichero distinto, guardar el diccionario con los términos y las posiciones; mientras que RAMIndex guarda directamente un diccionario con los términos y los postings.\n",
    "\n",
    "Vemos que DiskIndex también ocupa más espacio en disco porque además de guardar los postings, también tiene que guardar las posiciones de los postings en el fichero de postings.\n",
    "\n",
    "Por otro lado, vemos que el tiempo de carga de DiskIndex es menor que el de RAMIndex. Esto se debe a que DiskIndex carga solo el diccionario con los términos y las posiciones de los postings, mientras que RAMIndex tiene que cargar todos los postings.\n",
    "\n",
    "\n",
    "**Rendimiento con los documentos de urls.txt**\n",
    "|      | Construcción del | índice | Carga del índice |\n",
    "|------|--------------------|------------------|-----------------|\n",
    "|      | Tiempo de indexado | Espacio en disco | Tiempo de carga |\n",
    "| RAMIndex |1.9956238269805908 |128095| 0.004050016403198242|\n",
    "| DiskIndex |1.7437536716461182|225039| 0.001489400863647461|\n",
    "\n",
    "El tiempo de indexado de RAMIndex en este caso es ligeramente mayor, lo cual es contraintuitivo ya que, como hemos explicado anteriormente, DiskIndex tiene que hacer más operaciones en tiempo de indexado. Sin embargo, al ser una colección de documentos pequeña cualquier fluctuación debida a factores externos nos puede modificar las mediciones de tiempo y hacer que no sean muy representativas.\n",
    "\n",
    "En cuanto al espacio en disco, vemos que como cabría esperar, DiskIndex ocupa más espacio en disco que RAMIndex.\n",
    "\n",
    "Y para el tiempo de carga sí que vemos de nuevo que DiskIndex es más rápido que RAMIndex.\n",
    "\n",
    "**Rendimiento con los documentos de docs1k.zip**\n",
    "|      | Construcción del | índice | Carga del índice |\n",
    "|------|--------------------|------------------|-----------------|\n",
    "|      | Tiempo de indexado | Espacio en disco | Tiempo de carga |\n",
    "| RAMIndex |57.36173725128174 |5967296| 0.21604251861572266|\n",
    "| DiskIndex |57.85594940185547|7863727| 0.06230449676513672|\n",
    "\n",
    "Observamos que el tiempo de indexado de DiskIndex para esta colección de documentos es de nuevo mayor que con RAMIndex, por lo ya explicado anteriormente, pero la diferencia es pequeña.\n",
    "\n",
    "En cuanto al espacio en disco, de nuevo DiskIndex ocupa más espacio que RAMIndex. Vemos también que el espacio ocupado con docs1k.zip es ligeramente mayor que con la colección [toy2, urls.txt y docs1k.zip], como es normal ya que esta colección de documentos es más grande. De esta forma vemos que el espacio en disco es coherente con el tamaño de la colección de documentos.\n",
    "\n",
    "Y el tiempo de carga vemos de nuevo que el de DiskIndex es menor que el de RAMIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
